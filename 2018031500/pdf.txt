主编
作者
达卿
仁重
（排名不分先后）
搜 索 事 业 部 哲予 削觚 萱然 益风
席奈 龙楚 云志 ⼠雍
晨松 劲枩 冷江
阿⾥妈妈事业部 探微 庙算 亮博 红桦
计算平台事业部 临在 岑鸣
智能服务事业部 海青
致谢
技术战略部
（以上均为阿里花名）
序
当前的机器学习算法⼤致可以分为有监督的学习、⽆监督的学习和强化学
习（Reinforcement Learning）等。强化学习和其他学习⽅法不同之处在于强化学
习是智能系统从环境到⾏为映射的学习，以使奖励信号函数值最⼤。如果智能
体的某个⾏为策略导致环境正的奖赏，那么智能体以后产⽣这个⾏为策略的趋
势便会加强。强化学习是最接近于⾃然界动物学习的本质的⼀种学习范式。然
⽽强化学习从提出到现在，也差不多有半个世纪左右，它的应⽤场景仍很有限，
规模⼤⼀点的问题就会出现维数爆炸，难于计算，所以往往看到的例⼦都是相
对简化的场景。
最近因为与深度学习结合，解决海量数据的泛化问题，取得了让⼈印象深刻
的成果。包括 DeepMind 的⾃动学习玩 ATARI 游戏，以及 AlphaGo 在围棋⼤赛中
战胜世界冠军等，其背后的强⼤武器就是深度强化学习技术。相对于 DeepMind
和学术界看重强化学习的前沿研究，阿⾥巴巴则将重点放在推动强化学习技术
输出及商业应⽤。在阿⾥移动电商平台中，⼈机交互的便捷，碎⽚化使⽤的普
遍性，页⾯切换的串⾏化，⽤户轨迹的可跟踪性等都要求我们的系统能够对变
幻莫测的⽤户⾏为以及瞬息万变的外部环境进⾏完整地建模。平台作为信息的
载体，需要在与消费者的互动过程中，根据对消费者（环境）的理解，及时调整
提供信息（商品、客服机器⼈的回答、路径选择等）的策略，从⽽最⼤化过程累
积收益（消费者在平台上的使⽤体验）。基于监督学习⽅式的信息提供⼿段，缺
少有效的探索能⼒，系统倾向于给消费者推送曾经发⽣过⾏为的信息单元（商
品、店铺或问题答案）。⽽强化学习作为⼀种有效的基于⽤户与系统交互过程建
模和最⼤化过程累积收益的学习⽅法，在⼀些阿⾥具体的业务场景中进⾏了很
好的实践并得到⼤规模应⽤。
在搜索场景中，阿⾥巴巴对⽤户的浏览购买⾏为进⾏ MDP 建模，在搜索
实时学习和实时决策计算体系之上，实现了基于强化学习的排序策略决策模型，
序
. V .
从⽽使得淘宝搜索的智能化进化⾄新的⾼度。双 11 桶测试效果表明，算法指标
取得了近 20% 的⼤幅提升。
在推荐场景中，阿⾥巴巴使⽤了深度强化学习与⾃适应在线学习，通过持
续机器学习和模型优化建⽴决策引擎，对海量⽤户⾏为以及百亿级商品特征进
⾏实时分析，帮助每⼀个⽤户迅速发现宝贝，提⾼⼈和商品的配对效率，算法
效果指标提升了 10% 20%。
在智能客服中，如阿⾥⼩蜜这类的客服机器⼈，作为投放引擎的 agent，需
要有决策能⼒。这个决策不是基于单⼀节点的直接收益来确定，⽽是⼀个较为
长期的⼈机交互的过程，把消费者与平台的互动看成是⼀个马尔可夫决策过程，
运⽤强化学习框架，建⽴⼀个消费者与系统互动的回路系统，⽽系统的决策是
建⽴在最⼤化过程收益上，来达到⼀个系统与⽤户的动态平衡。
在⼴告系统中，如果⼴告主能够根据每⼀条流量的价值进⾏单独出价，⼴
告主便可以在各⾃的⾼价值流量上提⾼出价，⽽在普通流量上降低出价，如此
容易获得较好的 ROI，与此同时平台也能够提升⼴告与访客间的匹配效率。阿⾥
巴巴实现了基于强化学习的智能调价技术，对于来到⼴告位的每⼀个访客，根
据他们的当前状态去决定如何操作调价，给他们展现特定的⼴告，引导他们的
状态向我们希望的⽅向上做⼀步转移，在双 11 实测表明，CTR，RPM 和 GMV
均得到了⼤幅提升。
当然，强化学习在阿⾥巴巴内部的实践远不⽌此，鉴于篇幅限制，这本电
⼦书只介绍了其中的⼀部分。未来深度强化学习的发展必定是理论探索和应⽤
实践的双链路持续深⼊。希望这本电⼦书能抛砖引⽟，给⼯业界和学术界带来
⼀些输⼊，共同推进深度强化学习的更⼤发展。
阿⾥巴巴 研究员 青峰
2018 年 01 ⽉于杭州
目 录
第一章 基于强化学习的实时搜索排序策略调控
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
1.3 算法设计 . .
1.1 背景 . .
.
1.2 问题建模 . .
. .
.
1.2.1 强化学习简介 . .
1.2.2 状态定义 . .
.
1.2.3 奖赏函数设定 . .
.
.
1.3.1 策略函数 . .
.
1.3.2 策略梯度 . .
.
1.3.3 值函数的学习 . .
.
.
.
.
. .
.
.
.
1.4 奖赏塑形 . .
1.5 实验效果 . .
.
1.6 DDPG 与梯度融合 .
1.7 总结与展望 . .
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
第二章 延迟奖赏在搜索排序场景中的作用分析
.
.
.
.
.
.
2.1 背景 . .
.
. .
2.2 搜索排序问题回顾 .
.
.
2.3 数据统计分析 . .
.
2.4 搜索排序问题形式化 . .
. .
.
.
. .
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
1
1
3
3
5
6
6
6
7
9
10
12
14
16
18
18
19
21
24
. II .
⽬ 录
2.5 理论分析 . .
.
.
.
.
.
.
.
2.5.1 马尔可夫性质 . .
.
2.5.2 折扣率 . .
.
.
.
.
.
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
2.6 实验分析 . .
第三章 基于多智能体强化学习的多场景联合优化
.
3.1 背景 . .
3.2 问题建模 . .
.
.
3.3 应⽤ . .
3.4 实验 . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
. .
. .
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
3.2.1 相关背景简介 . .
.
3.2.2 建模⽅法 . .
.
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. .
.
3.3.1 搜索与电商平台 . .
. . . . . . . . . . . . . . . . . . . . . .
3.3.2 多排序场景协同优化 . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . .
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
.
.
3.4.1 实验设置 . .
3.4.2 对⽐基准 . .
3.4.3 实验结果 . .
3.4.4 在线⽰例 . .
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3.5 总结与展望 . .
第四章 强化学习在淘宝锦囊推荐系统中的应用
.
.
.
.
.
.
. .
.
.
4.1 背景 . .
. .
.
.
.
4.1.1 淘宝锦囊 . .
4.1.2 锦囊的类型调控 . .
.
4.1.3 ⼯作摘要 . .
.
.
.
.
.
4.2 系统框架及问题建模 . .
.
.
.
4.2.1 系统框架 . .
4.2.2 问题建模 . .
4.3 算法及模型设计 . .
.
.
.
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
27
27
28
31
34
34
36
36
37
43
43
45
46
47
47
48
51
51
55
55
55
55
57
57
57
58
60
⽬ 录
. III .
.
4.3.1 主体框架 . .
.
4.3.2 分层采样池 . .
.
4.3.3 基准约减 . .
4.3.4 算法流程 . .
.
.
.
.
.
.
.
.
4.4 实验与总结 . .
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . .
第五章 基于强化学习的引擎性能优化
5.1 背景 . .
.
5.2 问题建模 . .
.
.
.
.
.
.
.
.
. .
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
.
. .
. . . . . . . . . . . . . . . . . . . . . . .
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
5.2.1 状态定义 . .
.
. . . . . . . . . . . . . . . . . . . . . .
.
5.2.2 动作空间设计 . .
.
. . . . . . . . . . . . . . . . . . . . . .
5.2.3 状态转移函数 . .
. . . . . . . . . . . . . . . . . . . . . .
5.2.4 奖赏函数的设计 . .
. . . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . .
5.3.1 Loss Function . .
. . . . . . . . . . . . . . . . . . . . . .
5.3.2 Actor-crtitic ⽅法 . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
.
.
.
.
. .
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5.3 算法设计 . .
5.4 理论分析 . .
5.5 实验效果 . .
.
5.6 总结 . .
.
.
.
.
.
第六章 基于强化学习分层流量调控
. .
.
6.1 背景 . .
.
6.2 问题建模 . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
6.2.1 Dynamic Action Boundary by CEM . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
6.3 实验效果 . .
.
6.4 总结与展望 . .
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
第七章 风险商品流量调控
.
7.1 背景 . .
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
7.1.1 为什么进⾏风险商品流量调控 . . . . . . . . . . . . . . . .
. .
. .
.
.
.
60
61
62
64
64
65
65
66
69
69
69
70
71
71
72
72
73
74
75
75
77
78
80
80
81
81
81
. IV .
⽬ 录
7.1.2 为什么使⽤强化学习调控 . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
7.2 基于强化学习的问题建模 .
. . . . . . . . . . . . . . . . . . . . . .
7.2.1 状态空间的定义 . .
. . . . . . . . . . . . . . . . . . . . . .
7.2.2 动作空间的定义 . .
. . . . . . . . . . . . . . . . . . . . . .
7.2.3 奖赏函数的定义 . .
. . . . . . . . . . . . . . . . . . . . . .
.
7.2.4 模型选择 . .
.
. . . . . . . . . . . . . . . . . . . . . .
7.2.5 奖赏函数 scale . .
.
.
. .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
.
.
7.3 流量调控系统架构 .
7.4 线上效果 . .
.
.
.
.
.
.
.
.
第八章 虚拟淘宝
.
8.1 背景 . .
.
.
.
.
.
.
.
.
.
. .
. .
. . . . . . . . . . . . . . . . . . . . . .
8.1.1 强化学习⾯临的问题 . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
8.1.2 虚拟淘宝 . .
8.2 学习⽤户⾏为：监督学习 .
. . . . . . . . . . . . . . . . . . . . . .
8.3 学习⽤户意图：逆强化学习 . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
8.3.1 逆强化学习概述 . .
8.3.2 学习⽤户意图 . .
. . . . . . . . . . . . . . . . . . . . . .
.
8.3.3 ⽣成对抗式模仿学习 . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
8.4 构建⽤户⾏为模拟器 . .
.
.
.
8.4.1 问题建模 . .
8.4.2 算法设计 . .
8.4.3 实验结果 . .
.
.
.
.
.
.
.
.
.
.
第九章 组合优化视角下基于强化学习的精准定向广告 OCPC 业务优化
.
.
9.1 背景 . .
.
9.2 问题建模 . .
.
.
9.2.1 奖赏 .
9.2.2 动作 .
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
. .
. .
. .
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
.
82
82
82
84
84
85
86
87
87
89
89
89
89
89
90
91
91
92
92
92
94
95
96
96
97
97
97
.
.
.
.
.
.
.
.
.
.
.
.
9.3 建模粒度 . .
9.4 模型选择 . .
9.5 探索学习 . .
9.6 业务实战 . .
9.2.3 状态定义 . .
.
.
.
.
9.6.1 系统设计 . .
9.6.2 奖赏设计 . .
9.6.3 实验效果 . .
.
9.7 总结与展望 . .
.
.
⽬ 录
. V .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . . . .
.
98
. . . . . . . . . . . . . . . . . . . . . . . 101
. . . . . . . . . . . . . . . . . . . . . . . 104
. . . . . . . . . . . . . . . . . . . . . . . 105
. . . . . . . . . . . . . . . . . . . . . . . 106
. . . . . . . . . . . . . . . . . . . . . . 106
.
.
. . . . . . . . . . . . . . . . . . . . . . 108
.
. . . . . . . . . . . . . . . . . . . . . . 109
. . . . . . . . . . . . . . . . . . . . . . . 109
.
.
.
.
.
.
.
第十章 策略优化方法在搜索广告排序和竞价机制中的应用
111
. . . . . . . . . . . . . . . . . . . . . . . 111
10.1 业务背景 . .
10.2 ⼴告排序和竞价的数学模型和优化⽅法 . . . . . . . . . . . . . . . 112
10.3 ⾯向⼴告商、⽤户和平台收益的排序公式设计 . . . . . . . . . . . 114
10.4 系统简介 . .
. . . . . . . . . . . . . . . . . . . . . . . 115
.
10.4.1 离线仿真模块 . .
. . . . . . . . . . . . . . . . . . . . . . 115
.
10.4.2 离线强化学习进⾏排序策略模型初始化 . . . . . . . . . . 117
. . . . . . . . . . . . . . . . . . . . . . 118
. . . . . . . . . . . . . . . . . . . . . . . 121
. . . . . . . . . . . . . . . . . . . . . . 123
10.5 在线排序策略模型优化 . .
10.6 实验分析 . .
10.7 总结 . .
.
.
.
. .
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
第十一章 TaskBot －阿里小蜜的任务型问答技术
.
.
.
.
.
.
.
11.1 背景和问题建模 . .
.
11.2 模型设计 . .
.
.
11.2.1 Intent Network . .
11.2.2 Belief Tracker
. .
11.2.3 Policy Network . .
11.2.4 模型 .
.
.
.
11.3 业务实战 . .
. .
.
.
.
.
.
.
.
.
124
. . . . . . . . . . . . . . . . . . . . . . 124
.
. . . . . . . . . . . . . . . . . . . . . . . 125
. . . . . . . . . . . . . . . . . . . . . . 125
.
.
. . . . . . . . . . . . . . . . . . . . . . 126
. . . . . . . . . . . . . . . . . . . . . . 127
.
.
. . . . . . . . . . . . . . . . . . . . . . 128
. . . . . . . . . . . . . . . . . . . . . . . 129
. VI .
⽬ 录
11.4 总结 . .
.
.
.
.
.
.
.
. .
. .
. . . . . . . . . . . . . . . . . . . . . . 130
第十二章 DRL 导购－阿里小蜜的多轮标签推荐技术
.
.
.
.
12.1 背景 . .
.
.
12.2 算法框架 . .
.
12.3 深度强化学习模型 .
. .
. .
.
.
.
. .
.
12.3.1 强化学习模块 . .
.
12.3.2 最终模型 . .
.
.
.
.
12.4 业务实战 . .
.
12.5 总结和展望 . .
.
.
.
.
.
.
.
.
131
. . . . . . . . . . . . . . . . . . . . . . 131
. . . . . . . . . . . . . . . . . . . . . . . 133
. . . . . . . . . . . . . . . . . . . . . . 135
.
.
. . . . . . . . . . . . . . . . . . . . . . 136
.
. . . . . . . . . . . . . . . . . . . . . . 137
. . . . . . . . . . . . . . . . . . . . . . . 138
. . . . . . . . . . . . . . . . . . . . . . . 138
.
.
.
.
.
.
第一章 基于强化学习的实时搜索排
序策略调控
1.1 背景
淘宝的搜索引擎涉及对上亿商品的毫秒级处理响应，⽽淘宝的⽤户不仅数
量巨⼤，其⾏为特点以及对商品的偏好也具有丰富性和多样性。因此，要让搜
索引擎对不同特点的⽤户做出针对性的排序，并以此带动搜索引导的成交提升，
是⼀个极具挑战性的问题。传统的 Learning to Rank（LTR）⽅法主要是在商品
维度进⾏学习，根据商品的点击、成交数据构造学习样本，回归出排序权重。尽
管 Contextual LTR ⽅法可以根据⽤户的上下⽂信息对不同的⽤户给出不同的排
序结果，但它没有考虑到⽤户搜索商品是⼀个连续的过程。这⼀连续过程的不
同阶段之间不是孤⽴的，⽽是有着紧密的联系。换句话说，⽤户最终选择购买
或不够买商品，不是由某⼀次排序所决定，⽽是⼀连串搜索排序的结果。
实际上，如果把搜索引擎看作智能体（Agent）、把⽤户看作环境（Environ-
ment），则商品的搜索问题可以被视为典型的顺序决策问题（Sequential Decision-
making Problem）：
(1) ⽤户每次请求 PV 时，Agent 做出相应的排序决策，将商品展⽰给⽤户；
(2) ⽤户根据 Agent 的排序结果，给出点击、翻页等反馈信号；
(3) Agent 接收反馈信号，在新的 PV 请求时做出新的排序决策；
. 2 .
第⼀章 基于强化学习的实时搜索排序策略调控
(4) 这样的过程将⼀直持续下去，直到⽤户购买商品或者退出搜索。
以前向视⾓（Forward View）来看，⽤户在每个 PV 中的上下⽂状态与之前所
有 PV 中的上下⽂状态和 Agent 的⾏为有着必然因果关系，同⼀个 PV 中 Agent
采取的不同排序策略将使得搜索过程朝不同的⽅向演进；反过来，以后向视⾓
（Backward View）来看，在遇到相同的上下⽂状态时，Agent 就可以根据历史演
进的结果对排序策略进⾏调整，将⽤户引导到更有利于成交的 PV 中去。Agent
每⼀次策略的选择可以看成⼀次试错（Trial-and-Error），在这种反复不断地试错
过程中，Agent 将逐步学习到最优的排序策略。⽽这种在与环境交互的过程中进
⾏试错的学习，正是强化学习（Reinforcement Learning，RL）的根本思想。
强化学习最早可以追溯到巴甫洛夫的条件反射实验，它从动物⾏为研究和
优化控制两个领域独⽴发展，最终经 Bellman 之⼿将其抽象为马尔可夫决策过
程（Markov Decision Process，MDP）问题⽽完成形式化。对于环境反馈的有利
奖赏，Agent 将强化引发这种奖赏的动作，并在以后与环境交互的过程中更偏向
于执⾏该动作。我们尝试将强化学习⽅法引⼊商品的搜索排序中，以优化⽤户
在整个搜索过程中的收益为⽬标，根据⽤户实时⾏为反馈进⾏学习，实现商品
排序的实时调控。图1.1⽐较直观地展⽰了的⽤强化学习来优化搜索排序的过程。
如图所⽰，在三次 PV 请求之间，Agent 做出了两次排序决策（a1 和 a2），从⽽
引导了两次 PV 展⽰。从效果上来看，a1 对应 PV 中并没有发⽣商品点击，⽽ a2
对应 PV 上发⽣了 3 次商品点击。如果将商品点击看成是对排序策略的反馈信
号，那么 Agent 第⼆次执⾏的排序策略 a2 将得到正向的强化激励，⽽其第⼀次
排序策略 a1 得到的激励为零。本⽂接下来将对我们具体的⽅案进⾏详细介绍。
图 1.1: 搜索的序列决策模型
1.2 问题建模
1.2 问题建模
. 3 .
1.2.1 强化学习简介
马尔可夫决策过程（Markov Decision Process，MDP）是强化学习的最基本
理论模型 [33]。⼀般地，MDP 可以由⼀个四元组 <S, A, R, T> 表⽰：
(1) S 为状态空间（State Space），包含了 Agent 可能感知到的所有环境状态；
(2) A 为动作空间（Action Space），包含了 Agent 在每个状态上可以采取的
所有动作；
(3) R : S (cid:2) A (cid:2) S ! R 为奖赏函数（Reward Function），R(s; a; s
) 表⽰在
′ 时，Agent 从环境获得的奖赏值；
(4) T : S (cid:2) A(cid:2) S ! [0; 1] 为环境的状态转移函数 (State Transition Function)，
状态 s 上执⾏动作 a，并转移到状态 s
′
T (s; a; s
) 表⽰在状态 s 上执⾏动作 a，并转移到状态 s
′
′ 的概率。
图 1.2: 强化学习 agent 和环境交互
在 MDP 中，Agent 和环境之间的交互过程可如图1.2所⽰：Agent 感知当前
环境状态 st，从动作空间 A 中选择动作 at 执⾏；环境接收 Agent 所选择的动作
之后，给以 Agent 相应的奖赏信号反馈 rt+1，并转移到新的环境状态 st+1，等待
Agent 做出新的决策。在与环境的交互过程中，Agent 的⽬标是找到⼀个最优策
. 4 .
第⼀章 基于强化学习的实时搜索排序策略调控
(cid:3)，使得它在任意状态 s 和任意时间步骤 t 下，都能够获得最⼤的长期累积
略 (cid:25)
奖赏，即
1∑
= argmax(cid:25)E(cid:25)f
(cid:3)
(cid:25)
k=0
(cid:13)krt+kjst = sg;8s 2 S;8t (cid:21) 0:
(1.1)
在这⾥，(cid:25) : S (cid:2) A ! [0; 1] 表⽰ Agent 的某个策略（即状态到动作的概率分布），
E(cid:25) 表⽰策略 (cid:25) 下的期望值，(cid:13) 2 [0; 1) 为折扣率（Discount Rate），k 为未来时间
步骤，rt+k 表⽰ Agent 在时间步骤 (t + k) 上获得的即时奖赏。
强化学习主要通过寻找最优状态值函数（Optimal State Value Function）
(cid:3)
V
(s) = max
(cid:25)
E(cid:25)f
(cid:13)krt+kjst = sg;8s 2 S;8t (cid:21) 0
(1.2)
1∑
k=0
1∑
k=0
或最优状态动作值函数（Optimal State-Action Value Function）
(cid:3)
Q
(s; a) = max
(cid:25)
E(cid:25)f
(cid:13)krt+kjst = s; at = ag;8s 2 S;8a 2 A;8t (cid:21) 0
(1.3)
(cid:3)。经典的强化学习算法（如：SARSA, Q-learning 等）将值函数
来学习最优策略 (cid:25)
⽤状态空间到动作空间的⼀张表来进⾏表达，并通过⼴义策略迭代（Generalized
(cid:3)。然⽽，在
Policy Iteration）⽅法对最优值函数进⾏逼近，从⽽得到最优策略 (cid:25)
⼤规模状态/动作空间问题（包括连续状态/动作空间问题）中，值表形式的值函
数所需要的存储空间远远超过了现代计算机的硬件条件，使得这些经典的算法
不再适⽤。这也即强化学习中的著名的“维度灾难”问题。
值函数估计（Value Function Approximation）是解决维度灾难问题的主要⼿
段之⼀，其主要思想是将状态值函数或动作值函数进⾏参数化，将值函数空间
转化为转化为参数空间，达到泛化 (Generalization) 的⽬的。以状态值函数为例，
在参数向量 (cid:18) 下，任意状态 s 的值 V (s) 可以表达为
V(cid:18)(s) = f(cid:18)(ϕ(s)):
(1.4)
1.2 问题建模
. 5 .
其中，ϕ(s) 为状态 s 的特征向量，f 为定义在状态特征空间上的某个函数，其具
体形式取决于算法本⾝。因此，对值函数 V (s) 的学习也就转化为了对参数向量
(cid:18) 的学习。基于对函数 f 采⽤的不同形式，强化学习领域也发展出了不同的⼦
分⽀，这其中包括：线性函数估计⽅法，回归树⽅法，神经⽹络⽅法，基于核的
强化学习⽅法等。值得⼀提的是，深度强化学习（Deep Reinforcement Learning，
DRL）本质上属于采⽤神经⽹络作为值函数估计器的⼀类⽅法，其主要优势在
于它能够利⽤深度神经⽹络对状态特征进⾏⾃动抽取，避免了⼈⼯定义状态特
征带来的不准确性，使得 Agent 能够在更原始的状态上进⾏学习。
1.2.2 状态定义
在我们的⽅案中，⽤户被视为响应 Agent 动作的环境，Agent 需要感知环境
状态进⾏决策。因此，如何定义环境状态使其能够准确反映出⽤户对商品的偏
好是⾸要问题。假设⽤户在搜索的过程中倾向于点击他感兴趣的商品，并且较
少点击他不感兴趣的商品。基于这个假设，我们将⽤户的历史点击⾏为作为抽
取状态特征的数据来源。具体地，在每⼀个 PV 请求发⽣时，我们把⽤户在最近
⼀段时间内点击的商品的特征（包括：价格、转化率、销量等）作为当前 Agent
感知到的状态，令 s 代表状态，则有
s = (price1; cvr1; sale1; :::; pricen; cvrn; salen):
(1.5)
其中，n 表⽰历史点击商品的个数，为可变参数，pricei、cvri、salei 分别代表
商品 i（0 (cid:20) i (cid:20) n）的价格、转化率和销量。另外，为了区别不同群体的⽤户，
我们还将⽤户的长期特征加⼊到了状态的定义中，最终的状态定义为
s = (price1; cvr1; sale1; :::; pricen; cvrn; salen; power; item; shop):
(1.6)
其中，power、item 和 shop 分别代表⽤户的购买⼒、偏好宝贝以及偏好店铺特
征。在具体算法实现时，由于状态特征不同维度的尺度不⼀样，我们会将所有
维度的特征值归⼀化到 [0; 1] 区间内，再进⾏后续处理。
. 6 .
第⼀章 基于强化学习的实时搜索排序策略调控
1.2.3 奖赏函数设定
当状态空间 S 和动作空间 A 确定好之后（动作空间即 Agent 能够选择排序
策略的空间），状态转移函数 T 也随即确定，但奖赏函数 R 仍然是个未知数。奖
赏函数 R 定义的是状态与动作之间的数值关系，⽽我们要解决的问题并⾮是⼀
个天然存在的 MDP，这样的数值关系并不存在。因此，另⼀个重要的步骤是把
我们要达到的⽬标（如：提⾼点击率、提⾼ GMV 等）转化为具体的奖赏函数 R，
在学习过程中引导 Agent 完成我们的⽬标。
幸运的是，这样的转化在我们的场景中并不复杂。如前所述，Agent 给出商
品排序，⽤户根据排序的结果进⾏的浏览、商品点击或购买等⾏为都可以看成
对 Agent 的排序策略的直接反馈。我们采取的奖赏函数定义规则如下：
(1) 在⼀个 PV 中如果仅发⽣商品点击，则相应的奖赏值为⽤户点击的商品的数
量；
(2) 在⼀个 PV 中如果发⽣商品购买，则相应奖赏值为被购买商品的价格；
(3) 其他情况下，奖赏值为 0。
从直观上来理解，第⼀条规则表达的是提⾼ CTR 这⼀⽬标，⽽第⼆条规则表达
的则是提⾼ GMV。在第四章中，我们将利⽤奖赏塑形（Reward Shaping）⽅法
对奖赏函数的表达进⾏丰富，提⾼不同排序策略在反馈信号上的区分度。
1.3 算法设计
1.3.1 策略函数
在搜索场景中，排序策略实际上是⼀组权重向量，我们⽤ (cid:22) = ((cid:22)1; (cid:22)2; :::; (cid:22)m)
来表⽰。每个商品最终的排序次序是由其特征分数和排序权重向量 (cid:22) 的内积所
决定的。⼀个排序权重向量是 Agent 的⼀个动作，那么排序权重向量的欧式空
间就是 Agent 的动作空间。根据对状态的定义可知，我们的状态空间也是连续
1.3 算法设计
. 7 .
的数值空间。因此，我们⾯临的问题是在两个连续的数值空间中学习出最优的
映射关系。
策略逼近（Policy Approximation）⽅法是解决连续状态/动作空间问题的有
效⽅法之⼀。其主要思想和值函数估计⽅法类似，即⽤参数化的函数对策略进
⾏表达，通过优化参数来完成策略的学习。通常，这种参数化的策略函数被称为
Actor。我们采⽤确定性策略梯度算法（Deterministic Policy Gradient，DPG）算
法来进⾏排序的实时调控优化。在该算法中，Actor 的输出是⼀个确定性的策略
（即某个动作），⽽⾮⼀个随机策略（即动作的概率分布）。对于连续动作空间问
题，确定性策略函数反⽽让策略改进（Policy Improvement）变得更加⽅便了，因
为贪⼼求最优动作可以直接由函数输出。
我们采⽤的 Actor 以状态的特征为输⼊，以最终⽣效的排序权重分为输出。
假设我们⼀共调控 m（m (cid:21) 0）个维度的排序权重，对于任意状态 s 2 S，Actor
对应的输出为
(cid:22)(cid:18)(s) = ((cid:22)1
(cid:18)(s); (cid:22)2
(cid:18)(s); :::; (cid:22)m
(cid:18) (s)):
(1.7)
其中，(cid:18) = ((cid:18)1; (cid:18)2; …; (cid:18)m) 为 actor 的参数向量，对于任意 i(1≤i≤m)，(cid:22)i
维的排序权重分，具体地有
(cid:18)(s) 为第 i
:
(1.8)
∑
(cid:22)i
(cid:18) =
⊤
Ci exp((cid:18)
i ϕ(s))
j=1 exp((cid:18)
⊤
j ϕ(s))
m
在这⾥，ϕ(s) 为状态 s 的特征向量，(cid:18)1; (cid:18)2; :::; (cid:18)m 均为长度与 ϕ(s) 相等的向量，
Ci 为第 i 维排序权重分的常数，⽤来对其量级进⾏控制（不同维度的排序权重
分会有不同的量级）。
1.3.2 策略梯度
回顾⼀下，强化学习的⽬标是最⼤化任意状态 s 上的长期累积奖赏（参考
(cid:3) 的定义）。实际上，我们可以⽤⼀个更⼀般的形式来表达这⼀⽬标，
(cid:3) 和 Q
对 V
. 8 .
即
第⼀章 基于强化学习的实时搜索排序策略调控
1∑
(cid:13)t(cid:0)1p0(s
′
′
)T (s
; (cid:22)(cid:18)(s
′
); s)R(s; (cid:22)(cid:18)(s)) ds
′ ds
∫
S
∫
∫
∑1
S
J((cid:22)(cid:18)) =
S
t=1
(cid:26)(cid:22)(s)R(s; (cid:22)(cid:18)) ds
=
= Es(cid:24)(cid:26)(cid:22)[R(s; (cid:22)(cid:18)(s))]:
′
′
∫
(1.9)
′
S
)T (s
; (cid:22)(cid:18)(s
t=1 (cid:13)t(cid:0)1p0(s
′ 表⽰状态 s 在⼀直持续的学
其中，(cid:26)(cid:22)(s) =
习过程中被访问的概率，p0 为初始时刻的状态分布，T 为环境的状态转移函数。
不难推测，J((cid:22)(cid:18)) 实际上表达的是在确定性策略 (cid:22)(cid:18) 的作⽤下，Agent 在所有状态
上所能够获得的长期累积奖赏期望之和。通俗地讲，也就是 Agent 在学习过程
中得到的所有奖赏值。
); s) ds
显然，为了最⼤化 J((cid:22)(cid:18))，我们需要求得 J((cid:22)(cid:18)) 关于参数 (cid:18) 的梯度，让 (cid:18) 往
梯度⽅向进⾏更新。根据策略梯度定理（Policy Gradient Theorem），J((cid:22)(cid:18)) 关于
(cid:18) 的梯度为
∫
∇(cid:18)J((cid:22)(cid:18)) =
(cid:26)(cid:22)(s)∇(cid:18)(cid:22)(cid:18)(s)∇aQ(cid:22)(s; a)ja = (cid:22)(cid:18)(s) ds
S
= Es(cid:24)(cid:26)(cid:22)[∇(cid:18)(cid:22)(cid:18)(s)∇aQ(cid:22)(s; a)ja = (cid:22)(cid:18)(s)]:
(1.10)
其中，Q(cid:22)(s; a) 为策略 (cid:22)(cid:18) 下状态动作对（State-Action Pair）(s; a) 对应的长期累
积奖赏。因此，参数 (cid:18) 的更新公式可以写为
(cid:18)t+1   (cid:18)t + (cid:11)(cid:18)∇(cid:18)(cid:22)(cid:18)(s)∇aQ(cid:22)(s; a)ja = (cid:22)(cid:18)(s):
(1.11)
在这个公式中，(cid:11)(cid:18) 为学习率，∇(cid:18)(cid:22)(cid:18)(s) 为⼀个 Jacobian Matrix，能够很容易地算
出来，但⿇烦的是 Q(cid:22)(s; a) 及其梯度 ∇aQ(cid:22)(s; a) 的计算。因为 s 和 a 都是连续
的数值，我们⽆法精确获取 Q(cid:22)(s; a) 的值，只能通过值函数估计⽅法进⾏近似
计算。我们采⽤线性函数估计⽅法（Linear Function Approximation，LFA），将
Q 函数⽤参数向量 w 进⾏表达：
Q(cid:22)(s; a) (cid:25) Qw(s; a) = ϕ(s; a)
⊤
w:
(1.12)
1.3 算法设计
. 9 .
在这⾥，ϕ(s; a) 为状态动作对 (s; a) 的特征向量。采⽤线性值函数估计的好处不
仅在于它的计算量⼩，更重要的是在它能让我们找到合适的 ϕ(s; a) 的表达，使
得 ∇aQw(s; a) 可以作为 ∇aQ(cid:22)(s; a) 的⽆偏估计。⼀个合适的选择是令 ϕ(s; a) =
⊤∇(cid:18)(cid:22)(cid:18)(s)，则可以得到
a
∇aQ(cid:22)(s; a) (cid:25) ∇aQw(s; a) = ∇a(a
⊤∇(cid:18)(cid:22)(cid:18)(s))
⊤
因此，策略函数的参数向量 (cid:18) 的更新公式可以写为
(cid:18)t+1   (cid:18)t + (cid:11)(cid:18)∇(cid:18)(cid:22)(cid:18)(s)(∇(cid:18)(cid:22)(cid:18)(s)
w = ∇(cid:18)(cid:22)(cid:18)(s)
⊤
w:
(1.13)
⊤
w):
(1.14)
1.3.3 值函数的学习
在更新策略函数 (cid:22)(cid:18) 的参数向量 (cid:18) 的同时，值函数 Qw 的参数向量 w 也需要
进⾏更新。最简单地，w 的更新可以参照 Q-learning 算法 [4, 5] 的线性函数估计
版本进⾏，对于样本 (st; at; rt; st+1)，有：
(cid:14)t+1 = rt + (cid:13)Qw(st+1; (cid:22)(cid:18)(st+1)) (cid:0) Qw(st; at)
t ((cid:13)ϕ(st+1; (cid:22)(cid:18)(st+1) (cid:0) ϕ(st; at))
⊤
= rt + w
wt+1 = wt + (cid:11)w(cid:14)t+1ϕ(st; at)
= wt + (cid:11)w(cid:14)t+1(a
∇(cid:18)(cid:22)(cid:18)(st)):
⊤
t
(1.15)
其中，st、at、rt 和 st+1 为 Agent 在 t 时刻感知的状态、所做的动作、从环境获得的
奖赏反馈和在 (t+1) 时刻感知的状态，(cid:14)t+1 被称作差分误差（Temporal-Difference
Error），(cid:11)w 为 w 的学习率。
需注意的是，Q-learning 的线性函数估计版本并不能保证⼀定收敛。并且，
在⼤规模动作空间问题中，线性形式的 Q 函数较难在整个值函数空间范围中精
确地估计每⼀个状态动作对的值。⼀个优化的办法是引⼊优势函数（Advantage
Function），将 Q 函数⽤状态值函数 V (s) 和优势函数 A(s; a) 的和进⾏表达。我
们⽤ V (s) 从全局⾓度估计状态 s 的值，⽤ A(s; a) 从局部⾓度估计动作 a 在状
态 s 中的相对于其他动作的优势。具体地，我们有
Q(s; a) = Aw(s; a) + V v(s) = (a (cid:0) (cid:22)(cid:18)(s))
⊤∇(cid:18)(cid:22)(cid:18)(s)
⊤
w + ϕ(s)
⊤
v:
(1.16)
. 10 .
第⼀章 基于强化学习的实时搜索排序策略调控
在这⾥，w 和 v 分别为 A 和 V 的参数向量。最后，我们将所有参数 (cid:18)、w 和 v
的更新⽅式总结如下：
⊤
= rt + (cid:13)ϕ(st+1)
(cid:14)t+1 = rt + (cid:13)Q(st+1; (cid:22)(cid:18)(st+1)) (cid:0) Q(st; at)
vt (cid:0) ((at (cid:0) (cid:22)(cid:18)(st))
wt)
(cid:18)t+1 = (cid:18)t + (cid:11)(cid:18)∇(cid:18)(cid:22)(cid:18)(st)(∇(cid:18)(cid:22)(cid:18)(st)
⊤
wt+1 = wt + (cid:11)w(cid:14)t+1ϕ(st; at) = wt + (cid:11)w(cid:14)t+1(a
⊤∇(cid:18)(cid:22)(cid:18)(st)
⊤
wt + ϕ(st)
⊤
vt)
∇(cid:18)(cid:22)(cid:18)(st))
⊤
t
(1.17)
vt+1 = vt + (cid:11)v(cid:14)t+1ϕ(st)
1.4 奖赏塑形
第⼆章定义的奖赏函数是⼀个⾮常简单化的版本，我们在初步的实验中构
造了⼀个连续状态空间/离散动作空间问题对其进⾏了验证。具体地，我们采⽤
了上⽂定义的状态表⽰⽅法，同时⼈⼯选取 15 组固定的排序策略，通过线性值
函数估计控制算法 GreedyGQ 来学习相应的状态动作值函数 Q(s; a)。从实验结
果中，我们发现学习算法虽然最终能够稳定地区分开不同的动作，但它们之间
的差别并不⼤。⼀⽅⾯，这并不会对算法收敛到最优产⽣影响；但另⼀个⽅⾯，
学习算法收敛的快慢却会⼤受影响，⽽这是在实际中我们必须要考虑的问题。
在淘宝主搜这种⼤规模应⽤的场景中，我们较难在短时间内观察到不同的
排序策略在点击和成交这样的宏观指标上的差别。因此，我们有必要在奖赏函
数中引⼊更多的信息，增⼤不同动作的区分度。以商品点击为例，考虑不同的
⽤户 A 和 B 在类似的状态中发⽣的商品点击⾏为。若 A 点击商品的价格较⾼，
B 点击商品的价格较低，那么就算 A 和 B 点击的商品数量相同，我们也可以认
为的对 A 和 B 采⽤的排序策略带来的影响是不同的。同样的，对于商品的成交，
价格相同⽽销量不同的商品成交也具有差别。因此，在原有的基础上，我们将商
品的⼀些属性特征加⼊到奖赏函数的定义中，通过奖赏塑形（Reward Shaping）
的⽅法丰富其包含的信息量。
奖赏塑形的思想是在原有的奖赏函数中引⼊⼀些先验的知识，加速强化学
′”
习算法的收敛。简单地，我们可以将“在状态 s 上选择动作 a，并转移到状态 s
的奖赏值定义为
1.4 奖赏塑形
′
R(s; a; s
) = R0(s; a; s
′
) + (cid:8)(s):
. 11 .
(1.18)
′
) 为原始定义的奖赏函数，(cid:8)(s) 为包含先验知识的函数，也被称
其中，R0(s; a; s
为势函数（Potential Function）。我们可以把势函数 (cid:8)(s) 理解学习过程中的⼦⽬
标（Local Objective）。例如，在⽤强化学习求解迷宫问题中，可以定义 (cid:8)(s) 为
状态 s 所在位置与出⼜的曼哈顿距离（或其他距离），使得 Agent 更快地找到潜
在的与出⼜更近的状态。根据上⾯的讨论，我们把每个状态对应 PV 的商品信息
纳⼊ Reward 的定义中，将势函数 (cid:8)(s) 定义为
ML(ij(cid:22)(cid:18)(s)):
(1.19)
K∑
(cid:8)(s) =
i=1
其中，K 为状态 s 对应 PV 中商品的个数，i 表⽰的第 i 个商品，(cid:22)(cid:18)(s) 为 Agent
在状态 s 执⾏的动作，ML(ij(cid:22)(cid:18)(s)) 表⽰排序策略为 (cid:22)(cid:18)(s) 时对商品的点击（或
成交）的极⼤似然（Maximum Likelihood）估计。因此，(cid:8)(s) 表⽰在状态 s 上执
⾏动作 (cid:22)(cid:18)(s) 时，PV 中所有商品能够被点击（或购买）的极⼤似然概率之和。
下⾯我们给出的 ML(ij(cid:22)(cid:18)(s)) 具体形式。令商品 i 的特征向量（即价格、销
⊤
i (cid:22)(cid:18)(s) 即为商品 i 在
量、⼈⽓分、实时分等特征）为 xi = (x1
状态 s 下的最终排序分数。又令 yi 2 f0; 1g 为商品 i 实际被点击（或成交）的
⊤
label，并假设意商品 i 的实际点击（或成交）的概率 pi 与其排序分数 x
i (cid:22)(cid:18)(s) 满
⾜ ln pi
1(cid:0)pi
ML = pi
⊤
i (cid:22)(cid:18)(s)，则商品 i 的似然概率为
yi(1 (cid:0) pi)1(cid:0)yi = (
)1(cid:0)yi:
i ; x2
i ; :::; xm
i )，则 x
(1.20)
= x
1
1 + exp((cid:0)x
⊤
i (cid:22)(cid:18)(s))
)yi(
1
1 + exp(x
⊤
i (cid:22)(cid:18)(s))
为简化计算，我们对 BL 取对数，得到对数似然概率
MLlog(ij(cid:22)(cid:18)(s)) = yix
i (cid:22)(cid:18)(s) (cid:0) ln(1 + exp(x
⊤
⊤
i (cid:22)(cid:18)(s))):
将 PV 中所有商品的对数似然概率综合起来，则有
K∑
(cid:8)(s) =
i=1
i (cid:22)(cid:18)(s) (cid:0) ln(1 + exp(x
⊤
⊤
i (cid:22)(cid:18)(s))):
yix
(1.21)
(1.22)
. 12 .
第⼀章 基于强化学习的实时搜索排序策略调控
我们最终实现的奖赏塑形⽅法将点击和成交均纳⼊考虑中，对于只有点击
的 PV 样本，其对应的奖赏势函数为
i (cid:22)(cid:18)(s) (cid:0) ln(1 + exp(x
⊤
⊤
i (cid:22)(cid:18)(s))):
yc
i x
(1.23)
K∑
(cid:8)clk(s) =
i=1
K∑
(cid:8)pay(s) =
i=1
i 是商品 i 被点击与否的 label。⽽对于有成交发⽣的 PV 样本，我们将商
其中，yc
品价格因素加⼊到奖赏势函数中，得到
i (cid:22)(cid:18)(s) (cid:0) ln(1 + exp(x
⊤
yp
i x
⊤
i (cid:22)(cid:18)(s))) + ln Pricei:
(1.24)
i 和 Pricei 分别是商品 i 被购买与否的 label 和它的价格。从直观上来理
其中，yp
解，(cid:8)clk(s) 和 (cid:8)pay(s) 将分别引导 Agent 对点击率和 GMV 的对数似然进⾏优化。
实际上，我们所采⽤的奖赏塑形⽅法来⾃于 LTR ⽅法的启发。LTR ⽅法
的有效性在于它能够利⽤商品维度的信息来进⾏学习，其最终学习到的排序权
重和商品特征有直接相关性。我们通过把商品的特征灌注到奖赏函数中，能让
Agent 的动作在具体商品上产⽣的影响得到刻画，因此也就能更好地在数值信
号上将不同的动作区分开来。另外，与以往的奖赏塑形⽅法不同的是，我们采
⽤的势函数是随着策略的学习变化的，它让 Reward 和 Action 之间产⽣了相互
作⽤：Action 的计算将朝着最⼤化 Reward 的⽅向进⾏，⽽ Action 的⽣效投放也
反过来影响了 Reward 的产⽣。因此，学习算法实际上是在⾮独⽴同分布的数据
上进⾏训练的，我们将在最后⼀章对该问题进⾏探讨。
1.5 实验效果
在双 11 期间，我们对强化学习⽅案进⾏了测试，下图展⽰了我们的算法在
学习的过程中的误差变化情况，衡量学习误差的指标为 Norm of the Expected TD
Update（NEU），是差分误差（TD Error）与状态动作特征向量乘积的期望值，图
中的 RNEU 表⽰ NEU 的平⽅根。从理论上来讲，RNEU 越⼩表⽰算法学习到的
策略越接近最优。
1.5 实验效果
. 13 .
图 1.3: Norm of the Expected TD Update
可以看到，从启动开始，每个桶上的 RNEU 开始逐渐下降。之后，下降趋
势变得⽐较缓和，说明学习算法在逐步往最优策略进⾏逼近。但过了零点之后，
每个桶对应的 RNEU 指标都出现了陡然上升的情况，这是因为 0 点前后⽤户的
⾏为发⽣了急剧变化，导致线上数据分布在 0 点以后与 0 点之前产⽣较⼤差别。
相应地，学习算法获取到新的 reward 信号之后，也会做出适应性地调整。
接下来，我们再对双 11 当天排序权重分的变化情况进⾏考查。我们⼀共
选取了若⼲个精排权重分来进⾏实时调控，下⾯两幅图分别展⽰了 iPhone 和
Android 中，每个维度的排序权重分在⼀天内的变化。
图 1.4: 每个维度的排序权重分在⼀天内的变化（iPhone）
从 0 点到早上 10:00 这⼀时间段内，⽆论是在 Android 端还是 iPhone 端，都
没有出现某个维度的排序权重分占绝对主导地位。在 11 号凌晨和上午，全⽹
. 14 .
第⼀章 基于强化学习的实时搜索排序策略调控
图 1.5: 每个维度的排序权重分在⼀天内的变化（Android）
⼤部分的成交其实不在搜索端，在这段时间内，⽤户产⽣的数据相对没有这么
丰富，可能还不⾜以将重要的排序权重分凸显出来。⽽到了 10:00 以后，我们
可以发现某⼀个维度的排序权重分逐渐开始占据主导，并且其主导过程⼀直持
续到了当天结束。在 iPhone 端占据主导的是某⼤促相关的分（绿⾊曲线），⽽
Android 端的则是某转化率的分（红⾊曲线）。这其实也从侧⾯说明了 iPhone 端
和 Android 端的⽤户⾏为存在较⼤差别。
在最终的投放效果上，强化学习桶相对于基准桶整体提升了很⼤幅度，同
时强化学习桶在 CTR ⽅⾯的提升⾼于其他绝⼤部分⾮强化学习桶，证明我们所
采⽤的奖赏塑形⽅法确实有效地将优化 CTR 的⽬标融⼊了奖赏函数中。
1.6 DDPG 与梯度融合
在双 11 之后，我们对之前的⽅案进⾏了技术升级，⼀个直接的优化是 DPG
升级为 DDPG，即将 actor 模型和 critic 模型升级为 actor ⽹络 (cid:22)(sj(cid:18)(cid:22)) 和 critic ⽹
络 Q(s; aj(cid:18)Q)。此外，我们还增加了⼀个 ltr loss 层 L(a; X; Y )，⽤于衡量 actor ⽹
络输出的 a，在 pointwise ltr 上的 cross entropy loss，这⾥ X = [x1; x2; :::; xn] 是 n
个宝贝的归⼀化的特征分向量，Y = [y1; y2; :::; yn] 是对应的点击、成交的 label。
具体地：
n∑
i
L(a; X; Y ) =
1
n
yi log((cid:27)(aT xi)) + (1 (cid:0) yi) log(1 (cid:0) (cid:27)(aT xi))
(1.25)
这⾥ (cid:27)(aT xi) = 1/(1 + exp((cid:0)aT xi))。因此最终的 actor 的⽹络的梯度为
1.6DDPG 与梯度融合
. 15 .
N∑
i
∇(cid:18)(cid:22)J = (cid:0) 1
N
∇aQ(s; aj(cid:18)Q)js=si;a=(cid:22)(si)∇(cid:18)(cid:22)(cid:22)(sj(cid:18)(cid:22))jsi+
(cid:21)∇aL(a; X; Y )ja=(cid:22)(si)∇(cid:18)(cid:22)(cid:22)(sj(cid:18)(cid:22))jsi
(1.26)
⼤致的整体框架如图1.6所⽰。
图 1.6: 监督学习和强化学习的多任务学习⽹络
这个整体实现，较之前的 DPG ⽅案，⼀⽅⾯可以受益于深度神经⽹络强⼤
的表征能⼒，另⼀⽅⾯也可以从监督学习⽹络获得很好的梯度，获得较好的初
始化，并保证整个训练过程中的稳定性。
. 16 .
第⼀章 基于强化学习的实时搜索排序策略调控
1.7 总结与展望
总的来说，我们将强化学习应⽤到淘宝的搜索场景中只是⼀次初步尝试，有
很多⽅⾯都需要进⼀步探索，现将我们在未来需要改进的地⽅以及可能的探索
⽅向归纳如下：
（1）状态的表⽰：我们将⽤户最近点击的商品特征和⽤户长期⾏为特征作
为状态，其实是基于这样的⼀个假设，即⽤户点击过的商品能够较为精确地反
映⽤户的内⼼活动和对商品的偏好。但实际上，⽤户对商品的点击通常具有盲
⽬性，⽆论什么商品可能都想要看⼀看。也就是说，我们凭借经验所设定的状
态并⾮那么准确。深度强化学习对状态特征的⾃动抽取能⼒是它在 Atari Game
和围棋上取得成功的重要原因之⼀。因此，在短期内可以考虑利⽤深度强化学
习对现有⽅案进⾏扩展。同时，借助深度神经⽹络对状态特征的⾃动抽取，我
们也可以发现⽤户的哪些⾏为对于搜索引擎的决策是⽐较重要的。
（2）奖赏函数的设定：和状态的定义⼀样，我们在第⼆章设定的奖赏函数也
来⾃于⼈⼯经验。奖赏塑形（Reward Shaping）虽然是优化奖赏函数的⽅法，但
其本质上也是启发式函数，其更多的作⽤在于对学习算法的加速。逆强化学习
（Inverse Reinforcement Learning，IRL）是避免⼈⼯设定的奖赏函数的有效途径
之⼀，也是强化学习研究领域的重要分⽀。IRL 的主要思想是根据已知的专家
策略或⾏为轨迹，通过监督学习的⽅法逆推出问题模型的奖赏函数。Agent 在这
样的奖赏函数上进⾏学习，就能还原出专家策略。对于我们的问题，IRL 的现
有⽅法不能完全适⽤，因为我们的搜索任务并不存在⼀个可供模仿的专家策略。
我们需要更深⼊思考如何在奖赏函数与我们的⽬标（提升 CTR，提升成交笔数）
之间建⽴紧密的关系。
（3）多智能体强化学习（MARL）：我们将搜索引擎看作 Agent，把⽤户看
成响应 Agent 动作的环境，属于典型的单智能体强化学习（Single-Agent RL）模
式。在单智能体强化学习的理论模型（即 MDP）中，环境动态（Environmental
Dynamics，也即奖赏函数和状态转移函数）是不会发⽣变化的；⽽在我们的问
题中，⽤户的响应⾏为却是⾮静态的，同时也带有随机性。因此，单智能体强化
学习的模式未必是我们的最佳⽅案。要知道，⽤户其实也是在⼀定程度理性控
1.7 总结与展望
. 17 .
制下的，能够进⾏⾃主决策甚⾄具有学习能⼒的 Agent。从这样的视⾓来看，或
许更好的⽅式是将⽤户建模为另外⼀个 Agent，对这个 Agent 的⾏为进⾏显式地
刻画，并通过多智能体强化学习 [21] ⽅法来达到搜索引擎 Agent 和⽤户 Agent
之间的协同（Coordination）。
′
（4）第四章的末尾提到了奖赏函数与 Agent 的动作的相互作⽤带来的⾮独
⽴同分布数据问题，我们在这⾥再进⾏⼀些讨论。在 MDP 模型中，奖赏函数
)（有时又写成 R(s; a)）是固定的，不会随着 Agent 策略的变化⽽变化。
R(s; a; s
然⽽，在我们提出的奖赏塑形⽅法中，势函数 (cid:8)clk(s) 和 (cid:8)pay(s) 中包含了策略
参数 (cid:18)，使得 Agent 从环境获得的奖赏信号在不同的 (cid:18) 下有所不同。这也意味着
我们的 Agent 实际上是处于⼀个具有动态奖赏函数的环境中，这种动态变化不
是来⾃于外部环境，⽽是源于 Agent 的策略改变。这有点类似于⼈的⾏为与世
界的相互作⽤。因此我们可以将 J((cid:22)(cid:18)) 重写为
∫
1∑
∫
∫
(cid:22)J((cid:22)(cid:18)) =
(cid:13)t(cid:0)1p0(s
′
′
)T (s
; (cid:22)(cid:18)(s
′
); s)R(cid:18)(s; (cid:22)(cid:18)(s)) ds
′ ds
(1.27)
S
S
t=1
=
(cid:26)(cid:22)(s)R(cid:18)(s; (cid:22)(cid:18)) ds:
S
其中，R(cid:18) 为 Agent 的策略参数 (cid:18) 的函数。虽然 J((cid:22)(cid:18)) 与 (cid:22)J((cid:22)(cid:18)) 之间只有⼀个符
号之差，但这微⼩的变化也许会导致现有的强化学习算法⽆法适⽤，我们在未
来的⼯作中将从理论上来深⼊研究这个问题的解决⽅法。
第二章 延迟奖赏在搜索排序场景中
的作用分析
2.1 背景
我们⽤强化学习（Reinforcement Learning，RL）在搜索场景中进⾏了许多的
尝试，例如：对商品排序策略进⾏动态调节、控制个性化展⽰⽐例、控制价格 T
变换等。虽然从顺序决策的⾓度来讲，强化学习在这些场景中的应⽤是合理的，
但我们并没有回答⼀些根本性的问题，⽐如：在搜索场景中采⽤强化学习和采
⽤多臂⽼虎机有什么本质区别？从整体上优化累积收益和分别独⽴优化每个决
策步骤的即时收益有什么差别？每当有同⾏问到这些问题时，我们总是⽆法给
出让⼈信服的回答。因为我们还没思考清楚⼀个重要的问题，即：在搜索场景的
顺序决策过程中，任意决策点的决策与后续所能得到的结果之间的关联性有多
大？从强化学习的⾓度讲，也就是后续结果要以多⼤的⽐例进⾏回传，以视为
对先前决策的延迟激励。也就是说我们要搞清楚延迟反馈在搜索场景中的作⽤。
本⽂将以继续以搜索场景下调节商品排序策略为例，对这个问题展开探讨。本
⽂余下部分的将组织如下：第⼆节对搜索排序问题的建模进⾏回顾，第三节将
介绍最近的线上数据分析结果，第四节将对搜索排序问题进⾏形式化定义，第
五节和第六节分别进⾏理论分析和实验分析并得出结论。
2.2 搜索排序问题回顾
. 19 .
2.2 搜索排序问题回顾
在淘宝中，对商品进⾏搜索排序以及重排序涉及搜索引擎与⽤户之间的不
断交互。图2.1展⽰了这样的交互过程：
(1) ⽤户进⼊搜索引擎，输⼊ query；
(2) 搜索引擎根据⽤户输⼊的 query 和⽤户的特征，从若⼲个可能的排序动
作中（a1; a2; a3; :::）选择其中⼀个给对应 query 下的商品进⾏排序，选
择 top K 个商品（K ⼀般等于 10）；
(3) ⽤户在看到展⽰页⾯的商品之后，会在页⾯中进⾏⼀些操作，⽐如：点
击、加购感兴趣的商品；
(4) 当⽤户进⾏翻页时，搜索引擎会再次选择⼀个排序动作，对未展⽰的商
品重新进⾏排序，并进⾏商品展⽰；
(5) 随着⽤户不断地翻页，这样的交互过程会⼀直进⾏下去，直到⽤户购买
某个商品或者离开搜索引擎。
如果把搜索引擎看作智能体（Agent）、把⽤户看做环境（Environment），那么
图2.1展⽰的交互过程对于搜索引擎 Agent 来讲是⼀个典型的顺序决策问题。若
从强化学习的视⾓来看，上所展⽰的过程就是⼀次 Episode，可以重新⽤图2.2进
⾏描述。在图2.2中，蓝⾊的节点表⽰⼀次 PV 请求，也对应 Agent 进⾏状态感
知的时刻，红⾊的节点表⽰ Agent 的动作，绿⾊箭头表⽰对 Agent 动作的即时
奖赏激励。
需要注意的是，由于搜索引擎每⼀次的决策都是在 PV 请求时发⽣的，所
以决策过程中的状态与展⽰的商品页是⼀⼀对应的。更严格地来讲，每⼀个决
策点的状态应该是“这个决策点之前所有商品页⾯包含的信息总和”，包括这些页
⾯展⽰的商品信息，以及⽤户在这些页⾯上的实时⾏为。在⽬前的系统实现中，
由于性能、信息获取条件的限制，现有的状态表⽰中并没有完全囊括这些信息。
但抛开具体的状态表⽰⽅法不谈，我们可以认为⼀个商品页就是⼀个状态。在
. 20 .
第⼆章 延迟奖赏在搜索排序场景中的作⽤分析
图 2.1: 搜索引擎与⽤户交互⽰意图
图 2.2: 搜索引擎 Agent 决策过程⽰意图
下⼀节中，我们将以 PV 为单位对线上数据进⾏统计分析，希望能够发现这个搜
索排序问题的⼀些特性。
2.3 数据统计分析
. 21 .
图 2.3: 全类⽬数据下所有成交 Episode 的长度分布情况
2.3 数据统计分析
在强化学习中，有很多算法都是分 Episode 进⾏训练的。所谓 Episode 是指
⼀次从初始状态（Initial State）到终⽌状态（Terminal State）之间的经历。我们对线
上产⽣的训练数据按照 Episode 进⾏了组织，也就是将每个 Episode 对应的所有
商品展⽰页进⾏串联，形成“PV->PV->...->End”的⼀个序列，相当于是把 Episode
中的所有 State 进⾏串联。其中，“End”表⽰⼀个 Episode 的终⽌状态。在我们的
场景中，终⽌状态表⽰“⽤户离开搜索引擎”或者“进⾏了购买”。由于我们在⽇志
中⽆法获取“⽤户离开搜索引擎”这样的事件，所以我们能够完整抽取 Episode 的
数据其实都是有成功购买的 PV 序列。令 n 表⽰ Episode 的序列长度，我们统计
了 n = 1; 2; :::; 30 的 Episode 占总体成交 Episode 的⽐例，结果如图2.3所⽰。
从图2.3的结果中，可以看到 Episode 的长度越⼤，其对应的占总体的⽐例
越⼩。这与“越往后的 PV 转化率越低”的经验是相符的。从绝对数值上看，超过
60% 的成交都是在前 6 个 PV 中发⽣的，⽽ n = 1、2、3 的⽐例更是分别超过
了 20%、15% 和 10%。当然，图2.4的结果来⾃于对全类⽬数据的统计。为了消
除类⽬间差异给统计结果带来的影响，我们选取了某这三个成交量较⼤的类⽬，
分别进⾏了相同的统计分析，相应的结果展⽰在图2.4、2.5、2.6中。
虽然分类⽬统计结果与全类⽬的结果在绝对数值上有⼀定差别，但还是呈
现出了相同的趋势。如果不考虑具体的数值，我们⾄少可以得出⼀个结论：用户
在看过任意数量的商品展示页之后，都有可能发生成交。根据这个结论，我们可
. 22 .
第⼆章 延迟奖赏在搜索排序场景中的作⽤分析
图 2.4: 类⽬ A 下所有成交 Episode 的长度分布情况
图 2.5: 类⽬ B 下所有成交 Episode 的长度分布情况
图 2.6: 类⽬ C 下所有成交 Episode 的长度分布情况
以将⼀次搜索会话过程⽤图2.7的抽象⽰意图来描述。如图所⽰，垂直⽅向的箭
2.3 数据统计分析
. 23 .
图 2.7: 仅考虑成交和翻页的搜索会话图⽰
图 2.8: 考虑成交、翻页和⽤户离开的搜索会话图⽰
头由上向下表⽰⽤户不停翻页的过程。每翻⼀页，⽤户选择商品的范围就增加
⼀页，PV 的 History 也对应地发⽣变化。横向地来看，⽤户在任意的 PV History
下，都有可能选择购买某个被展⽰的商品，或者继续往下翻页。当然，如果考虑
到⽤户也有可能离开搜索引擎，我们可以得到图2.8中的更⼀般的⽰意图。
在我们的场景中，“成交”和“离开搜索引擎”均被视为⼀个 Episode 的终⽌状
态。如果把图 8 和马尔可夫决策过程（MDP）的状态、状态转移等要素对应起
来，就可以发现搜索排序问题的明显特征：任意非终止状态都有一定的概率转
. 24 .
第⼆章 延迟奖赏在搜索排序场景中的作⽤分析
移到终止状态。这同⼀些典型的强化学习应⽤场景相⽐有很⼤不同。⽐如，在
⽹格世界和迷宫问题中，只有与邻近终点的位置才有⾮零的概率转移到终⽌状
态。在接下来的内容中，我们将根据搜索排序问题的特点对其进⾏形式化定义，
并在此基础上做相应的理论分析。
2.4 搜索排序问题形式化
本节提出搜索会话马尔科夫决策过程模型（Search Session Markov Decision
Process, SSMDP），作为对搜索排序问题的形式化定义。我们⾸先对搜索会话过
程中的上下⽂信息和⽤户⾏为进⾏建模，形式化定义商品页、商品页历史、成
交转化率等概念，它们是定义状态和状态转移关系的基础。
定义 1. [Top K List] 给定商品集合 D，排序函数 f，以及⼀个正整数 K
(1 (cid:20) K (cid:20) jDj)，关于 D 和 f 的 top K list，记为 LK(D; f )，是⽤函数 f 对 D 中商
品进⾏打分以后的前 K 个商品的有序列表 (I1;I2; :::;IK)。其中，Ik 是排在第 k
位的商品（1 (cid:20) k (cid:20) K），并且对于任意 k
′ (cid:21) k，都有 f (Ik) > f (Ik′)。
定义 2. [Item Page] 令 D 为关于某个 query 的商品全集，K (K > 0) 为⼀个
页⾯能够展⽰的商品数量。对于⼀个搜索会话的任意时间步 t (t (cid:21) 1)，其对应
的 item page pt 是关于的第 (t (cid:0) 1) 步的打分函数 at(cid:0)1 和未展⽰商品集合 Dt(cid:0)1 的
top K list LK(Dt(cid:0)1; at(cid:0)1)。对于初始时间步 t = 0，有 D0 = D。对于其他任意时
间步 t (cid:21) 1，有 Dt = Dt(cid:0)1 n pt。
定义 3. [Item Page History] 令 q 为⼀个搜索会话的 query。对于初始时间步
t = 0，对应的初始 item page history 为 h0 = q。对于任意其他时间步 t (cid:21) 1，对应
的 item page history 为 ht = (ht(cid:0)1; pt)。在这⾥，ht(cid:0)1 为第 (t (cid:0) 1) 步的 item page
history，pt 为第 t 步的 item page。
对于任意时间步骤 t，item page history ht 包含了⽤户在 t 时刻能够观察到的
所以上下⽂信息。由于商品全集 D 是⼀个有限集合，不难发现⼀个搜索会话最多
⌉ 个 item page。对于搜索引擎来讲，它在⼀个搜索会话中最多决策 ⌈jDj
⌉
包含 ⌈jDj
次。根据我们之前的数据分析，不同的⽤户会在不同的时间步上选择购买或者离
K
K
2.4 搜索排序问题形式化
. 25 .
开。如果我们把所有⽤户看作⼀个能够采样出不同⽤户⾏为的 environment，就意
味着这个 environment 可能会在任意时间步上以⼀定的成交转化概率（conversion
probability）或者放弃概率（abandon probability）来终⽌⼀个搜索会话。我们形
式化定义这两种概率如下。
定义 4. [Conversion Probability] 对于⼀个搜索会话中的任意 item page his-
tory ht (t > 0)，令 B(ht) 表⽰⽤户在观察到 ht 之后发⽣购买⾏为的随机事件，则
ht 的 conversion probability，记为 b(ht)，就是事件 B(ht) 在 ht 下发⽣的概率。
定义 5. [Abandon Probability] 对于⼀个搜索会话中的任意 item page history
ht (t > 0)，令 L(ht) 表⽰⽤户在观察到 ht 之后离开搜索会话的随机事件，则 ht
的 abandon probability，记为 l(ht)，就是事件 L(ht) 在 ht 下发⽣的概率。
由于 ht 是在 (t (cid:0) 1) 时刻的 item page history ht(cid:0)1 上执⾏动作 at(cid:0)1 的直接结
果，因此 b(ht) 和 l(ht) 也表征了 Agent 在 ht(cid:0)1 上执⾏动作 at(cid:0)1 之后环境状态的
转移：（1）以 b(ht) 的成交概率终⽌搜索会话；（2）以 l(ht) 的离开概率终⽌搜
索会话；（3）以 (1 (cid:0) b(ht) (cid:0) l(ht)) 的概率继续搜索会话。⽅便起见，我们对⽤
户继续进⾏搜索会话对概率也进⾏形式化描述。
定义 6. [Continuing Probability] 对于⼀个搜索会话中的任意 item page his-
tory ht (t > 0)，令 C(ht) 表⽰⽤户在观察到 ht 之后继续停留在会话中的随机事
件，则 ht 的 continuing probability，记为 c(ht)，就是事件 C(ht) 在 ht 下发⽣的
概率。
显然，对于任意 item page history h，都有 c(h) = 1 (cid:0) b(h) (cid:0) l(h) 成⽴。特殊
地，对于初始 item page history h0 来讲，C(h0) 是⼀个必然事件（即 c(h0) = 1）。
这是因为在第⼀个 item page 展⽰给⽤户前，不可能存在成交转化事件和离开事
件。
基于上⾯定义的⼏个概念，我们可以定义 Search Session MDP（SSMDP）如
下：定义 7. [Search Session MDP] 令 q 为某个 query，D 为和 q 相关的商品全集，
K 为⼀个页⾯可展⽰的商品数量，关于 q、D 和 K 的 Search Session MDP 是⼀
个元组，记为 M = ⟨T;H;S;A;R;P⟩。该元组中的每个要素分别为：
• T = ⌈jDj
K
⌉ 为搜索会话最⼤决策步数
. 26 .
第⼆章 延迟奖赏在搜索排序场景中的作⽤分析
∪
T
t=0
∪HB
Ht 为关于 q、D 和 K 的所有可能的 item page history 的集合，其
∪HL 为状态空间，HC = fC(ht)j8ht 2 Ht; 0 (cid:20) t < Tg 是包
• H =
中 Ht 为 t 时刻所有可能 item page history 的集合 (0 (cid:20) t (cid:20) T )
• S = HC
含所有的继续会话事件的⾮终⽌状态集合（nonterminal state set），HB =
fB(ht)j8ht 2 Ht; 0 < t (cid:20) Tg 和 HL = fL(ht)j8ht 2 Ht; 0 < t (cid:20) Tg 分别是
包含所有成交转化事件和离开事件的终⽌状态集合（terminal state set）
• A 为动作空间，包含搜索引擎所有可能的排序打分函数
• R : HC (cid:2) A (cid:2) S ! R 为奖赏函数
• P : HC (cid:2) A (cid:2) S ! [0; 1] 为状态转移函数，对于任意时间步 t（0 (cid:20) t < T ）、
任意 item page history ht 2 Ht 和任意动作 a 2 A，令 ht+1 = (ht;LK(Dt; a))，
′ 2 S 的概
则 agent 在状态 C(ht) 上执⾏动作 a 后，环境转移到任意状态 s
率为
8>>>>>><>>>>>>:
P(C(ht); a; s
′
) =
′
′
if s
if s
′
if s
b(ht+1)
l(ht+1)
c(ht+1)
0
= B(ht+1);
= L(ht+1);
= C(ht+1);
otherwise.
(2.1)
在⼀个 Search Session MDP 中，环境即是所有可能⽤户共同构成的总体，环
境的状态表征了⽤户总体在对应 item page history 下的动向（继续会话、成交或离
开）。环境状态的转移则直接基于我们之前定义的 conversion probability、abandon
probability 以及 continuation probability。奖赏函数 R 可以根据具体的业务⽬标进
⾏定义。基于让天下没有难做的⽣意的使命，也就是尽可能多地促进⽤户与卖
家之间的交易，我们给出如下的奖赏函数范例。对于任意时间步 t（0 (cid:20) t < T ）、
任意 item page history ht 2 Ht 和任意动作 a 2 A，令 ht+1 = (ht;LK(Dt; a))，则
agent 在状态 C(ht) 上执⾏动作 a 并且环境转移到任意状态 s
′ 2 S 的奖赏为
R(C(ht); a; s
′
) =
′
= B(ht+1);
if s
otherwise,
2.5 理论分析
8<:m(ht+1)
0
. 27 .
(2.2)
其中，m(ht+1) 表⽰ item page history ht+1 对应的成交均价。
2.5 理论分析
2.5.1 马尔可夫性质
上⼀节定义的 Search Session MDP（SSMDP）可以看作是 MDP 模型的⼀个
实例，但要保证 SSMDP 是良定义的，我们需要证明 SSMDP 中的状态都具有马
尔可夫性质（Markov Property）。马尔可夫性质指的是对于任意的状态动作序列
s0; a0; s1; a1; s2; :::; st(cid:0)1; at(cid:0)1; st，都有如下等式成⽴：
Pr(stjs0; a0; s1; a1; :::; st(cid:0)1; at(cid:0)1) = Pr(stjst(cid:0)1; at(cid:0)1):
(2.3)
也即是说，当前状态 st 的发⽣概率仅仅取决于最近⼀个状态动作对 (st(cid:0)1; at(cid:0)1)，
⽽并⾮整个序列。我们可以证明对于⼀个 SSMDP，它的所有状态都具有马尔可
夫性质。
命题 1. 对于任意 Search Session MDP M = ⟨T;H;S;A;R;P⟩，其状态空
间 S 中的任意状态都具有马尔可夫性质。证明: 我们只需证明对于任意时间步 t
(0 (cid:20) t (cid:20) T ) 和关于 t 的任意状态动作序列 s0; a0; s1; a1; :::; st(cid:0)1; at(cid:0)1; st，都有等
式 Pr(stjs0; a0; s1; a1; :::; st(cid:0)1; at(cid:0)1) = Pr(stjst(cid:0)1; at(cid:0)1) 成⽴即可。
除了状态 st 以外，序列 s0; a0; s1; a1; :::; st(cid:0)1; at(cid:0)1; st 中的其他所有状态都是
⾮终⽌状态（non-terminal state）。根据状态的定义，对于任意时间步 t
′
必然存在⼀个 item page history ht′ 与状态 st′ 相对应，且有 st′ = C(h(t
整个序列可以重写为 C(h0); a0; C(h1); a1; :::; C(ht(cid:0)1); at(cid:0)1; st。需注意的是，对于
任意时间步 t
< t)，
))。因此，
< t)，都有
′ (0 < t
′
′ (0 < t
′
ht′ = (ht′(cid:0)1;LK(Dt′(cid:0)1; at′(cid:0)1));
(2.4)
第⼆章 延迟奖赏在搜索排序场景中的作⽤分析
. 28 .
′ (cid:0) 1) 时刻的动作 at′(cid:0)1 和未展⽰商
成⽴。其中，LK(Dt′(cid:0)1; at′(cid:0)1) 也就是关于 (t
品 Dt′(cid:0)1 的 top K list。给定 ht′(cid:0)1，集合 Dt′(cid:0)1 ⼀定是确定的。所以，ht′ 也就是
状态动作对 (C(ht′(cid:0)1); at′(cid:0)1) 的必然和唯⼀结果。那么事件 (C(ht′(cid:0)1); at′(cid:0)1) 也就
能够等价地表⽰为事件 ht′。基于此，我们可以进⾏如下推导：
Pr(stjs0; a0; s1; a1; :::; st(cid:0)1; at(cid:0)1)
=Pr(stjC(h0); a0; C(h1); a1; :::; C(ht(cid:0)1); at(cid:0)1)
=Pr(stjh1; h2; :::; ht(cid:0)1; C(ht(cid:0)1); at(cid:0)1)
=Pr(stjht(cid:0)1; C(ht(cid:0)1); at(cid:0)1)
=Pr(stjC(ht(cid:0)1); at(cid:0)1)
=Pr(stjst(cid:0)1; at(cid:0)1):
(2.5)
第三步推导成⽴是由于对于任意时间步 t
似地，第四步推导成⽴是由于事件 C(ht(cid:0)1) 已经包含了 ht(cid:0)1 的发⽣。
< t)，ht′(cid:0)1 都包含在 ht′ 中。类
′ (0 < t
′
2.5.2 折扣率
在这⼀⼩节我们将讨论本⽂最重要的问题：延迟奖赏（delay reward）对于
搜索排序的优化到底有没有作⽤？简单地来说，也就是 Search Session MDP 的
折扣率（discount rate）到底应该设多⼤。在任意 MDP 中，折扣率 (cid:13) 的⼤⼩直接
决定了 future rewards 在 agent 的优化⽬标中所占⽐重。我们将分析优化长期累
积奖赏与优化搜索引擎的经济指标这两个⽬标之间的关系给出答案。
令 M = ⟨T;H;S;A;R;P⟩ 为⼀个关于 query q、商品全集 D 和正整数 K
(K > 0) 的 SSMDP。给定⼀个确定性策略 (cid:25) : S ! A，记每个时间步 t（0 (cid:20) t (cid:20) T ）
对应的 item page history 为 (cid:25) by h(cid:25)
t ，我们把在策略 (cid:25) 下能够访问的所有状态都
展⽰在图2.9中。
在这个图中，红⾊的节点表⽰ item page history，注意它们并不是 SSMDP 的
t ) 和 m(h(cid:25)
t )
状态。⽅便起见，在本⽂接下来的部分，我们将把 C(h(cid:25)
分别简化记为 C (cid:25)
t )、c(h(cid:25)
t )、b(h(cid:25)
t 、c(cid:25)
t 、b(cid:25)
t 和 m(cid:25)
t 。
2.5 理论分析
. 29 .
图 2.9: Agent 策略 (cid:25) 下能够访问 SSMDP 中的所有状态
不失⼀般性，我们设 SSMDP M 的折扣率为 (cid:13) (0 (cid:20) (cid:13) (cid:20) 1)。由于 SSMDP
是⼀个有限时间步 MDP（finite-horizon MDP），所以折扣率可以取到 1。对于任
意时间步 t（0 (cid:20) t < T ），状态 C (cid:25)
}
t 的 state value 为
(cid:12)(cid:12)C (cid:25)
{ T(cid:0)t∑
{
rt+1 + (cid:13)rt+2 + (cid:1) (cid:1) (cid:1) + (cid:13)T(cid:0)t(cid:0)1rT
(cid:13)k(cid:0)1rt+k
k=1
t
V (cid:25)
(cid:13) (C (cid:25)
t ) = E(cid:25)
= E(cid:25)
}
(cid:12)(cid:12)C (cid:25)
t
:
(2.6)
}
{
其中，对任意 k（1 (cid:20) k (cid:20) T (cid:0) t），rt+k 为 agent 在未来时刻 (t + k) 的 item page
history h(cid:25)
t+k 中收到的即时奖赏。根据我们奖赏函数的定义，rt+k 在策略 (cid:25) 下的
期望值为 E(cid:25)
t+k) 为 item page history
t+km(cid:25)
t 发⽣的条件下的长期累积奖
t+k 的成交价格期望。由于 V (cid:25)
h(cid:25)
t+k 的概率考虑进来。记
赏期望，所以我们还要把从 C (cid:25)
从状态 C (cid:25)
t+k。在这⾥，m(cid:25)
(cid:13) (C (cid:25)
t 到达 item page history h(cid:25)
t+k)，根据状态转移函数的定义可得
t+k 的概率为 Pr(C (cid:25)
t ) 表达的是在 C (cid:25)
t+k = m(h(cid:25)
t 到达 h(cid:25)
! h(cid:25)
= b(cid:25)
rt+k
t
8<:1:0
Pr(C (cid:25)
t
! h(cid:25)
t+k) =
k = 1;
(cid:5)k(cid:0)1
j=1 c(cid:25)
t+j 1 < k (cid:20) T (cid:0) t:
(2.7)
从状态 C (cid:25)
t 到 item page history h(cid:25)
t+1 的概率为 1 是因为 h(cid:25)
t+1 是状态动作对 (C (cid:25)
t ; (cid:25)(C (cid:25)
t ))
. 30 .
第⼆章 延迟奖赏在搜索排序场景中的作⽤分析
的直接结果。将上⾯的⼏个公式综合起来，我们可以进⼀步计算 V (cid:25)
(cid:13) (C (cid:25)
t ) 如下：
V (cid:25)
(cid:13) (C (cid:25)
t ) = E(cid:25)
{
T(cid:0)t∑
(cid:12)(cid:12)C (cid:25)
t
rt+1
(cid:13)k(cid:0)1Pr(C (cid:25)
=
k=1
= b(cid:25)
t+1m(cid:25)
rt+2
t+k)b(cid:25)
}
{
+ (cid:13)E(cid:25)
! h(cid:25)
T(cid:0)t∑
t+1b(cid:25)
(cid:13)k(cid:0)1
t
((
k=2
t+1 + (cid:13)c(cid:25)
t+2m(cid:25)
= b(cid:25)
t+1m(cid:25)
t+1 +
}
(cid:12)(cid:12)C (cid:25)
t
+ (cid:1) (cid:1) (cid:1) + (cid:13)T(cid:0)t(cid:0)1E(cid:25)
rT
t+k
t+km(cid:25)
t+2 + (cid:1) (cid:1) (cid:1) + (cid:13)T(cid:0)t(cid:0)1
(cid:5)k(cid:0)1
j=1 c(cid:25)
t+km(cid:25)
b(cid:25)
)
t+j
t+k
(
)
(cid:5)T(cid:0)t(cid:0)1
j=1
:
{
}
(cid:12)(cid:12)C (cid:25)
)
t
(2.8)
c(cid:25)
t+j
b(cid:25)
T m(cid:25)
T
根据图2.9中展⽰的每个 item page history 的 conversion probability 以及成交
价格期望，我们也可以将搜索引擎在策略 (cid:25) 的作⽤下在⼀个搜索会话中引导的
成交额期望表达出来，即
(
)
E(cid:25)
gmv = b(cid:25)
1 m(cid:25)
1 + c(cid:25)
= b(cid:25)
1 m(cid:25)
1 +
1 b(cid:25)
T∑
(
)
2 + (cid:1) (cid:1) (cid:1) +
2 m(cid:25)
(cid:5)k(cid:0)1
j=1 c(cid:25)
j
k m(cid:25)
b(cid:25)
k :
(cid:5)T
k=1c(cid:25)
k
b(cid:25)
T m(cid:25)
T
(2.9)
k=2
(cid:13) (C (cid:25)
(cid:13) (C (cid:25)
gmv。
(cid:13) ，不难发现当折扣率 (cid:13) = 1 时，有 E(cid:25)
通过⽐较 Egmv 和 V (cid:25)
0 ) 成⽴。也
就是说，当 (cid:13) = 1 时，最⼤化长期累积奖赏将直接带来搜索引擎成交额的最⼤
化。当 (cid:13) < 1 时，由于 E(cid:25)
(cid:13) 并不⼀定能够
最⼤化 E(cid:25)
0 ) 的上界，所以最⼤化 V (cid:25)
gmv 是 V (cid:25)
gmv = V (cid:25)
命题 2. 令 M = ⟨T;H;S;A;R;P⟩ 为任意 Search Session MDP。对于任意确
gmv 成⽴，
(cid:13) 为 agent 在策略 (cid:25) 和折扣率 (cid:13) 下的状态值函数，C(h0) 为搜索会话的初始
gmv 为搜索引擎在策略 (cid:25) 下的单次搜索会话成交额期望。仅当 (cid:13) = 1 时，有
定性策略 (cid:25) : S ! A 和折扣率 (cid:13)（0 (cid:20) (cid:13) (cid:20) 1），都有式⼦ V (cid:25)
(cid:13) (C(h0)) (cid:20) E(cid:25)
其中 V (cid:25)
(
∑
状态，E(cid:25)
(cid:13) (C(h0)) = E(cid:25)
V (cid:25)
k=2(1 (cid:0) (cid:13)k(cid:0)1)
成⽴。这是显然的，因为⼆者之差，即
时⼀定为正。
gmv 成⽴。证明: 我们只需证明当 (cid:13) < 1 时，有 V (cid:25)
(cid:5)k(cid:0)1
j=1 c(cid:25)
j
(cid:13) (C(h0)) < E(cid:25)
k 在 (cid:13) < 1
b(cid:25)
k m(cid:25)
)
gmv
T
⾄此，我们可以回答之前提出的问题：站在提⾼搜索引擎成交额的⾓度，搜
索排序问题中考虑延迟奖赏是必要且必须的。从理论上，这是因为最⼤化⽆折
2.6 实验分析
. 31 .
扣累积奖赏能够直接优化搜索引擎的成交额。究其深层原因，是因为⽤户在搜
索商品的每个步骤（即每个 item page history）的⾏为都是基于之前观察到的所
有信息（或者⼤部分信息）做出的反应，这天然决定了搜索排序问题的 sequential
decision-making 本质。
2.6 实验分析
我们根据图 9 设计了⼀个搜索排序的模拟实验来说明上⼀节理论分析结果。
我们设定⼀个搜索会话的最⼤决策步数为 30，因为从第 2 节的数据分析来看，
长度为 30 的 PV 序列总占⽐不超过 1%。我们通过线上真实数据⽣成商品候选
集合。在每个状态上，agent 可以选择动作集 A = fa1; a2; a3; a4; a5g 中的任意⼀
个动作对商品进⾏排序，进⾏页⾯展⽰。对于任意时间步 t（0 < t (cid:20) 30）以及任
意 item page history ht，ht 通过其最近 4 个商品页的商品特征来进⾏表⽰。同时，
ht 的 conversion probability、continuing probability 以及 abandon probability 也直
接通过 ht 的特征⽣成。
由于实验规模不⼤，我们可以直接⽤ Tabular 强化学习⽅法对问题进⾏求
解。我们分别采⽤ Q-learning 算法、Actor-Critic ⽅法和 Dynammic Programming
⽅法，在折扣率 (cid:13) 为 0、0:1、0:5、0:9 和 1:0 的情况下进⾏测试。每⼀个实验设置
运⾏算法 50 次，每⼀次包含 80 万次搜索会话，我们记录下学习过程中每个搜
索会话的平均成交额以及初始状态 h0 的 state value。这⾥仅给出部分结果。我
们⾸先对⽐两个极端情况 (cid:13) = 0 和 (cid:13) = 1:0 时，每个算法取得的 GMV 指标。如
图2.10所⽰，三个算法在 (cid:13) = 1:0 时的 GMV 都要⾼于 (cid:13) = 0 时的 GMV。单看结
果最直观的 DP ⽅法，可以发现它在两个折扣因⼦下的 GMV 有 6% 的 gap。如
前所述，折扣因⼦ (cid:13) = 0 即是分别独⽴优化 Agent 在每个状态上的即时奖赏，⽽
(cid:13) = 1:0 则是直接优化 GMV。
我们选取 Actor-Critic ⽅法，考察它在不同折扣率下的 GMV 曲线以及初始
状态 h0 的 state value V(cid:13)(h0) 的变化情况，结果如图2.11所⽰。可以看到，Actor-
Critic ⽅法在不同折扣率下的 GMV 指标与图2.10的结果⼀致，⽽图右边的值函
. 32 .
第⼆章 延迟奖赏在搜索排序场景中的作⽤分析
图 2.10: (cid:13) = 0 和 (cid:13) = 1 情况下测试算法的 GMV 曲线
数变化曲线更直接证明我之前的理论分析。只有折扣因⼦ (cid:13) = 1:0 时，V(cid:13)(h0) 的
值才和对应的 GMV ⼏乎相等。需要注意的是，Actor-Critic ⽅法在 (cid:13) 等于 0:9 和
1:0 时的 GMV 曲线基本重合，并不能说明优化 V0:9 能够优化 GMV，⽽只能说
明优化 V0:9 与优化 V1:0 得到的最优策略恰好相同，⼆者本质上并不⼀样。
⾄此，我们已经通过理论分析和实验证明了搜索排序问题是⼀个有限时间
步的⽆折扣（Finite-Horizon Undiscounted）的顺序决策问题。当然，我们所有的
结论都是在以最⼤化成交额为⽬标的前提下得到的。如果追求点击率或者转化
率之类的⽬标，结论可能会不同，但都可以采⽤与本⽂类似的⽅法进⾏分析。
2.6 实验分析
. 33 .
图 2.11: Actor-Critic ⽅法在各个折扣率设置下的 GMV 曲线和值函数曲线
第三章 基于多智能体强化学习的多
场景联合优化
3.1 背景
淘宝平台下有⾮常多的⼦场景，例如搜索、推荐、⼴告；每个⼦场景又有⾮
常多细分，例如搜索包括默认排序、店铺内搜索、店铺搜索等；推荐内有猜你
喜欢、今⽇推荐、每⽇好店等。基于数据驱动的机器学习和优化技术⽬前⼤量
的应⽤于这些场景中，并已经取得的不错的效果——在单场景内的 A/B 测试上，
点击率、转化率、成交额、单价都能看到显著提升。
然⽽，⽬前各个场景之间⽬前是完全独⽴优化的，这样会带来⼏点⽐较严
重的问题：
  a. ⽤户在淘宝上购物会经常在多个场景之间切换，例如：从主搜索到
猜你喜欢，从猜你喜欢到店铺内。不同场景的商品排序仅考虑⾃⾝，会导致⽤
户的购物体验是不连贯或者雷同的。例如：从冰箱的详情页进⼊店铺，却展⽰
⼿机；各个场景都展现趋同，都包含太多的 U2I（点击或成交过的商品）。
  b. 多场景之间是博弈（竞争）关系，期望每个场景的提升带来整体提
升这⼀点是⽆法保证的。很有可能⼀个场景的提升会导致其他场景的下降，更
可怕的是某个场景带来的提升甚⾄⼩于其他场景更⼤的下降。这并⾮是不可能
的，那么这种情况下，单场景的 A/B 测试就显得没那么有意义，单场景的优化
也会存在明显的问题。因为这⼀点尤为重要，因此我们举⼀个更简单易懂的例
3.1 背景
. 35 .
⼦（如图3.1）。⼀个 1000 ⽶长的沙滩上有 2 个饮料摊 A 和 B，沙滩上均分分布
者很多游客，他们⼀般会找更近的饮料摊去买饮料。最开始 A 和 B 分别在沙滩
250 ⽶和 750 ⽶的位置，此时沙滩左边的⼈会去 A 买，右边的⼈去 B 买。然后
A 发现，⾃⼰往右边移动的时候，会有更多的⽤户（A/B 测试的结论），因此 A
会右移，同样 B 会左移。A 和 B 各⾃“优化”下去，最后会都在沙滩中间的位置，
从博弈论的⾓度，到了⼀个均衡点。然⽽，最后“优化”得到的位置是不如初始位
置的，因为会有很多游客会因为太远⽽放弃买饮料。这种情况下，2 个饮料摊各
⾃优化的结果反⽽是不如不优化的。
图 3.1: ⼀⽚长沙滩上 2 个零⾷店关于选择开店位置的博弈过程。上图表⽰初始
位置，中间图表⽰博弈过程，下⽅图表⽰均衡状态。红⾊的⽤户表⽰愿意在 A
购买，蓝⾊的⽤户表⽰愿意在 B 购买，灰⾊⽤户表⽰太远⽽偏向于放弃购买
多场景问题实际并不⽌存在于淘宝上，实际上⽬前⽐较⼤型的平台或者⽆
线 APP 都不⽌⼀个场景。即使不谈 Yahoo，Sina 等综合性⽹站，像 Baidu、Google
等功能⽐较单⼀、集中的应⽤，也会有若⼲场景（如⽹页、咨询、地图等）。那
么这些平台或应⽤都会⾯临类似的问题。
综上，研究⼤型在线平台上的多⼦场景联合优化，⽆论从淘宝平台的应⽤
上，还是从科研的⾓度，都具有重要意义。
. 36 .
第三章 基于多智能体强化学习的多场景联合优化
为了解决上述的问题，本⽂提出⼀个多场景联合排序算法，旨在提升整体
指标。我们将多场景的排序问题看成⼀个完全合作的、部分可观测的多智能体序
列决策问题，利⽤ Multi-Agent Reinforcement Learning 的⽅法来尝试着对问题进
⾏建模。该模型以各个场景为 Agent，让各个场景不同的排序策略共享同⼀个⽬
标，同时在⼀个场景的排序结果会考虑该⽤户在其他场景的⾏为和反馈。这样使
得各个场景的排序策略由独⽴转变为合作与共赢。由于我们想要使⽤⽤户在所
有场景的⾏为，⽽ DRQN 中的 RNN ⽹络可以记住历史信息，同时利⽤ DPG 对
连续状态与连续动作空间进⾏探索，因此我们算法取名 MA-RDPG(Multi-Agent
Recurrent Deterministic Policy Gradient)。
本章内容，带来的核⼼进步主要有以下三点:
• 我们将多场景联合优化（排序）的问题，建模成⼀个完全合作、部分可观
测的多智能体序列决策问题。
• 我们提出了⼀个全新的、基础的多智能体强化学习模型，叫作 Multi-Agent
Recurrent Deterministic Policy Gradient。该模型能使多个智能体（多个场景）
合作以取得全局的最佳效果。
• 我们将算法应⽤在了淘宝的线上环境，测试表明我们的模型能显著提升淘
宝的整体指标。
3.2 问题建模
3.2.1 相关背景简介
排序学习（Learning to rank, L2R [25]）被⼴泛应⽤于在线的排序系统中。排序
学习最基本的思想是最优的排序策略是能通过⼤量的训练样本被学习得到。每
条训练样本包含⼀个搜索词（query）以及改搜索词下展⽰的商品序列，排序策
略将每个商品的多个特征映射成⼀个排序分值。排序策略的参数能够通过多种
不⽤的⽅式学习得到，例如基于 point-wise [9, 21] 的优化，基于 pair-wise [2, 29]
的优化和基于 list-wise methods [3, 5] 的优化算法。
3.2 问题建模
. 37 .
深度递归 Q 网络: 在实际的应⽤场景中，环境中的状态可能是只能被部
分观测的。因此强化学习中的智能体不能观测到全部的状态，这种设定被称为
“部分可观测”。深度递归 Q ⽹络（Deep Recurrent Q-Networks, DRQN）被提出
来，通过递归编码之前的观测，解决这种部分观测的问题。DRQN 在当前⾏为
之前，先使⽤递归神经⽹络编码之前的状态，取代 Q ⽹络中消除“状态-⾏为函
数”Q(st; at)，它会估计 Q(ht(cid:0)1; ot; at)，其中 ht(cid:0)1 表⽰ RNN 中的隐状态，通过之
前的观测 o1; o2; : : : ; ot(cid:0)1 综合⽽来。递归神经⽹络必须使⽤这个函数去更新它的
隐状态 ht = g(ht(cid:0)1; ot)，其中 g 是⼀个⾮线性函数。
在多智能体强化学习（Multi-Agent Reinforcement Learning，MARL） [4,
24, 13, 28] 中，存在⼀组⾃主的、可交互的智能体，他们处于⼀个相同的环境。
每个智能体会观测到我们各⾃的信息，然后根据⾃⾝的策略函数，决定⼀个当
前⾏为。不同智能体智能通过完全合作、完全竞争或者混合⽅式相处。完全合
作下，所有智能体共享⼀个相同⽬标；完全竞争下，不同智能体的⽬标是相反
的；混合模型则介于前两者之间。
3.2.2 建模方法
问题描述
我们将任务设计成⼀个完全合作、部分可观测、智能体序列决策问题。更
具体的：
多智能体，在系统中，每⼀个⼦场景都拥有⾃⼰的排序策略。每个智能体
产出⼀个排序策略，同时学习⾃⼰的策略函数，该函数将⾃⼰的状态映射到⼀
个⾏为上。
序列决策，⽤户时序的与系统进⾏交互，因此智能体的⾏为也是序列性的。
在每⼀个时间点上，智能体通过返回⼀个商品序列给⽤户，完成⼀次⽤户与场
景的交互。当前的决策会对未来接下来的决策产⽣影响。
完全合作，所有的智能体共同优化⼀个相同的⽬标值。更进⼀步，每个智
能体会发送消息给其他智能体来进⾏通信，整体的收益、⽬标通过⼀个综合的
裁判来评判。
. 38 .
第三章 基于多智能体强化学习的多场景联合优化
图 3.2: 模型的整体结构。模型有⼀个综合的、全局的“裁判”来评价整体的收益。
⼀个通信模块⽤来⽣成消息，消息可以被多个智能体共享。每条消息编码了⼀
个智能体的历史观测和⾏为，被⽤于逼近全局的环境状态。每个智能体⽹络接
收它独⾃的观测以及受到的信息，同时独⽴产出⼀个⾏为。
部分观测，每个智能体智能观测⼀部分环境，同时能接受其他智能体发送
的信息。
模型
下⾯我们会详细的介绍我们提出的多智能体递归确定策略梯度法，来解决
上述完全合作、部分可观测、智能体序列决策问题。
整体模型。
图3.2展⽰了我们模型的整体结构。为了简单起见，我们仅考虑 2 个智能体
的情况，每个智能体表⽰⼀个能⾃我优化的场景和策略。收到深度策略梯度法
(DDPG [22]) 的启发，我们的模型同样基于 actor-critic ⽅法 [17]。我们设计了 3
个重要的模块让多智能之间能有协同与合作，分别是⼀个整体的、全局的“裁判”，
独⽴的智能体，以及通信机制。
全局的裁判会维护⼀个⾏为-值函数，该函数表⽰，在当前状态下，进⾏⼀
个⾏为时未来整体收益的期望。每个智能体维护⼀个⾏为⽹络，将状态确定的
映射到⼀个唯⼀的⾏为上。每个智能体的决策⾏为会在它的场景内进⾏优化。
3.2 问题建模
. 39 .
图 3.3: MA-RDPG 算 法 的 详 细 结 构。 中 ⼼ 的 裁 判 会 模 拟 “⾏ 为-值” 函 数
Q(ht(cid:0)1; ot; at)，它表⽰当接受到信息 ht(cid:0)1 和观测 ot 时，采取⾏为 at 会获得的
整体收益。每个智能体才产⽣⼀个确定的⾏为，基于函数 ai
t)。信息
会被通信模块更新，基于观测 ot 和⾏为 at。红⾊表⽰信息，蓝⾊表⽰观测，绿
⾊表⽰⾏为。
t = (cid:22)i(ht(cid:0)1; oi
我们设计了⼀种基于 LSTM [12] 的消息机制。LSTM 是⼀种递归神经⽹络，
他能将所有智能体的全部观测和⾏为编码成⼀个消息向量。该消息向量会被发
送到不同的智能体，以此形成协作。由于这种机制的存在，每个智能体的决策
并不只是基于⾃⼰的状态以及之前⾏为，同时会考虑其他智能体的状态与⾏为。
这种通信能让每个智能体去模拟全局的环境状态，让他们进⾏更准确的决策。
模型细节。
⼀个经典的强化学习问题中，会存在⼀个形如 (o1; r1; a1;(cid:1)(cid:1)(cid:1) ; at(cid:0)1; ot; rt) 的
历史的⾏为序列，其中 o/r/a 分别表⽰观测、收益以及⾏为。正如之前提及
的，我们问题中的环境是部分可观测的，这也就是说状态 st 代表的是过往的
第三章 基于多智能体强化学习的多场景联合优化
. 40 .
经验，即 st = f (o1; r1; a1;(cid:1)(cid:1)(cid:1) ; at(cid:0)1; ot; rt)1我们考虑的是⼀个 N 个智能体的问
题，fA1; A2; : : : ; ANg，每个智能体对应⼀个特征的优化场景（例如排序、推荐
等）。在这种多智能体的设定下，环境的状态 (st) 是全局的，被多个智能体共
享；但是观测 (ot = (o1
t ))，记忆短期收
益 (rt = (r(st; a1
t ;(cid:1)(cid:1)(cid:1) ; oN
t ; o2
t );(cid:1)(cid:1)(cid:1) ; r(st; aN
t ); r(st; a2
t ))) 都是独⾃拥有的。
t ))，⾏为 (at = (a1
t ;(cid:1)(cid:1)(cid:1) ; aN
t ; a2
t = r(st; ai
t，然后会从环境中得到⼀个暂时收益 ri
更具体的来说，每个智能体 Ai 会根据⾃⼰的策略 (cid:22)i(st) 和状态 st 进⾏每次
t)，同时状态会从 st
决策⾏为 ai
更新为 st+1。在我们的任务中，多个智能体会协同合作，期望达到整体的最⼤
t ;(cid:1)(cid:1)(cid:1) ; aN
收益。我们有⼀个全局的“⾏为-值”函数（critic）Q(st; a1
t ) 去预估整体
的全局收益，在当前状态去采取⾏为 (a1
t ) 时。我们同时又⼀个全局
的状态表⽰，每个智能体在获得本地观测后，会执⾏⼀个本地⾏为。因此，我
们的模型属于⼀种 actor-critic 强化学习，包含⼀个中⼼的 critic 以及多个独⽴的
actor（每个代表⼀个智能体）。
t ;(cid:1)(cid:1)(cid:1) ; aN
t ; a2
t ; a2
如图3.3所⽰，在时间点 t，智能体 Ait 从环境中接受当前的观测 oit
t 。环境
的全局状态被所有智能体共享，不⽌依赖于各个智能体的历史状态和⾏为，同
时也依赖当前观测 ot，换句话说，st = f (o1; a1;(cid:1)(cid:1)(cid:1) ; at(cid:0)1; ot)。为了达到这个⽬
的，我们设计了⼀个通信模块，该模块使⽤ LSTM 来编码之前的观测以及⾏为，
编码得到的是⼀个向量形式的信息。通过智能体之间的通信交流，整体的状态
可以近似为 st (cid:25) fht(cid:0)1; otg，这是因为信息 ht(cid:0)1 已经包含了所有之前的观测和⾏
t )，⽬标是最⼤化整
为。每个智能体 Ait 选择⼀个⾏为 ait
体的未来收益，该收益通过⼀个中⼼的 criticQ(st; a1
t ) 来评价。需要
注意，在每个时间点，ot = (o1
t = (cid:22)it(st) (cid:25) (cid:22)it(ht(cid:0)1; oit
t ) 是有所有的观测组合得到的。
t ;(cid:1)(cid:1)(cid:1) ; aN
t ;(cid:1)(cid:1)(cid:1) ; oN
t ; a2
t ; o2
通信模块。我们设计了⼀个通信模块，⽬的是让智能体之间能通过相互传
递信息以更好的合作。信息包含了⼀个智能体本地的历史观测和⾏为。在时间
t 和⼀个来⾃环境的消息 ht(cid:0)1。通信模块会
点 t，智能体 Ait 接收到⼀个观测 oit
根据之前的消息 ht(cid:0)1 以及当前的观测 ot ⽣成⼀个新的消息 ht，通过这个消息，
该智能体能达成与其他合作智能体的信息共享。如图 3.4，我们使⽤⼀个 LSTM
1在⼀个完全可观测的环境中, st = f (ot)。
3.2 问题建模
. 41 .
图 3.4: 通信模块。之前的观测 (ot) 以及⾏为 (at) 都会被作为输⼊进⼊ LSTM ⽹
络，隐状态 (ht(cid:0)1) 被作为消息发送到其他智能体。需要注意，ot; at 是向量。
结构来达到这个⽬的，更形式化的，通信模块⼯作模式如下：
ht(cid:0)1 = LST M (ht(cid:0)2; [ot(cid:0)1; at(cid:0)1];  )
(3.1)
注意，ot 和 at 由所有智能体的观测和⾏为组成，同时每个 ai
t 也是⼀个实值向量。
受益于信息 ht(cid:0)1，每个单独的智能体能够去近似得到全局的环境状态，st (cid:25)
t，却不能得到全局的状
fht(cid:0)1; otg。这解决了每个智能体智能接受本地的观测 oi
态 st 的问题。
Private Actor。每个智能体（agent）都是⼀个独⽴的“演员”（actor），它们会接
受本地的观测以及共享的信息，然后最初⼀个决策。由于我们考虑的是连续⾏
N i); ai 2 RN i。
为的强化学习问题，我们定义⾏为是⼀个实质向量 ai = (wi
因此，每个⾏为是⼀个 N i 维向量，每⼀个维度都是⼀个实数值。这个向量会被
当作排序的参数控制搜索排序。
1; : : : ; wi
由于这是⼀个连续⾏为类型，类似的⼯作常见于控制问题中 [32, 22, 11]。受
相关⼯作的启发，我们使⽤⼀个确定策略的⽅法，⽽不是随机策略的⽅式。每
个智能体的 actor 对应函数 (cid:22)i(st; (cid:18)i)，其中参数是 (cid:18)i，改函数将⼀个状态确定的
映射到⼀个⾏为上。在时刻 t，智能体 Ait 根据 actor ⽹络决定⾃⼰的⾏为：
t = (cid:22)it(st; (cid:18)it) (cid:25) (cid:22)it(ht(cid:0)1; oit
ait
t ; (cid:18)it)
(3.2)
. 42 .
第三章 基于多智能体强化学习的多场景联合优化
其中 st (cid:25) fht(cid:0)1; otg，如之前描述，表⽰通信模块。因此，actor 的⾏为同时
依赖于信息 ht(cid:0)1 和⾃⼰当前的观测 observation oit
t .
Centralized Critic。和 DDPG 算法⼀样，我们设计了⼀个评价（critic）⽹
络来拟合“⾏为-值”函数，该⽹络⽤来近似未来整体收益的期望。因为所有智能
体可以共享⼀个⽬标，我们使⽤⼀个全局的 critic 函数 Q(st; a1
t ; ϕ) 来
拟合未来的整体收益，当全部的智能体在状态 st (cid:25) fht(cid:0)1; otg 时采取⾏为 at =
fa1
t ;(cid:1)(cid:1)(cid:1) ; aN
g。
t ; a2
t ; : : : ; aN
t
以上公式在所有智能体都是活动的状态下，是基础⽽有效的。在我们的设
g ，at =
g。于是，我们需要简化“⾏为-值”函数为 Q(ht(cid:0)1; ot; at; ϕ)，并且⾏为函数为
定下2，因此只会有⼀个智能体 Ait 在时刻 t 是活跃的，此刻 ot = foit
fait
(cid:22)it(ht(cid:0)1; ot; (cid:18)it)。
t
t
模型训练
centralized critic ⽹络 Q(ht(cid:0)1; ot; at; ϕ) 我们和 Q-learning ⼀样 [38] 使⽤ Bell-
man 公式训练。我们最⼩化以下 loss 函数：
L(ϕ) = Eht(cid:0)1;ot[(Q(ht(cid:0)1; ot; at; ϕ) (cid:0) yt)2]
(3.3)
其中
(3.4)
private actor ⽹络的更新则基于最⼤化整体的期望。假设在时刻 t，Ait 是活
yt = rt + (cid:13)Q(ht; ot+1; (cid:22)it+1(ht; ot+1); ϕ)
跃的，那么⽬标函数是：
J((cid:18)it) = Eht(cid:0)1;ot[Q(ht(cid:0)1; ot; a; ϕ)ja=(cid:22)it (ht(cid:0)1;ot;(cid:18)it )]
根据链式法则, 每个 actor 的参数梯度可以表⽰如下:
∇(cid:18)it J((cid:18)it)
(cid:25) Eht(cid:0)1:ot[∇(cid:18)it Qit(ht(cid:0)1; ot; a; ϕ)ja=(cid:22)it (ht(cid:0)1;ot;(cid:18)it )]
= Eht(cid:0)1;ot[∇aQit(ht(cid:0)1; ot; a; ϕ)ja=(cid:22)it (ht(cid:0)1;ot)∇(cid:18)it (cid:22)it(ht(cid:0)1; ot; (cid:18)it)]
2因为⼀个⽤户同⼀时刻只会存在于⼀个场景中
(3.5)
(3.6)
3.3 应⽤
通信模块训练的⽬标是最⼩化以下函数：
L( )
= Eht(cid:0)1;ot[(Q(ht(cid:0)1; ot; at; ϕ) (cid:0) yt)2jht(cid:0)1=LST M (ht(cid:0)2;[ot(cid:0)1;at(cid:0)1]; )]
(cid:0) Eht(cid:0)1;ot[Q(ht(cid:0)1; ot; at; ϕ)jht(cid:0)1=LST M (ht(cid:0)2;[ot(cid:0)1;at(cid:0)1]; )]
. 43 .
(3.7)
整体的训练流程见 1。我们使⽤⼀个 replay buffer [22] 来存储每个智能体
与环境的交互，并使⽤ minibatch 的⽅式更新。在每个训练时刻，我们选出⼀组
minibatch，然后并⾏的训练他们，同时更新 actor ⽹络和 critic ⽹络。
3.3 应用
以上章节描述了⼀个通⽤的多智能体强化学习框架，能解决多场景协作优
化的问题。这⼀章节，我们着重描述这个算法在淘宝上的应⽤。更具体的，我们
在 2 个相关的排序场景下的应⽤。
⾸先我们会给出⼀个简单的淘宝电商平台整体介绍，然后我们会详细介绍
MA-DRPG 算法在淘宝的应⽤。
3.3.1 搜索与电商平台
在⼀个电商平台上，往往都存在多个不同的排序场景，每个场景都会有⾃
⼰独⽴的排序策略。特别的，我们选择了淘宝上 2 个最重要的搜索排序场景来
应⽤ MA-DRPG 算法：主搜索和店铺内搜索。下⾯会简单的独⽴介绍：
主搜索是淘宝电商平台的最⼤⼊⼜，当⽤户在淘宝主页⾯上的搜索框上输
⼊⼀个请求词时，主搜索会返回相关的商品。主搜索会对淘宝上各个业务线上
的商品进⾏综合排序，每秒会收到⼤于 40,000 个搜索请求，每天会有 35 亿以上
的页⾯展⽰次数和 15 亿以上的点击，1 亿以上的⽤户。
店铺内搜索对⼀个特定店铺的商品进⾏排序。这总排序⼀般发⽣在⼀个⽤
户进⼊到⼀个特定的店铺的时候，例如“Nike 官⽅旗舰店”等。在这种排序下，⽤
. 44 .
第三章 基于多智能体强化学习的多场景联合优化
(a) 2 个搜索引擎独⽴优化
(b) 使⽤ MA-RDPG，2 个搜索引擎联合优化
图 3.5: 独⽴优化与协同优化的对⽐.
户可以输⼊⼀个关键词，也可以补输⼊任何关键词。在⼀天中，会有超过 5000
万⽤户使⽤店铺内搜索，并有超过 6 亿的点击和 15 亿的页⾯展⽰数。
⽤户会频繁的在 2 种场景下切换：当⼀个⽤户在主搜索上看到⼀条漂亮的
连⾐裙时，她会进⼊对应的店铺看看有没更好看的；当⼀个⽤户在店铺内搜索
时⾐服，也许会觉得商品太少，进⽽到主搜索去找更多的⾐服，从⽽又进⼊其
3.3 应⽤
. 45 .
他的店铺。我们的⼀个简单统计发现，会有 25:46% 的⽤户会从主搜索进⼊店铺
内搜索，⽽会有 9:12% 的⽤户在使⽤店铺搜索后又使⽤主搜索。
在已有的模型中 [8, 15, 10]，不同场景中的排序策略会独⽴的优化，优化的
⽬标也仅仅考虑⾃⾝⽽不管其他的场景。图 Figure 3.5(a) 描述了传统的多场景
优化⽅式。图中红⾊上部分表⽰主搜索，下部蓝⾊部分表⽰店铺搜索。2 个搜索
引擎是独⽴优化的。
3.3.2 多排序场景协同优化
图 3.5(b) 描述了主搜索和店铺内搜索的联合优化⽅式。不同于 2 个场景的
独⽴优化，MA-RDPG 对 2 个智能体协同建模，主搜索和店铺内搜索各⾃学习⾃
⾝场景的排序策略权重。不同场景之间的协同，通过下⾯ 2 种⽅式：⾸先，我们
拥有⼀个共同的全局优化⽬标；其次，他们会产⽣并⼴播⾃⼰的排序策略。为
了让说明更加清晰，我们将描述⼀些将 MA-DRPG ⽤于淘宝上的核⼼概念。
环境（Environment）。环境是在线的电商系统。它的状态会随着 2 个排序
场景的策略变化⽽变化，它会对每个排序策略给出相应的收益。
智能体（Agents）。有 2 个智能体，分别是搜索排序和店铺内排序。在每⼀
个时刻，⼀个搜索引擎会根据⾃⼰的排序策略给⽤户返回⼀个商品列表。2 个智
能体共同优化⼀个整体的淘宝平台收益，例如 GMV。
状态（States）。如前⽂提到，状态是部分可观测的。每个排序场景智能观
测到当前场景下的信息，例如⽤户的数据（年龄、性别、购买⼒等）、⽤户点击
的商品（价格、销量等）、搜索词的类型等。⼀个 52 维的向量被⽤来表⽰⼀个
状态。如 MA-DRPG 算法中所说，完全的状态向量除了包括本地的观测，还包
括全局的消息。
行为（Actions）。每个智能体需要在⽤户搜索时输出⼀个排序的商品列表，
因此我们将每个智能体的⾏为定义为⼀组 ranking feature 的权重。计算排序分
数，是将⼀个商品的特征值与对应的特征权重做内积；改变⾏为，意思则是变
更排序特征的权重。在主搜索中，我们⾏为的维度是 7，⽽在店铺内搜索中，我
们⾏为的维度是 3。
. 46 .
第三章 基于多智能体强化学习的多场景联合优化
图 3.6: Actor ⽹络。红⾊虚线的部分输出⼀个实数值的排序向量（绿⾊），蓝⾊
和红⾊分别表⽰本地观测和接收消息。
每个智能体有它独⾃的策略函数，图3.6给出了 actor ⽹络的结构。⽹络结构
是⼀个 3 层的感知机，前 2 层使⽤ ReLu 作为激活函数，⽽最后⼀层使⽤ softmax
作为激活函数。⽹络的输⼊是本地观测以及收到的消息，⽽输出是⼀组排序的
特征权重。
收益（Reward）。在我们的系统中，设计的收益不只是成交⾏为，其他⾏
为也被考虑到。这样，我们能更多的使⽤⽤户的反馈⾏为特征。
如果产⽣了⼀个成交⾏为，会得到⼀个等于成交价格的收益；如果⼀个点
击发⽣了，会得到⼀个等于 1 的收益；如果没有任何点击或者成交，会得到⼀
个值为-1 的收益；如果⽤户直接离开了搜索，那么会得到⼀个值为-5 的收益。
3.4 实验
为了验证 MA-RDPG 算法的效果，我们将算法应⽤到了淘宝的在线环境
——主搜索和店铺内搜索中。
3.4 实验
. 47 .
3.4.1 实验设置
训练流程。训练流程如图3.5(b) 所⽰。我们的训练过程基于淘宝的实时在线
训练系统，⾸先系统会实时的获取⽤户的⾏为⽇志，为 MA-RDPG 算法提供训
练样本；然后这些样本存储在⼀个 replay buffer 中；最后，更新模型，并将更新
后的模型应⽤于线上。这个流程不断反复，因此这个在线模型是在线动态更新
的，从⽽捕捉到⽤户的⾏为变化。
参数设置。对每个智能体（搜索场景），本地的观测都是⼀个 52 维的向量，
⾏为分别对应 7 维和 3 维的向量。由于通信模块和评价⽹络都需要各个不同场
景的⾏为，因此为简单，我们使⽤⼀个长度为 10 的向量（0 补充空的部分）作
为 LSTM 和评价⽹络的输⼊。
对通信模块来说，输⼊是⼀个 52 + 7 + 3 = 62 维的向量，同时输出是⼀个
10 维的向量。⽹络结构如图3.4。在 actor ⽹络，输⼊维度是 52 + 7 + 3 = 62，⽹
络隐层的⼤⼩分别是 32/32/7（3）。前 2 层的激活函数使⽤的 ReLU，最后⼀层
的激活函数是 Softmax。⽹络的结构如图3.6。
评价⽹络有 2 个隐层，每层的神经元个数是 32，使⽤ ReLU 作为激活函数。
Bellman 公式中的收益衰减系数设为 (cid:13) = 0:9。在我们的实验中，我们使⽤
(cid:0)5，分别对应 actor
RMSProp 来学习⽹络的参数；学习率则是使⽤的 10
⽹络和 critic ⽹络。我们使⽤的 replay buffer 的⼤⼩是 104，每个 minibatch 的⼤
⼩是 100。
(cid:0)3 和 10
3.4.2 对比基准
排序算法的对⽐基准如下：
经验权重 (EW)。这种算法中，主搜索和店铺内搜索的排序权重是⼈⼯根据
经验确定的。
Learning to Rank (L2R)。在这种算法中，特征的权重使⽤⼀种基于 point-
wise 的 L2R 算法学习得到。算法使⽤⼀个同样的结构的神经⽹络，如图3.6所⽰，
但是输⼊不包含收到的消息。这个⽹络通过⽤户的反馈⾏为学习得到。
EW，L2R，MA-RDPG3 个算法主要的不同在于产出排序特征权重的⽅式。
. 48 .
第三章 基于多智能体强化学习的多场景联合优化
在 MA-RDPG 算法中，排序的特征权重使⽤ actor ⽹络产出，⼀些有代表性的特
征在表 3.1中列出。
基于上述算法，我们将 MA-RDPG 与 3 中独⽴优化的⽅法进⾏对⽐：1)
EW+L2R; 2) L2R+EW; 3) L2R+L2R。加号左边表⽰主搜索使⽤的排序算法，右
边表⽰店铺内搜索使⽤的排序算法。
3.4.3 实验结果
实验对比指标
我们与上述基准算法对⽐，报告提升的相对百分⽐，基准是主搜和店铺内
搜索都是⽤ EW 算法。使⽤的指标，GMV gap 定义为 (GM V (x)(cid:0)GM V (y))
，对了进
⾏⼀次公平的对⽐，我们将算法使⽤标准的 A/B 测试进⾏，3% 的⽤户被选作测
试组，实验进⾏ 10 天左右。我们同时提供了每个场景的实验效果，以便我们分
析 2 个场景之间的关联。
GM V (y)
实验分析
实验的结果被列在表 3.2中，从中我们能得到以下结论：一，我们的 MA-
RDPG 算法的在线效果明显优于其他算法。更具体的，MA-RDPG 从整体收益来
看是⽐ L2R+L2R 算法更加有效的；L2R+L2R 算法是淘宝在线使⽤的算法，效
果⼀直⽐较优秀，但是该算法只考虑各⾃场景本⾝，⽽没有考虑场景之间的协
同合作。这说明，不同场景之间的合作是确实能提升提升 GMV 的。
二，使⽤ MA-RDPG 后，店铺内搜索 GMV 的提升⾮常明显，同时主搜索有
⼀定的提升。这种现象产⽣的主要原因是有更多的⽤户是从主搜索到店铺内搜
索的，⽽不是反过来。因此，从这 2 个场景的合作中，店铺内搜索能够获得更⼤
的收益。
三，L2R+EW 的实验结果进⼀步证明了不同场景之间是需要进⾏合作的，因
为我们可以⽐较明显的看到，在单独优化主搜索的时候，店铺内搜索的指标是
会被损伤的。
3.4 实验
. 49 .
图 3.7: 上/中: critic/actor ⽹络的学习过程。下: GMV 的在线提升随时间的变化.
图3.7的下⾯，我们可以看到 MA-RDPG 对在线 GMV 的提升对⽐时间的变
化。我们可以看到算法对 GMV 的提升是连续⽽稳定的。
行为分析
如之前提到，每个智能体都是使⽤的连续⾏为，因此我们可以分析不同搜
索场景下，⾏为随时间的变化，如图3.8所⽰。因为每个⾏为都是是个实质的向
量，因此我们曲线中画的是不同维度上⾏为的平均值。
上⾯⼀张⼦图画的是主搜索上⾏为随时间变化的曲线。Action_1 拥有最⼤
的权重，它对应的 feature 是 CTR 预估分。这表⽰，CTR 预估是我们学习得到
的最重要的特征，这与我们对排序的认识也是⼀致的。Action_6 是第⼆重要的
. 50 .
第三章 基于多智能体强化学习的多场景联合优化
图 3.8: 主搜和店铺内搜索的 action 均值变化情况。
特征，它表⽰的是商品对应店铺的热度。在 L2R 模型中，我们会看到这并不是
⼀个重要的特征，但是在我们的实验中，它表现得出乎我们意料的重要。这主
要是因为通过这个特征，主搜索能直接将更多的⽤户流量引导到店铺内搜索中，
从何实现合作。
下⾯⼀张⼦图描述的是店铺内搜索的⾏为随时间变化。Action_0 是最重要
的特征，它表⽰的是⼀个商品的销量；这表⽰在⼀个店铺内，热销的商品往往
会更容易达成成交。
尽管不同⾏为的分布⼀开始会变化的⾮常⼤，但是在训练 15 ⼩时的时候，
就趋于平稳。这与图3.7上也是⼀致的。
3.5 总结与展望
. 51 .
3.4.4 在线示例
在这⼀⼩节我们进⼀步分析了⼀些典型的例⼦，说明 MA-RDPG 是怎么让
主搜索和店铺内搜索合作起来的。考虑到在线系统的变化太多，我们这⾥集中
分析⼀些典型情况，对⽐ MA-RDPG 和 L2R+L2R 算法的排序结果。
第⼀个例⼦反映的是主搜索怎么去帮助店铺内搜索，从⽽得到更多的整体
收益。我们假设这样⼀个场景：⼀个强购买⼒的⼥性⽤户点击了很多⾼价⽽低
转化的商品，然后搜索了⼀个关键词“连⾐裙”。不同算法的结果展⽰在图3.9(a)
中。显然，MA-RDPG 更容易返回⼀些⼤店中的⾼价⽽低销量的商品，这能让⽤
户更容易进到店铺中。对⽐ L2R+L2R 算法，MA-RDPG 能从⼀个更加全局的⾓
度进⾏排序，它不仅考虑了当前的短期点击和成交，同时会考虑潜在的会在店
铺内进⾏的成交。
第⼆个例⼦，我们选的情景是⼀个男⼠想买⼀个冰箱。他⾸先在主搜搜索了
“冰箱”，然后点击进了其中⼀个商品，通过该商品进⼊了店铺；这个店铺是⼀个
⾮常⼤型的家电卖家。图3.9(b) 中分别是店铺内搜索 MA-RDPG 和 L2R+L2R 的
排序结果对⽐。从图中可以看到，MA-RDPG 算法更容易排出冰箱，⽽ L2R+L2R
展⽰的则更加发散。这主要是因为店铺内搜索的时候，会接受到主搜索传递的
信息，因此会基于主搜的排序结果产⽣更合理的排序。
3.5 总结与展望
随着 AI 技术的发展，越来越多的新技术涌现，我们的排序也经历了从规则
到浅层监督学习，到深度学习、强化学习等智能排序算法的转变。新的问题与挑
战，迫使我们不断地开拓与进取。从⼀个场景的优化，到现在尝试着联合两个
场景⼀起优化，这只是联合优化的⼀⼩步，现在的做法也⽐较简单，甚⾄还存
在着⾮常多的不⾜和缺陷，还有更多更复杂的问题需要我们去克服去解决。我
们相信在⾯对未来越来越复杂的电商排序场景下，平台需要的不是各场景之间
的互搏与内耗，⽽是协同与合作，我们期待更多更⾼级的多场景联合优化的⽅
法涌现，为平台创造更⼤的价值。
. 52 .
第三章 基于多智能体强化学习的多场景联合优化
Algorithm 1: MA-RDPG
Input: The environment
Output: (cid:18) = f(cid:18)1; : : : ; (cid:18)Ng
1 Initialize the parameters (cid:18) = f(cid:18)1; : : : ; (cid:18)Ng for the N actor networks and ϕ for
the centralized critic network;
2 Initialized the replay buffer R;
3 foreach training step e do
4
for i = 1 to M do
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
h0 = initial message, t = 1;
while t < T and ot ̸= terminal do
Select the action at = (cid:22)it(ht(cid:0)1; ot) for the active agent it;
Receive reward rt and the new observation ot+1;
Generate the message ht = LST M (ht(cid:0)1; [ot; at]);
t = t + 1;
end
Store episode fh0; o1; a1; r1; h1; o2; r2; h3; o3; : : :g in R;
end
Sample a random minibatch of episodes B from replay buffer R ;
foreach episode in B do
for t = T downto 1 do
Update the critic by minimizing the loss:
L(ϕ) = (Q(ht(cid:0)1; ot; at; ϕ) (cid:0) yt)2, where
yt = rt + (cid:13)Q(ht; ot+1; (cid:22)it+1(ht; ot+1); ϕ);
Update the it-th actor by maximizing the critic:
J((cid:18)it) = Q(ht(cid:0)1; ot; a; ϕ)ja=(cid:22)it (ht(cid:0)1;ot;(cid:18)it );
Update the communication component.;
end
end
21
22 end
3.5 总结与展望
. 53 .
Scenario
表 3.1: Examples of Ranking Features
Feature Name
Description
Click
Through
Rate
Rating Score
Shop Popularity
Latest Collection
An CTR estimation using logistic
regression, considering features of
users, items and their interactions
Average user ratings on a certain item
Popularity of the item shop
Whether an item is the latest
collection or new arrivals of the shop
Sales Volume
Sales volume of an in-shop item
Main
Search
In-shop
Search
表 3.2: GMV gap evaluated on an online E-commerce platform. A+B means algorithm
A is deployed for the main search and B for the in-shop search. The values are the
relative growth ratio of GMV compared with the EW+EW setting.
day
1
2
3
4
5
6
7
avg.
EW + L2R
in-shop
total
main
total
L2R + EW
L2R + L2R
MA-RDPG
total
main
main
in-shop
in-shop
in-shop
main
total
0.04% 1.78% 0.58% 5.07% -1.49% 3.04% 5.22% 0.78% 3.84% 5.37% 2.39% 4.45%
0.01% 1.98% 0.62% 4.96% -0.86% 3.16% 4.82% 1.02% 3.64% 5.54% 2.53% 4.61%
0.08% 2.11% 0.71% 4.82% -1.39% 2.89% 5.02% 0.89% 3.74% 5.29% 2.83% 4.53%
0.09% 1.89% 0.64% 5.12% -1.07% 3.20% 5.19% 0.52% 3.74% 5.60% 2.67% 4.69%
-0.08% 2.24% 0.64% 4.88% -1.15% 3.01% 4.77% 0.93% 3.58% 5.29% 2.50% 4.43%
0.14% 2.23% 0.79% 5.07% -0.94% 3.21% 4.86% 0.82% 3.61% 5.59% 2.37% 4.59%
-0.06% 2.12% 0.62% 5.21% -1.32% 3.19% 5.14% 1.16% 3.91% 5.30% 2.69% 4.49%
0.03% 2.05% 0.66% 5.02% -1.17% 3.09% 5.00% 0.87% 3.72% 5.43% 2.57% 4.54%
. 54 .
第三章 基于多智能体强化学习的多场景联合优化
(a) Main search results.
(b) In-shop search results.
图 3.9: 搜索结果对⽐
第四章 强化学习在淘宝锦囊推荐系
统中的应用
4.1 背景
4.1.1 淘宝锦囊
在⼿淘的搜索中，当⽤户输⼊ query 进⾏搜索之后，⼀⽅⾯有适合他的商品
展现出来，另⼀⽅⾯，如何更好地理解⽤户意图，为其推荐更合适的关键词进
⾏细分查找，引导⽤户到他想找的商品，也是⼀件⾮常重要的事情。因此在⼿
淘搜索场景下，我们以“锦囊”这种产品形态来承载对⽤户意图的理解和细分。同
时锦囊的内容也分了不同的类型。⽬前，我们已经设计了两万多种类型的锦囊，
如“细选”，“相关搜索”，以及和商品 PID 相关的“袖长”，“裙长”等类型的锦囊。⽐如
当⽤户输⼊“连⾐裙”这⼀并不是特别精确的 query 后，我们可能提供“适⽤年龄”
类型锦囊，⽽⽤户在锦囊中点击“18-24 岁”来精细化⾃⼰的需求，从⽽更快地找
到想要的商品，提升购物的满意度，下图 4.1展⽰了锦囊的典型形态：
4.1.2 锦囊的类型调控
锦囊存在的⽬的是为了给⽤户提供良好的导购功能，我们不希望给⽤户带
来过多的打扰。锦囊的原理是在当前的搜索结果页 (SRP) 中插⼊⼀个内容框，提
供⼀定数⽬的候选词，或者是⼀个交互式的问答，⽤户⼀旦点击，就会将原先
. 56 .
第四章 强化学习在淘宝锦囊推荐系统中的应⽤
图 4.1: 淘宝上锦囊的典型形态
的 query 和锦囊进⾏结合，重新搜索从⽽得到新的 SRP，如下图 4.2所⽰。因此，
如何根据⽤户的历史⾏为，以及当前的实时状态，给其推荐合适的锦囊，就变
得很重要。这个⼯作的⽬标就是在不增加锦囊的展⽰ PV(page-view) 基础上，提
升锦囊的 CTR，以及使⽤⽤户数，即表明给⽤户提供了更加需要的锦囊，⽅便
了⽤户的购物过程。
4.2 系统框架及问题建模
. 57 .
图 4.2: 淘宝上锦囊被点击后的⽣效逻辑
4.1.3 工作摘要
本⼯作主要是针对淘宝的锦囊产品的候选类型的实时推荐。学习⽅法上，由
于两万多个候选动作的存在，我们基于 deep double Q-learning [35] 进⾏适应性
改变，将动作 ID 进⾏ embedding, 再进⾏ Q 值的学习 [19]。然⽽，由于我们的电
商情形和传统的强化学习应⽤的游戏等问题的存在很⼤不同：1. ⽤户分布变化
较⼤，导致了策略评估不准；2. ⽤户群体变化很⼤，导致了很差的策略可能被
认为是好策略的问题。我们又提出了两种⽅法来缓解：1. 采⽤ stratified sampling
replay 取代原始的 random replay；2. 利⽤基准数据的实时变化进⾏约减。并结合
整个系统进⾏了相关实验，最终 CTR，UV 等指标均达到了相关预期。
4.2 系统框架及问题建模
4.2.1 系统框架
考虑到系统的安全性以及实现的可能性，我们把锦囊类型的推荐系统分割
成两个部分，主要包括：1. 学习模块；2. 推荐实际⽣效模块。其中，学习模块的
功能是实时的获取⽤户的浏览，点击⽇志，⽤户特征，锦囊特征等，然后从中获
; r 等，并进⾏训练，并更新⽹络参数。同时，学习
取得到我们训练需要的 s; a; s
′
. 58 .
第四章 强化学习在淘宝锦囊推荐系统中的应⽤
图 4.3: 锦囊推荐系统的框架.
模块中得到的实时状态 s, 以及⽹络参数会放到实时的存储中 (iGraph)。⽽实际
⽣效的推荐模块，则⼀⽅⾯根据⽤户 ID 去获取相应的 state 等信息另⼀⽅⾯则
去获取最新的⽹络参数，然后在⽣效模块中就可以进⾏⽹络的前向计算，得到
相应的结果，最终进⾏锦囊类型的推荐。图 4.3 展⽰了我们的系统的框架：
4.2.2 问题建模
考虑到⽤户在淘宝平台上的⾏为具有序列决策性，符合强化学习中延迟反
馈的设定，并且强化学习能很好地捕捉实时变化，因此我们决定⽤强化学习来
求解问题。强化学习通常⽤马尔科夫决策过程 (MDP) 来建模，在我们的问题上，
我们建模成如下的形式 [33, 34]：
4.2 系统框架及问题建模
. 59 .
状态
状态应当能够代表⽤户的长期和当前的特征，以及对商品和锦囊的⼀些偏
好。因此，我们⾸先添加⽤户的⼀些特征，如性别、年龄、购买⼒、偏好等。然
后，还添加了 query 的相关特征，如当前 query 的类型、不同类型的⽤户整体和
当前⽤户的偏好情况。此外，状态中还包含了⽤户当前的⾏为、页⾯编号、查看
和点击的商品的特征等信息。
动作
我们的⽬标是学习⼀种策略，它可以决定在当前页⾯上显⽰哪种类型的锦
囊，所以我们就直接将锦囊类型作为动作。现在有超过⼆万种类型的锦囊，动
作空间⾮常⼤，直接采⽤原始 DQN ⽹络最后⼀层输出各个动作的 Q 值的形式
没办法继续使⽤，需要进⾏⼀定的改变。
奖赏函数
⾸先，我们想推荐给⽤户的是⽤户最需要的，最有可能去点击的锦囊。当
点击发⽣时，应该给予正的奖赏，当点击不发⽣时，情况则正好相反。同时，考
虑到锦囊作为⼀种导购性质的产品，⽤户在前⾯页数点击锦囊⽐在靠后⾯点击
时应该更有价值，这说明推荐的锦囊更加被⽤户需要。因此，在奖赏的设计中
也应考虑点击发⽣的时间。奖赏函数如下所⽰：
r1 = I (cid:3) (1 + (cid:11) (cid:3) e
(4.1)
(cid:0)x)
其中当点击发⽣时 I 为 1 否则为 0，x 为页数，⽽ (cid:11) 则是⼀个系数。
然后，我们又考虑到不同⽤户的习惯。有些⽤户习惯于点击锦囊，⽽某些
⽤户很少去点击。如果说⼀个很少有点击⾏为的⽤户选择了点击我们提供的锦
囊，可以认为该锦囊是更有价值的，也就是应该给予更多的奖赏，因此我们有：
r2 = I (cid:3) e
(cid:0)y
(4.2)
. 60 .
第四章 强化学习在淘宝锦囊推荐系统中的应⽤
其中，y 表⽰的是在最近的 100 次 PV 中⽤户点击锦囊的次数。最终我们将以上
进⾏结合得到：
r = r1 + (cid:12) (cid:3) r2
(4.3)
(cid:12) 是⼀个 0 ⾄ 1 之间的系数。
4.3 算法及模型设计
强化学习中，主要有两⼤类学习⽅法，分别是基于值函数 (value-based) 的
以及基于策略 (policy-based) 的⽅法。考虑到我们的问题有两万多个离散的候选
动作，以及问题的应⽤情形，我们采⽤了前者作为⽅案 [33]。
4.3.1 主体框架
基于值函数的⽅法中，最有名的就是 DeepMind 的 DQN 系列的⽅法了，DQN
在 Atari 上取得了⾮常好的表现。DQN 中每⼀轮迭代的 loss function [26]：
[(
yDQN
i
)
(cid:0) Q(s; a; (cid:18)i)
2
]
Li((cid:18)i) = E
′
s;a;r;s
其中：
yDQN
i
= r + (cid:13) max
′ Q(s
a
′
′
(cid:0)
)
; (cid:18)
; a
(4.4)
(4.5)
(4.6)
(cid:0) 表⽰的是⼀个独⽴的⽬标⽹络 (target network) 的参数。除此之外，experience
(cid:18)
replay 也被采⽤来提升 DQN 的效果。Agent 从很多的 episodes 中获得 experiences
ti = (si; ai; ri; si+1) ，从⽽形成了⼀个 buffer，D = t1; t2; ::::::ti。然后从这个 buffer
中随机采样来获得样本进⾏⽹络的训练。则其 loss function 可表达为：
[(
]
)
2
Li((cid:18)i) = E
′(cid:24)u(D)
s;a;r;s
yDQN
i
(cid:0) Q(s; a; (cid:18)i)
然⽽，由于 Q-learning 中存在对值函数估计的过于乐观的问题，又有了将
double Q-learning 引⼊的 deep double Q-learning [35]. 主要是将其中的 target 更换
4.3 算法及模型设计
. 61 .
图 4.4: ⽹络结构.
为如下形式：
yDDQN
i
= r + (cid:13)Q(s
′
; argmax
′
a
′
′
; a
Q(s
(cid:0)
)
; (cid:18)i); (cid:18)
(4.7)
⽽其余部分则保持和原 DQN 部分相同。
同时，因为我们的候选动作有两万多个，不能像⼀般的 DQN ⽹络⼀样，在
最后⼀层⾥⼀次性得到所有动作的 Q 值，从⽽形成策略。我们采⽤的⽅法是，⾸
先，给每⼀个候选的锦囊类型⼀个 ID。然后，将此 ID 与 state 过来得到的量进⾏
concat，这个结合的量继续送⼊⽹络进⾏计算。最终得到的就是对应的 Q(s; a)。
⽹络的结构可以⼤概如下图 4.4所⽰：
4.3.2 分层采样池
通常，在 deep RL 中，都会采⽤ replay memory 来保存历史样本。从 nature
DQN 开始，该⽅法已经取得了很⼤成功和效果。然⽽，在以往所遇到的问题中，
环境的分布通常较为固定。但是在我们的问题中，Agent 是推荐系统，环境则是
. 62 .
第四章 强化学习在淘宝锦囊推荐系统中的应⽤
⼴⼤的⽤户，策略则是向⽤户推荐何种类型的锦囊，⽽⽤户的分布通常变化⾮
常快。⽤户分布的变化会带来⼀些问题，其中之⼀是由于不同类型的⽤户⾏为
之间有很⼤习惯差异，当⽤户分布发⽣变化时，则可能导致错误的策略评估。考
虑⼀个特殊情况，1 型⽤户更愿意点击 A 型锦囊，⽽⽬前 1 型⽤户占⼤多数，当
系统已经学会了主要推荐的是 A 锦囊的策略时，2 型⽤户却成为平台上的多数
⽤户。在这种情况下，显然会导致学习效果变差。
为了缓解这个问题，我们决定使⽤ stratified sampling replay 代替以前的 ran-
dom replay。同样，仍然有⼀个 replay memory ⽤于保存以前的那些样本。然⽽，
与 random replay 不同，我们会在某些指标上进⾏分层抽样，⽐如根据⽤户性别，
年龄，购买⼒等，⽽不是随机进⾏采样，从⽽使得具有更⼩的⽅差。
其中，在分配各层样本数量时，有多种⽅案，我们主要尝试了 Proportional
Allocation 以及 Optimal Allocation 两种⽅法并进⾏实验。其中，前者是按照各层
在总体中占有的⽐例来进⾏同⽐例的采样。后者的分配⽬标则是需要使得最后
的⽅差最⼩，当然，这⾥就需要先估计各层数据的均值，⽅差等。[27, 14]
4.3.3 基准约减
除了⽤户分布的巨⼤变化之外，整个⽤户群体在不同时间段的⾏为特性也
会发⽣波动，可能⽤户在晚上就是⽐⽩天要更加愿意去点击。⼀种情景是，当系
统推荐策略保持不变时，锦囊的点击率会突然增加，然⽽这可能仅仅是因为⽤
户更愿意在那个时候去点击。这也显然导致了⼀个问题：当策略没有朝正确的
⽅向更新时，却由于外部环境的变化，它被误认为取得了很好的学习表现，从
⽽使得最终的学习效果并不好。
出于这个原因，我们提出了另⼀个⽅法：基准约减 (Benchmark Elimination)。
⾸先，我们随机选择⼀些⽤户作为我们的基准⽤户。其次，以优化 CTR 为⽬
标，⽤离线监督学习⽅法进⾏训练，得到⼀个模型，并将此固定模型应⽤于上
述⽤户群体中。之后，对于这些⽤户，我们也实时记录⽤户的浏览、点击等⾏
为，并⽤和上述相同的⽅案计算平均奖赏 rbenchmark。最后，在我们的推荐系
统⾥的⽤户⾥也得到了奖赏 rorigin 之后，将两种 reward 之差作为最终的奖赏：
4.3 算法及模型设计
. 63 .
Algorithm 2: 锦囊类型推荐算法
Require:
(cid:0): (cid:18) 的⼀份副本,
(cid:0): target network 更新频率
D: 空的 replay 池, Nr: replay 池的最⼤容量
(cid:18): 初始的⽹络参数, (cid:18)
Nb: 训练的 batch size, N
rb: 实时的 benchmark 奖赏.
1: for episode e 21,2,3,...M do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
for t 2 0; 1; ::: do
从环境 E 获取 state s, action a, reward rorigin, next state s
基准约减: r = rorign (cid:0) rbenchmark
将 transition tuple (s; a; r; s
if jDj (cid:21) Nr then
替换掉最久之前的的 tuple
end if
⽤分层采样的⽅法采样⼀个 mini batch：Nb tuples (s; a; r; s
对每⼀个 Nb tuples 建⽴⽬标值函数
′
定义 amax(s
; a
′
) 加⼊到 D
; (cid:18)) = argmaxa
{
′
′ Q(s
; (cid:18))
′
′
′
) (cid:24) SRS(D)
yj =
r + (cid:13)Q(s
′
′
; amax(s
r;
; (cid:18)); (cid:18)
(cid:0)
′
if s
is terminal
);
otherwise:
(4.8)
⽤如下 loss 进⾏梯度下降过程：jjyj (cid:0) Q(s; a; (cid:18))jj2
每间隔 N
(cid:0) 步进⾏更新：(cid:18)
(cid:0)   (cid:18)
13:
14:
15:
16: end for
end for
r = rorigin (cid:0) rbenchmark. 这种⽅法是为了抵消外部环境的⾃然变化带来的负⾯影
响，使得推荐系统能够向着更加正确的⽅向来进⾏更新。
. 64 .
第四章 强化学习在淘宝锦囊推荐系统中的应⽤
4.3.4 算法流程
如上所述，我们采⽤了强化学习的值函数估计的⽅法，并使⽤了深度神经
⽹络来进⾏拟合。此外，为了解决⽤户分布和⽤户群体快速变化这两个问题，我
们增加了分层采样 replay 和基准约减两个⽅法。算法流程如 2 所⽰。基准约减
将在第 4 ⾏进⾏，之后，这些 transitions 都被存储在 D，并将按照⼀定的指标进
⾏分层随机抽样 (SRS)。然后就可以⽤学习到的 Q 值来得到策略。
4.4 实验与总结
我们将上述算法应⽤到淘宝实际的锦囊类型推荐中进⾏了实验，最终的
CTR 和 UV 均⽐以往⽅法有所提升。⼀⽅⾯，我们对⽐了我们强化学习的⽅
法和直接⽤⼀样的⽹络结果的单纯监督学习⽅法，发现强化学习⽅法有所增益。
另⼀⽅⾯，我们也测试了提出的 stratified sampling relay 对⽐ random replay 的效
果, 以及⽤了基准约减对⽐直接原始 reward 的表现，实验的结果证明了这两种
⽅法对系统的效果均起到了很好的作⽤。
以往的强化学习⽅法更多的应⽤在游戏领域中，⽽电商环境具有更加明显
的不确定性，⽅差更⼤。本⽂提出的⽅法也是试图探索⽅法来减弱这种影响。然
⽽，笔者能⼒有限，在应对真实复杂的环境，以前的⽅法确实仍然存在太多问
题。⽐如⽤户的特征并不完全在淘宝平台上展现，并不完全符合马尔科夫性等
等，还需要更多的研究和尝试，以求取得更多的突破。
第五章 基于强化学习的引擎性能优
化
5.1 背景
和⾕歌类似，淘宝的搜索排序是建⽴在数⼗种排序因⼦（当然，⾕歌有数
百种）之上的，随着近年来深度模型的⼴泛应⽤，越来越多的复杂且耗时的因
⼦被引⼊到搜索排序中，这⼀⽅⾯带来了排序效果上的收益，另⼀⽅⾯，也对
线上引擎的性能带来了新的挑战，⽽这样的挑战不仅来⾃于⾼耗时排序策略⽆
法全量⽣效，也来⾃于双 11 这样的突发性⾼流量对引擎的瞬间压⼒。
通常来说，⾯对这样的⼤规模流量访问，当引擎的处理能⼒不⾜时，通常有
2 种做法：⼀种是算法端准备⼀个廉价的⽅案，去掉效果好但耗时⾼的因⼦，这
个⽅案⽐最好的策略差很多，但是引擎肯定可以扛得住；另⼀种是引擎端执⾏临
时性的降级⽅案，⽐如，下线不重要业务、减少召回数量、通过粗排过滤更多宝
贝等⽅法。可以看到，不管哪⼀种，都是效果对性能的硬妥协（hard compromise），
所以我们尝试问⾃⼰这样⼀个问题，can we make it softer, and smarter?
当然，答案是，完全有可能！当我们观察我们线上的排序因⼦，我们发现，
即使每⼀个因⼦的上线初期都经过了 A/B 测试验证了其有效性，但总体来看，
因⼦之间的相关性仍然很⾼，我们抽取了⼀个⼦集，计算了两两之间的⽪尔逊
积矩相关系数（Pearson product-moment correlation coefficient），如下图所⽰：
格⼦中颜⾊越⽩，对应的 2 个因⼦之间的相关性就越⾼，不难看出存在⼤
. 66 .
第五章 基于强化学习的引擎性能优化
图 5.1: 排序因⼦的⽪尔逊积矩相关系数矩阵
量的相关因⼦对；另⼀⽅⾯，我们还发现，不同的上下⽂（context，这⾥指得是
< user; query > 对）下，商品的转化率也是差异很⼤，举个例⼦，⾼购买⼒的
⽤户在长尾 query 上的查询，转化率通常要⽐平均⾼很多，因此我们猜想，在类
似这样的流量下，是不是仅仅使⽤廉价的因⼦就够了？
5.2 问题建模
如上节所介绍，对于⼀次搜索请求 (u; q)i，淘宝的排序引擎会依次计算待排
序的每个⽂档 d 的 n 个排序因⼦，即 s(d) = (x1(d); x2(d); :::; xn(d)) 2 Rn。当然，
部分因⼦的计算会同时依赖 u,q 和 d，但这⼀点对我们考虑的问题是透明的，所
以不失⼀般性，我们⽤ xj(d) 来指代 xj(u; q; d)。最后将这些因⼦输⼊到⼀个最
终 ranker 进⾏总分的计算
5.2 问题建模
(
)
F (u; q; d) = f
x1(d); x2(d); :::; xn(d)
. 67 .
(5.1)
值得指出的是，这⾥并没有对 f 的形式做任何假设，可以是⼀个线性模型，
⼀个 DNN ⽹络，甚⾄是⼀个 GBDT。
我们先考虑某⼀个特定 context 的优化，即对某个 (u; q) 表⽰，召回之后有
m 个商品待排序，我们可以使⽤全部因⼦集合 Ω 计算
Fo = [f
s(d1)); f (s(d2)); :::; f (s(dm)
]
亦可取⼀个⼦集 S (cid:26) Ω，计算近似值
(
)
Fa = [f
(cid:25)S(s(d1))); f ((cid:25)S(s(d2))); :::; f ((cid:25)S(s(dm))
]
这⾥的 (cid:25)S((cid:1)) 指的是因⼦全集向⼦集的映射，于是我们的⽬标可以写成
(5.2)
(5.3)
(
)
(5.4)
这⾥ D(pjjq) 表⽰的是 KL 距离，⽽⽬标中的第⼆项表⽰是⼦集的⼤⼩，其
min
S(cid:26)Ω
D(FojjFa) + (cid:21)jjSjj0
直觉含义是，在尽量⽤少的因⼦的情况下，尽可能逼近原先的排序函数 Fo。
然后，即使对单个 context，Eq. (2) 都不是特别好解的问题，其本质上是⼀
个 optimal subset selection 问题，可以证明是 NP-Hard 问题。换⾔之，我们要尝
试对所有的 (u; q)，都要分别求解⼀个 NP-Hard 问题，这显然是不现实的。解决
这个问题的第⼀步，是把最优⼦集的解在 context 特征层上进⾏泛化，即我们不
直接求解⼦集，⽽是通过定义：
Su;q = H(u; qj(cid:18))
(5.5)
转⽽去求解⼀个全局的模型参数 (cid:18)。同时，在机器学习的多个⼦领域都有
optimal subset selection 的问题，例如 feature selection，ensemble pruning 等，其
. 68 .
第五章 基于强化学习的引擎性能优化
图 5.2: 因⼦选择的序列决策模型
中不乏很多有效的启发式以及近似算法。受 Google 近年来的⼯作的启发（通
过 DRL 求解 TSP 问题），我们将⼦集选取定义为⼀个最优决策序列，序列的奖
赏，即 reward，则可以定义为我们想要的 metric，例如将我们的 loss 取反。由于
reward 可以在模拟环境中得到，因此可以通过离线的充分训练，让模型有机会
探索到更优的解，同时通过策略梯度更新模型，直⾄收敛。
状态的定义是⼀个关键环节，在我们的⽅案中，我们将排序模型作为环境
相应 Agent 的动作请求，⽽ Agent 通过转移状态和 reward 来决定下⼀次的动作。
因此，如何设计状态等 MDP 的关键环节成为⽅案的核⼼。我们将⼀步⼀步展开
我们的设计。
5.2 问题建模
. 69 .
图 5.3: 状态转移建模
5.2.1 状态定义
为了能够包含 contexts 的信息，我们在状态中引⼊了⽤过的特征（包括：年
龄，性别，购买⼒），query 特征（query ⾏业，包含⼆级类⽬），记录前⾯步骤
的决策的动作 ai 以及 T 当前决策的总步骤 T 。所以最终状态的定义如下：
s = (age; sex; power; a1; a2; : : : ; an; T )
(5.6)
这样状态不仅包含了⽤户与 query 的上下⽂信息，同时包含了历史的决策
信息。由于状态特征不同维度的尺度不⼀样，我们会将所有维度的特征值归⼀
化到 [0; 1] 区间内，再进⾏后续处理。
5.2.2 动作空间设计
对于每个状态 s，动作 ai 2 fSkip; Evalg; 其中 Eval 代表 feature xi 被保留
作为排序 feature，反之，skip 代表 feature xi 不被保留作为排序标准。
5.2.3 状态转移函数
状态转移函数 T 我们的设计⽅中⽐较简单，如图5.3所⽰：
agent 根据当前状态 s, 做出决策，选择动作 ak；这时将动作 ak 存储在 s
′ 中，
同时最后⼀维计数 k + 1。重复以上过程直到最后⼀维到达某个值。
. 70 .
第五章 基于强化学习的引擎性能优化
5.2.4 奖赏函数的设计
奖赏函数在我们的⽅案中处于核⼼位置，正是由于正确的奖赏函数设计，才
使得强化学习算法在保证 ranking effectiveness，同时最⼤化的节省搜索引擎的性
能开销。⾸先，我们定义⼀个 fSkip; Evalg 到 f0; 1g 的 b 函数
b(ak) =
if ak = Skip
if ak = Eval
(5.7)
我们的主要⽬标是在保持排序的有效性（⽐如我们的⽅案是保持原有的序）
的基础下，同时尽最⼤的可能减少 feature 的使⽤。所以我们的奖赏函数在⿎励
减少使⽤ Feature 的同时，当排序结果太差的时候会给出⼀个惩罚 (penalty). 定
义如下：
8<:0
1
8<:rp
0
8<:0
ci;j
k
∑
∑
T (sk; ak) =
t < C
otherwise.
(5.8)
ni
j=1
ni
其中，t =
I
i(k)) 定义了两个排序的 pairwise
loss。如果这个 loss 太⼤，超过某个阈值 C, 就会触发⼀个很⼤的惩罚 rp 使得
agent 减少 drop feature 的数量。然后我们在定义没有没有 penalty 的奖赏函数：
k=1;k̸=j 1(cid:25)opt((cid:25)
I
i(j) > (cid:25)
^R(sk; ak) =
if ak = Skip
if ak = Eval
(5.9)
这⾥ ci;j
k 的 feature xi;j
k 的计算开销函数. 直觉上, 这个设计能更多的倾向于
跳过⾼开销的 feature。最后我们把上⾯两个函数联合起来，就得到了我们想要
的最终奖赏函数：
R(sk; ak) = (cid:0) ^R(sk; ak) (cid:0) T (sk; ak):
(5.10)
这个函数设计既能达到节省性能开销，同时也能保证排序的有效性。
5.3 算法设计
. 71 .
图 5.4: 系统框架图
5.3 算法设计
直觉上来讲，在原有学习的基础我们的⽬的是学习⼀个 b 向量，具体设定
见图5.4：
5.3.1 Loss Function
我们的 Loss 函数定义如下
J ((cid:18)) = Eqi [L(b((cid:18)); qi; wqi)] ;
(5.11)
这⾥ b((cid:18)) 是 (cid:18) 2 Rd 参数化的函数 d > 0 是参数向量的维度. 我们重新将奖
赏函数写为：
. 72 .
第五章 基于强化学习的引擎性能优化
(
(cid:0)JL;(cid:18)
jQj∑
(
i=1
min
(cid:18)
1jQj
)
~(cid:18); qi; wi
+ (cid:22)
)
(cid:13)(cid:13)(cid:13)~(cid:18)
(cid:13)(cid:13)(cid:13)
1
(cid:13)(cid:13)(cid:13)~(cid:18)
(cid:13)(cid:13)(cid:13)
1
这⾥
是 L1 泛数, (cid:22) 是正则参数，然后
)
∫
~(cid:18); pi; wi
p(cid:18)((cid:28) )R((cid:28) ) log p~(cid:18)((cid:28) )
p(cid:18)((cid:28) )
d(cid:28)
=
(cid:28)
这⾥ p(cid:18)((cid:28) ) 是轨迹 (cid:28) 出现的概率，(cid:25) 是我们的策略函数，即
p(cid:18)((cid:28) ) = p0(s0)
p (sm+1 j sm; am; qi; wi) (cid:25)~(cid:18)(am j sm; qi; wi);
(
JL;(cid:18)
M(cid:0)1∏
M∑
m=0
m=0
(5.12)
(5.13)
(5.14)
(5.15)
R((cid:28) ) =
1
M
(cid:13)mrm+1;
5.3.2 Actor-crtitic 方法
我们采⽤⽬前流⾏的 Actor-critic 来优化上述的 loss function，其中我们利⽤
⼀个 policy ⽹络来作为 actor，然后利⽤⼀个参数化的⽹络来估计每个状态 sk 的
值函数。因此，我们的 critic ⽹络的⽬标函数如下：
(cid:18)c(sk) (cid:0) rk+1 (cid:0) (cid:13)V (cid:25)
(cid:13)(cid:13)V (cid:25)
(cid:13)(cid:13)2 :
L((cid:18)c) =
(cid:18)c(sk+1)
(5.16)
其中 (cid:18)c 是刻画 critic ⽹络的参数。我们的算法伪代码如下:
5.4 理论分析
⽬前我们初步得到⼀下理论结果，更多的理论结果与分析将在以后的⽂章
中陆续给出：
5.5 实验效果
. 73 .
图 5.5: 算法伪代码
图 5.6: 理论结果
5.5 实验效果
基于强化学习端的训练主要是在实时计算平台完成，在引擎实时⽣效，⽇
常测试成交转化和成交额均没有下跌（尽管在我们的设计中，是允许指标微跌
的），同时节省了⼤约 30% 搜索引擎的耗时开销，在双⼗⼀当天，我们也上线测
试，在全量已经减少精排数的基础之上，相⽐基准桶，再节省了 20% 的引擎性
能开销。
. 74 .
第五章 基于强化学习的引擎性能优化
图 5.7: 实验结果
5.6 总结
我们将强化学习应⽤到了搜索引擎的性能优化上⾯，⽬前在业界据我们所
知是⾸次应⽤。这位强化学习在⼯业界的应⽤提供了⼀种新的思路，也为业务
压⼒越来越⼤的淘宝搜索提供了⼀种优化⽅案。⽬前我们已经取得了初步的实
验结果，我们将继续优化我们的⽅法，希望能够为集团的其他基础设施也提供
类似的优化⽅案。
第六章 基于强化学习分层流量调控
6.1 背景
福利经济学告诉我们，市场可以解决两⼤问题，效率和公平。在满⾜⼀定
的条件情况下，通过市场机制可以实现帕累托最优，达到单独改变任何⼀个个
体都不能实现更优的状态，以此实现效率的最优化。但效率最优往往是不够的，
⼀个贫富差距巨⼤的社会仍然有可能是帕累托最优的，但是是⼀个极不稳定的
状态，⼀个稳定的社会结构还需要考虑公平，福利经济学第⼆定理因此指出，通
过改变个体之间禀赋的初始分配状态，仍然可以通过竞争性市场来达到帕累托
有效配置，从⽽兼顾公平。
事实上，今天的淘宝俨然已经成为了⼀个规模不⼩的经济体，因此，社会
经济学⾥⾯讨论的问题，在我们这⼏乎⽆不例外的出现了。早期的淘宝多数是
通过效率优先的⽅式去优化商品展⽰的模式，从⽽产⽣了给消费者最初的刻板
印象：低价爆款，这在当时是有⼀定的历史局限性⽽产⽣的结果，但肯定不是
我们长期希望看到的情形。因为社会⼤环境在变化，⼈们的消费意识也在变化，
如果我们不能同步跟上，甚⾄是超前布局的话，就有可能被竞争对⼿赶上，错
失良机。因此有了我们近⼏年对品牌的经营，以⾄于现在再搜索“连⾐裙”这样的
词，也很难看到 9 块 9 包邮的商品，⽽这个在 3 年之前仍然很常见。⽽这⾥的
品牌和客单等因素，是通过⼀系列的计划经济⼿段来进⾏⼲预的，类似于上⽂
福利经济学第⼆定理中的禀赋分配，依据的是全局的的观察和思考，很难⽽且
也不可能通过⼀个局部的封闭系统（例如搜索的排序优化器）来实现。
因此，越来越多的运营和产品同学，鉴于以上的思考，提出了很多⼲预的
. 76 .
第六章 基于强化学习分层流量调控
图 6.1: 局部最优和全局最优
分层，这⾥的分层指的是商品/商家类型的划分，可以从不同的维度来划分，⽐
如，按照对平台重要性将天猫商家划分成 A、B、C 和 D 类商家；按照品牌影响
⼒将商品划分为⾼调性和普通商品；按照价格将商品划分为⾼端、中等、低端商
品等。⽽早期的算法同学对这些可能也不够重视，⼀个经典的做法即简单加权，
这通常往往会带来效率上的损失，因此结果⼤多也是不了了之。但当我们认真
审视这个问题的时候，我们其实可以预料，损失是必然的，因为⼀个纯粹的市
场竞争会在当前的供需关系下逐步优化，达到⼀个局部最优，所以⼀旦这个局
部最优点被⼀个⼤的扰动打破，其打破的瞬间必然是有效率损失的，但是其之
后是有机会达到⽐之前的稳定点更优的地⽅。
所以，这其实给我们算法同学带来 2 个问题：
(1) 如果尽可能的减少瞬时损失？
(2) 如何尽快到达新的有可能更优的局部最优点？
对应的解决⽅案也很⾃然：
(1) 进⾏个性化的⼲预，减少不必要的损失，例如⼲预的分层为物流时效，
那么当时对物流不敏感⽽对销量更看重的那些⽤户，则没有必要进⾏很
强的⼲预
6.2 问题建模
. 77 .
(2) 通过更⼴泛，更 smart 的 exploration，仍然以上⾯的例⼦，因为当前的
整体排序没有考虑物流时效，所以我们的数据中就没有这样的属性，所
以我们⽆法从监督学习来学习到类似更多次⽇达这样的商品被排到⾸
页的效率会如何变化，这只能逐渐的“试”出来，再从之后的⽤户反馈中
总结经验，是⼀个典型的“trial and error”的过程
所以当我们进⼀步抽象时，会发现这⾃然的定义了⼀个强化学习问题：个性
化的⼲预可以看作针对不同的状态，所采取的动作不⼀样，⽽更⼴泛，更 smart
的 exploration 则对应着要将强化学习的搜索学习过程。
6.2 问题建模
我们把搜索⾏为看成是⽤户与搜索引擎交互的 MDP，搜索引擎作为 agent、
观察到的⽤户和搜索词信息作为 state、排序策略作为 action（流量调控 feature
只是众多 action 中的⼀员）、⽤户反馈（pv/click/pay）作为 reward，排序参数优
化问题也可通过 RL 来求解。为了引⼊流量结构变化的影响，我们将分层流量占
⽐的变化和⽤户⾏为反馈⼀起作为 reward，具体地
• 搜索场景下的上下⽂通常包括 query profile 和 user profile，其中 query profile
由 query 的物理属性（词性、词长度等）和淘宝属性（类⽬、⾏业等）组
成，user profile 由 user 长期的⾏为偏好、实时的状态和⾏为序列表⽰，因
此我们将这些表⽰为 state，记为 s 2 Rd。
• 假设需要进⾏⼲预的信号有 m 个，把每个分层抽象成⼀个 rank feature，如
果商品属于该分层则 score 为 1，否则为 0；每个分层对应的 weight 组成
action。一个常用的 trick 是，不直接输出 action 的绝对值，而是在神经网
络的最后一个输出层，使用 sigmoid（亦可使用 tanh，二者是可以相互表
示的），将 actor 网络的输出每一维限定在 [0; 1] 之内，即 o 2 [0; 1]m，然后
再经过⼀个变换进⾏⽣效
ak = Lk + (U k (cid:0) Lk)ok;8k 2 f1; 2:::; mg
(6.1)
. 78 .
第六章 基于强化学习分层流量调控
这⾥ U k 和 Lk 是第 k 维分层 rank feature 的权重的 upper bound 和 low bound，
⼀般来⾃于领域知识，但通常受限于经验的局限，因此我们尝试了⼀种新的⽅
法进⾏⾃动赋值，将在下⼀节进⾏阐述。3. reward 设计的第⼀要素即分层⽐例，
即展⽰商品中属于分层商品占总商品的⽐例 pi((cid:25))。与此同时，由于在流量调控
的同时需要兼顾效率，⽤户的⾏为反馈必须作为 reward 考虑的因素，反馈中考
虑 click、pay 和 cart ⾏为，每种⾏为的影响因⼦不同，pay>cart>click，这⾥统⼀
表⽰为每个 PV 中⽤户 click、cart、pay 的数量 nclick; ncart; nbuy 的⼀个函数，即
GMV(nclick; ncart; nbuy)。此外，分层⽐例需要设定对照组，举个例⼦，⽺绒衫的
搜索结果中⾼价商品⽐例明显⾼于⽑⾐，因为 query 本⾝已经体现出了价格差
异，与流量调控的 action 并⽆关系，所以在计算实际的分层⽐例时，我们会将
其原值减去同 query 在基准桶的分层⽐例 pi((cid:25)basic)，即
r(s; a) = GMV(nclick; ncart; nbuy) +
pi((cid:25)) (cid:0) pi((cid:25)basic)
(6.2)
m∑
(
(cid:21)i
i
)
6.2.1 Dynamic Action Boundary by CEM
上⽂的建模建⽴在 PV 粒度的奖赏，但是由于⽤户的⾏为的不确定性（这
个不确定性⼀⽅⾯来⾃于⽤户的点击购买⾏为具有随机性，另⼀⽅⾯来⾃于我
们对⽤户建模的不确定性），所以瞬时奖赏会有很⼤的 variance，会对学习带来
很⼤的影响，所以此时如果在整个实数空间进⾏搜索的话，很有可能收敛不了。
因此我们设计了 upper bound 和 low bound，使得 RL 算法只需要在局部进⾏搜
索，降低了学习的难度，但这又带来了 2 个新的问题：1. 如何确保 upper bound
和 low bound 的合理性？2. 如何防⽌选取了⼀个局部的最优区间？
针对以上 2 个问题，我们设计了⼀个通过 Cross Entropy Method（CEM）的
⽅法来实时的动态更新 action 的 upper bound 和 low bound，具体⽽⾔，我们不
考虑 state，考虑⼀个全局最优 actionak，我们假设其符合⼀个⾼斯分布，在
ak(cid:3) 2 N ((cid:22)k; (cid:27)2
k)
(6.3)
每次迭代的开始，我们从这个分布上采样 s 个样本，即 Ω1; Ω2; :::; Ωs，然后
对这些 action 进⾏充分的投放，得到对应的 action 的⼀个充分置信的 reward 值，
6.2 问题建模
即
R(Ω1) =
R(Ω2) =
R(Ωs) =
1
N1
1
N2
1
Ns
i
N1∑
N2∑
Ns∑
i
ri(Ω1)
ri(Ω2)
(cid:1) (cid:1) (cid:1)
ri(Ωs)
i
. 79 .
(6.4)
(6.5)
(6.6)
(6.7)
然后我们对 R(Ω1); R(Ω2); :::; R(Ωs) 进⾏排序，选取出 top p 的⼦集 D，以
最⼤化⾼斯分布产⽣这些样本的概率，即
∑
max
k;(cid:27)2(cid:3)
(cid:3)
(cid:22)
(cid:3)
k; (cid:27)2(cid:3)
k ) =
f ((cid:22)
log N (Ωij(cid:22)
(cid:3)
k; (cid:27)2(cid:3)
k )
(6.8)
i2D
实际上，上⾯的式⼦是有最优解的，(cid:22)
k
(cid:3)
k 即 D 中所有样本的均值，(cid:27)2(cid:3)
k 则是
所有样本的⽅差，但如果直接求解，则模型则会迭代过快，⼀⽅⾯会完全忘记
之前迭代的信息，另⼀⽅⾯，因为这个会直接输出给上⾯的 RL 学习的 actor 使
⽤，所以 bound 不能变化多块，否则 RL 很有可能不能及时跟上变化，因此，我
们采⽤了缓慢更新的⽅法，即
(cid:22)k   (cid:22)k + (cid:11)
(cid:27)k   (cid:27)k + (cid:11)
@f
@(cid:22)k
@f
@(cid:27)k
(6.9)
(6.10)
在更新之后，我们使⽤下⾯的⽅法赋值第 k 维 action 的 upper bound 和 low
bound，确保 RL 调节的 action 在⼀个全局较优的空间内
Lk = (cid:22)k (cid:0) 2(cid:27)k
U k = (cid:22)k + 2(cid:27)k
(6.11)
(6.12)
(6.13)
我们的 RL 实现选择了我们⾃⼰在 AI4B 中实现的 DDPG，整体流程如下：
. 80 .
第六章 基于强化学习分层流量调控
图 6.2: 实验结果
• 使⽤ CEM 选取初始 upper bound 和 low bound
• 启动 RL 进⾏学习，与此同时，使⽤ CEM 动态调节 upper bound 和 low
bound
6.3 实验效果
双 11 期间在 gmv 损失可控的情况下，⽬标商家流量占⽐⼤幅提升。
6.4 总结与展望
本⽂的主要⼯作是基于强化学习的分层流量调控框架实现，在⼀⼩部分流
量上探索分层调控策略对指标的影响，再结合探索策略的收益在剩余流量上精
细化投放。作为流量结构调整的实施部分，框架本⾝还有很多需要改进的地⽅，
在 reward 设计⽅⾯，不同分层流量的 reward 融合、分层流量 reward 与⾏为反馈
reward 的融合都是需要深⼊的⽅向；在探索策略设计⽅⾯，⽬前还是单个维度
explore，效率较低，后⾯会尝试多个维度同时 explore。另外，⽂章开头提到的
如何评估流量结构变化的长期影响是⼀个更有价值的课题。
第七章 风险商品流量调控
7.1 背景
7.1.1 为什么进行风险商品流量调控
风险商品长期以来都是淘宝平台⾯临的顽疾，如果不加以合理的控制，会
严重威胁到平台在消费者⼼⽬中的形象。当前平台⾯临风险商品多，风险类型
多样化，例如：
• 假货商品通过复制、仿制⼤牌商品，以低价的⽅式出售牟取暴利，危害极
⼤，⾮常容易产⽣ PR 事件；
• 次品、瑕疵品等劣质商品，差评率和退款率⾼，严重影响平台的正品感知，
损坏平台在买家⼼⽬中的形象；
• ⼤量的标题品牌堆砌、品牌标属不⼀致、图⽂不⼀的侵权商品是权利⼈关
注的重点，被称为商品的霾；
• 未按淘宝规范要求及⼴告法发布的滥发商品，违规类型多、违规⼿段层出
不穷，且商品量⼤。
传统的风险治理⼿段主要有两种：
1. 通过商品管理，将违规商品下架或删除，对卖家进⾏罚分、关店等，使⽤
条件为确定性风险，投诉成⽴率低，⽽且还要考虑客满压⼒；
. 82 .
第七章 风险商品流量调控
2. 流量处罚，治理⼿段主要为在搜索端对商品直接屏蔽或减固定的分数。优
点是快速⾼效，但缺点为不考虑⼤盘情况，流量调节的粒度不够细致。
因此，我们亟需⼀套精准流量调控机制，在⼤盘稳定的前提下，有效降低
风险商品的流量，提升平台洁净度。
7.1.2 为什么使用强化学习调控
在风险流量调控项⽬ 1.0 中，我们基于⼈⼯设定的权重对风险商品进⾏调
控，在⼀定程度上实现了风险商品流量调控的⽬的，但也存在⼀些明显问题：
1. 基于⼤盘整体的表现确定的全局权重，⽆法实现更细粒度的流量调控。不
同风险状态下的 query，流量调控的权重也是相同的，其流量调控效果显
然⼤打折扣；
2. 降权权重是固定的，⽆法随着环境的变化⽽动态调整。
因此，项⽬⽬标是采⽤更加智能的算法，将流量调控的粒度从全局细化到
query，并实现实时动态的权重寻优。最终在⼤盘稳定的前提下，获取更好的风
险流量调控效果。建模思路：已知 query 当前的状态，选择⼀组排序 feature 的
权重，能够最⼤程度的降低当前 query 下的风险商品流量，同时平衡平台收益。
这是⼀个典型的序列决策问题，⾮常适合⽤强化学习的框架来求解。
7.2 基于强化学习的问题建模
7.2.1 状态空间的定义
我们的⽬标是在⼤盘稳定的前提下降低风险商品的流量，因此 state 的定义
分为 query 的正向表现 (CTR/CVR/...) 以及 query 的负向风险程度，包含了 query
的离线特征和实时特征。
7.2 基于强化学习的问题建模
. 83 .
图 7.1: 状态的定义及计算流程
其中，query 的离线特征⽤于刻画 query 的长期特性，⽐如：历史 7 天的 CTR、
CVR 等，可以反映 query 历史的成交效率⾼低；历史 7 天的品质退款率、假货退
款率可以反映 query 下历史退款风险的⼤⼩。除了离线特征以外，我们还考虑了
query 的实时特征，这能让我们对 query 的短期状态变化进⾏快速感知，对环境
变化做出及时的响应，包括 query 的实时风险分、实时 CTR、实时 CVR 等。在
我们的定义中，query 的实时特征是通过对⽤户在 query 下长度为 T 的时间段内
的⾏为数据累积的结果，之所以这样做是因为我们⽤到了 query 的 CTR、CVR
这样的特征，累积长度为 T 的时间可以有效地降低特征计算的偏差，在我们的
实现中 T 取 15 分钟。
. 84 .
第七章 风险商品流量调控
7.2.2 动作空间的定义
Action 定义为⼀组 rank feature 的权重向量，每⼀维代表⼀个 rank feature 的
权重⼤⼩。我们使⽤的 rank feature 包括三个：
1. 商品的风险分；
2. 商品是否为⾼风险商品；
3. 商品的 GMV 分。
特征虽然不多，但却是我们实现调控的有⼒抓⼿。商品的风险分和是否⾼
风险商品可以帮助我们有效调节风险商品的曝光程度，GMV 分可以帮助我们在
降低风险流量的同时，平衡⼤盘的正向收益，其中 GMV 分计算时融合商品的炒
信风险，确保正向的纯净。
7.2.3 奖赏函数的定义
奖赏函数设计时，同样会兼顾正负向收益，定义如下：
正向 reward：
∑
Rp =
(cid:12)1clki + (cid:12)2 (cid:2) pricei
(cid:2) ordi
负向 reward：
Rn = (cid:0)
∑
i
i
rscorei (cid:2) ((cid:13)1pvi + (cid:13)2clki + (cid:13)3ordi) (cid:2) 1:5yi
总收益为 R = (cid:11)1Rp + (cid:11)2Rn
其中：
(7.1)
(7.2)
1. Rp 为正向收益，⽬的是平衡⼤盘的正向指标 (GMV、CTR)，主要包括两
个部分，分别为点击和成交对应的正向收益，当⽤户点击或者购买商品时
会产⽣对应的正向 reward；
7.2 基于强化学习的问题建模
. 85 .
2. Rn 为负向收益，⽬的是引导搜索排序尽可能的降低 query 下的风险商品
流量，由三个部分组成，⽤户在 query 下的每次曝光、点击和成交都会根
据对应商品的风险⼤⼩进⾏加权，进⽽计算得出负向 reward；
3. ord、clk、pv 分别表⽰ query 下的商品是否有成交、点击、展现，y 表⽰
商品是否为⾼风险的商品，当⼀个商品为⾼风险商品时，我们会对其负向
reward 加权，引导搜索排序的结果尽可能的少展⽰这样的⾼风险商品，进
⽽降低风险商品流量；
4. rscore 表⽰商品本⾝的风险分，(cid:11)、(cid:12)、(cid:13) 为调节因⼦，可以调节总收益中不
同部分的重要程度，⽐如正向 reward 和负向 reward 的相对重要程度，展
现、点击、成交的相对重要度等。
7.2.4 模型选择
在模型选择的问题上，我们调研了多种不同的算法：
1. 基于值函数的 Q-Learning、DQN 等，这些⽅法适⽤于离散动作空间，⽽我
们场景下的 action 为 rank feature 的权重值，是⼀个连续变化的量，所以这
些算法不适合于我们的问题，⽽且 DQN 算法在实际训练时也有收敛慢的
缺点；
2. 经典的 Policy Gradient 算法虽然适⽤于连续动作输出的场景，但缺点是训
练的过程太慢，因为算法必须在每⼀轮 Episode 结束后才能进⾏梯度的估
计和策略的更新；
3. Actor-Critic 算法通过引⼊ critic ⽹络对每⼀步的 action 进⾏评价解决了必
须在 Episode 结束后才能更新策略的问题，算法可以通过 step by step 的⽅
式进⾏更新，但该算法的缺点是由于使⽤连续的样本更新模型，样本之间
的相关性强⽽影响模型的收敛性；
. 86 .
第七章 风险商品流量调控
4. Google DeepMind 团队把在 DQN 训练中取得成功的 Experience Replay 机
制和 TargetNetwork 两个组件引⼊了 Actor-Critic 算法，极⼤的提⾼了模型
训练的稳定性和收敛性，在很多复杂的连续动作控制任务上取得了⾮常好
的效果。
我们的场景属于连续动作空间问题，所以我们最终选择了性能较好的 DDPG
模型作为我们⽅案的核⼼算法。整体的⽹络结构如下图所⽰：
图 7.2: 模型⽹络结构
7.2.5 奖赏函数 scale
在实际的训练过程中我们发现，由于 reward 考虑了正负两个⽅⾯，正向
reward 的取值范围远⼤于负向 reward(因为正向的成交 reward 考虑了价格因素)，
导致负向的风险因素⼏乎不起作⽤，为了解决这个问题，我们对原始的 reward
计算结果进⾏了 scale 的后处理，使正负 reward 具备可⽐较性。具体地，我们使
7.3 流量调控系统架构
. 87 .
⽤ Ln 变换对 reward 进⾏了后处理，处理之后正负向 reward 的分布区间变的更
具可⽐性。
正向 reward 的 scale：
′
p =
R
(cid:11)1 log(1 + Rp)
(cid:11)1 + (cid:11)2
负向 reward 的 scale：
n = (cid:0)(cid:11)2 log(1 (cid:0) Rn)
′
R
(cid:11)1 + (cid:11)2
scale 后的总收益为 scale 后的正向 reward 和负向 reward 的和，即 R
′
n。
R
(7.3)
(7.4)
′
= R
′
p +
7.3 流量调控系统架构
我们基于搜索⼯程同学研发的 Porsche 平台进⾏ DDPG 模型的训练、query
实时特征的计算、训练样本的⽣成等任务，训练后的模型定时推送到 IGpraph 上
存储，虽然模型是实时训练的，但在具体实现时我们是每隔 10 分钟推送⼀次新
模型，避免频繁写对 IGraph 造成压⼒。QP 负责读取模型，并根据当前 query 的
特征预测 rank feature 的权重，rank 取到权重后计算最后的 rank 分，影响线上排
序效果。
7.4 线上效果
双⼗⼀当天，在保证⼤盘稳定的前提下 (GMV 不降低，客单价不降低)，平
台的假货退款率和品质退款率显著降低，⾼风险商品的流量和成交额显著下降。
. 88 .
第七章 风险商品流量调控
图 7.3: 强化学习流量调控系统整体架构
第八章 虚拟淘宝
8.1 背景
8.1.1 强化学习面临的问题
在某些场景下中应⽤强化学习（例如围棋游戏中的 AlphaGo），进⾏策略探
索的成本是⾮常低的。⽽在电商场景下，策略探索的成本会⽐较昂贵，⼀次策
略评估可能需要⼀天并且差的策略往往对应着经济损失，这是在线应⽤强化学
习遇到的⼀个普遍问题，限制了强化学习在真实场景下的应⽤。针对这个问题，
我们通过逆向建模环境，尝试构建了⼀个“淘宝模拟器”，在该模拟器上，策略探
索的⼏乎没有成本，并且可以快速进⾏策略评估。⽽且在这样⼀个模拟器上，不
仅可以对各种 RL 算法进⾏离线尝试，⽽且还可以进⾏各种⽣态模拟实验，辅助
战略性决策。
8.1.2 虚拟淘宝
8.2 学习用户行为：监督学习
模拟器的关键在于模拟⽤户的⾏为。传统的监督学习⽅法将⽤户的观察（ob-
servation）作为特征，⽤户的⾏为作为标签（label），试图在这些数据上训练得
到⽤户的⾏为策略。
这种简单的⽅式不是很奏效，原因是数据分布⾼度依赖于当时的线上策略，
. 90 .
第⼋章 虚拟淘宝
图 8.1: 真实淘宝和虚拟淘宝
图 8.2: 监督学习⽅案
导致数据会很不充分，这样由于⽅差漂移（covariate shift）带来的 compounding
error 会使得算法失效。
8.3 学习用户意图：逆强化学习
8.3 学习⽤户意图：逆强化学习
. 91 .
图 8.3: 多智能体逆强化学习
8.3.1 逆强化学习概述
强化学习是求累积回报期望最⼤时的最优策略，在求解过程中⽴即回报是
⼈为给定的。然⽽，在很多任务中，尤其是复杂的任务中，⽴即回报很难指定。
那么如何获取即时回报呢？逆向强化学习的提出者 Ng 认为：专家在完成某项任
务时，其决策往往是最优的或接近最优的，那么可以这样假设，当所有的策略
所产⽣的累积回报期望都不⽐专家策略所产⽣的累积回报期望⼤时，强化学习
所对应的回报函数就是根据⽰例学到的回报函数。简单地讲，逆向强化学习可
以定义为从专家⽰例中学到回报函数。传统强化学习在很多复杂问题上难以学
得较优策略，⽽逆强化学习通过专家策略，往往能够取得更好的效果。例如在
预测司机⾏为以及规划机器⼈步态等问题，逆强化学习都取得了很好地效果。
8.3.2 学习用户意图
⽤户看到了商品，为什么会购买？我们假设，⽤户有⼀个购买商品的意图
（intention），⽤户看到商品之后，⽤户本⾝的属性以及商品的⼀些属性使得⽤户
第⼋章 虚拟淘宝
. 92 .
有了购买的意图。我们⽤奖赏函数（reward function）r : S (cid:2)A ! R 表⽰⽤户的
内在⾏为意图，其中 S 是⽤户的观察空间（包括了⽤户的特征以及⽤户看到的
信息），A 是⽤户的动作空间。那么，如何获得 r(s; a) 呢。我们可以将淘宝⽤户
视为“专家”，⽤逆强化学习⽅法，通过淘宝⽤户的历史⾏为，学出其内在的奖励
函数。然后，利⽤强化学习⽅法，学习出⽤户的⾏为策略，即构建了⽤户⾏为模
拟器。
8.3.3 生成对抗式模仿学习
如果给了专家历史数据，逆强化学习（IRL）能够⽅法能够学出专家的奖励
函数，相对于⾏为克隆（behavior cloning）⽅法，该⽅法能够处理历史数据不够
充分的问题。然⽽迭代使⽤ RL ⽅法使得 IRL 效率⾮常低。最近，理论表明显式
地学习出奖励函数并⾮必要，可以直接学得专家策略，⽣成对抗式模仿学习在
理论上等价于逆强化学习，并且效率更⾼ [Ho and Ermon, 2016]。
8.4 构建用户行为模拟器
8.4.1 问题建模
∑
淘宝搜索是建⽴在数⼗种排序因⼦之上的，对于⽤户的每⼀次搜索请求
u = (user; query)，淘宝的排序引擎会依次计算每个⽂档 d 的 n 个排序因⼦
(x1(d); x2(d); :::; xn(d))，然后将这些因⼦进⾏加权求和得出最后的总分 s(u; d) =
i=0 wi;uxi(d)，⽤ s(u; d) 进⾏排序，最后展⽰排名靠前的商品。排序因⼦的质
量和排序权重 w(u) 的选择，都决定着排序结果的好坏。这⾥，我们重点关注排
序权重的选择问题。
n
为了在淘宝搜索中应⽤ RL ⽅法，⾸先需要为优化引擎策略建⽴马尔科夫
决策过程（MDP）M =< S; A; P; R; (cid:13) >
• 状态空间 S 我们将⽤户的⼀些特征和查询信息 u 作为状态 s，⽤户特征
我们选择了⽤户的性别、年龄、购买⼒，查询信息我们只提取到查询⾏业
8.4 构建⽤户⾏为模拟器
. 93 .
的粒度。
• 动作空间 A 我们将排序的权重 w 2 Rn 作为动作 a。
• 奖励函数 r 如果⽤户购买了商品，我们会返回⼀个正的奖励，否则返回
0。
• 策略 (cid:25) 定义参数化策略 (cid:25)(cid:18) : S ! A。
智能体（agent）在状态 st 下作了动作 at 之后，应该转移到哪⼀个状态？即
时的奖赏应该是多少？在很多强化学习问题（例如 Atari 游戏，围棋）中，这都
不是问题，因为我们与环境进⾏交互⾮常⽅便，我们只需要做出⼀个动作，然后
等待环境反馈的结果就可以了，并不会带来更多的开销。⽽在电商场景下，与
环境的交互是昂贵且耗时的，⼤量探索式的交互是不切实际的。
我们希望⽤模仿学习的⽅法学习⽤户的意图，也就是模拟线上环境。将环
境，也就是在线的⽤户，视为专家（expert）。我们专家历史数据（每天交易产⽣
的⼤量⽇志），可以从中学得⽤户的策略作为我们的环境。注意到，我们⽇志的
量虽然很⼤，但它是⾼度有偏的，因为只有发⽣购买⾏为的 pv 才会被记录，所
以基于⾏为克隆的模仿学习是不适⽤的。为了区别与训练引擎策略的 MDP 过程
M，我们⽤ Me =< Se; Ae; Pe; Re; (cid:13)e > 表⽰模拟环境时的 MDP 过程。
• 状态空间 Se 将提取的⽤户特征和引擎权重作为状态。
• 动作空间 Ae 将⽤户购买⾏为作为环境的动作。
• 奖励函数 re 训练判别器 Dw，⽤来判断 < u; w > 是否来⾃真实数据，⽤
Dw 的输出作为奖励。
• 策略 (cid:25)e 定义参数化策略 (cid:25)e(cid:18) : s ! a。
模拟器框架如图 [8.4]：输⼊是⽤户的特征（性别、年龄、购买⼒、query ⾏业），
⾸先，经过引擎⽹络，输出引擎的动作，然后，⽤户特征以及引擎动作经过模拟
器⽹络产⽣⽤户⾏为，即是否购买。另外，判别器⽹络会根据⽤户特征以及引
擎权重给出奖励。
. 94 .
第⼋章 虚拟淘宝
图 8.4: 模拟器⽹络结构
8.4.2 算法设计
我们在⽣成对抗模仿学习（GAIL）的框架下，提出了 LERD（Learn Envi-
ronment with Restricted Data）算法，过程如下：
• 初始化引擎 (cid:25) 和模拟器策略 (cid:25)e 以及判别器函数 Dw。
• 循环执⾏，直到终⽌条件。
– ⽤ (cid:25)e 和 (cid:25) 采样出⼀系列轨迹 (cid:28)i。
– ⽤真实轨迹 (cid:28) 以及采样出的轨迹 (cid:28)i 更新判别器，即在如下的⽅向上
更新 Dw
(cid:0)E(cid:28)i[∇w log(Dw(s; a))] (cid:0) E(cid:28) [∇w log(1 (cid:0) Dw(s; a))]
– 将 Dw 作为强化学习的奖励函数，⽤强化学习⽅法更新 (cid:25)e，(cid:25)。
8.4 构建⽤户⾏为模拟器
. 95 .
图 8.5: LERD 算法
8.4.3 实验结果
初步的实验结果表明，相同的引擎策略（random 策略），模拟器上模拟的购
买率与真实购买率类似，同时在模拟器上训练 TRPO 算法，得到的策略在模拟
环境上购买率有明显提升。
第九章 组合优化视角下基于强化学
习的精准定向广告 OCPC 业务优化
9.1 背景
在精准定向单品（按点击扣费）⼴告业务中，⼴告主会为⼴告设置⼀个固
定出价，作⽤在指定的场景和定向类型下。如果⼴告主能够根据每⼀条流量的
价值进⾏单独出价，可以带来两点好处
1. ⼴告主可以在各⾃的⾼价值（如点击、成交）流量上提⾼出价，⽽在普通
流量上降低出价，如此容易获得较好的 ROI（投资回报率）；
2. 流量细分后，平台能够提升⼴告与访客间的匹配效率，体现为 CTR（点击
率）、GMV （成交总额）等⽤户指标提升，⽽在⼤盘 ROI 不变的情况下，
商业指标 RPM（千次展现收⼊）也能相应提升。
在单品⼴告业务⾥，⼴告主⽆法对单流量价值进⾏评估并实时出价，所以
这个根据价值预估做智能调价的担⼦⾃然落到了⼴告平台⽅的肩膀上。2016 年
起，我们在智能调价⽅⾯进⾏了深⼊的技术探索，相关成果 [39] 已发表。
本⽂是在已有的智能调价系统基础上的改进⼯作。智能调价系统的⽬标可
以简单概括为：在保障单广告主 ROI 的约束下，提升大盘 GMV、RPM。我们
认为，原系统在达成这个⽬标的解法上有以下⼏点可以持续改进
9.2 问题建模
. 97 .
1. GMV、RPM 两指标的优化被分成单独的阶段，⼆者串联依赖于后阶段调
整尽量保证前阶段排序不变，会带来效果损失；
2. 模型需要⼿动调参，限制了模型的复杂度和迭代效率；
3. 离线调参的评价标准是预估值，⽽⾮真实效果，所以离线最优参数很可能
不是在线最优。
总结以上，我们本次改进的⽬标是，将之前的基于预估值反馈的、多⽬标
分离优化的离线学习，变为基于线上真实效果反馈的、多⽬标联合优化的在线
实时学习。强化学习技术显然和我们的需求很匹配。
9.2 问题建模
9.2.1 奖赏
背景中提到，智能调价系统的⽬标是在保障单⼴告主 ROI 约束的前提下，
提升⼤盘 GMV、RPM。关于 ROI 约束，⽬前我们通过在单个流量上根据 CVR
（预估转化率）和 HCVR（历史转化率）计算调价上界（公式 9.1）的⽅法来保障。
如此，我们问题中的奖赏就是 RPM、GMV 两⽬标。
ocpc_bid <
cvr
hcvr
bid
(9.1)
9.2.2 动作
智能调价系统中的动作是每个⼴告的调价⽐例，更确切地说是本次参竞⼴
告的调价⽐例构成的向量。
图 9.1 展⽰了我们对于 OCPC 问题的理解。如果仅考虑优化 RPM、GMV，
我们可以先将每个⼴告调价到 ROI 所允许的最⾼，然后为每个⼴告预估其展现
奖赏 RP M + (cid:11) GM V ，最后按照奖赏从⾼到低排序选出 a、b、c 三个⼴告进⾏
. 98 .
第九章 组合优化视⾓下基于强化学习的精准定向⼴告 OCPC 业务优化
展现。然⽽，这种做法打破了⼴告按照 ECPM 排序的商业逻辑，也削弱了 OCPC
做调价的意义。
图 9.1: 以调价⽐例为动作的 OCPC 问题
在 OCPC 业务中，我们的动作被限定为调价比例，排序公式被限定为 ctr(cid:11) (cid:2)
bid (cid:2) action，但流量优化⽬标仍旧是 RP M + (cid:11) GM V ，所以我们要尽可能保
证调价后按 ECPM 排序的 Top 3 ⼴告 x、y、z 正是 a、b、c。在强化学习建模中，
我们希望通过奖赏预估（Critic）指导动作⽣成⽹络（Actor）在每个⼴告上输出
适当的调价⽐例达成两种排序间的统⼀。
9.2.3 状态定义
在强化学习中，状态⾸先要能够决策动作继⽽优化（长期）奖赏，其次得是
合理转移着的。基于这两点，我们认为本次流量⼴告候选集的全部打分是我们
需要的⼀类重要状态。接下来我们将分别说明
1. 候选集信息是必要的；
2. 候选集信息主要指各种打分；
3. 候选集打分是具有⾃然转移的⽤户状态。
9.2 问题建模
. 99 .
候选集信息必要
在机器翻译任务中，我们使⽤ LSTM[12] 模型建模句⼦已⽣成部分的隐状
态（hidden state），据此从全量词库中挑选⼀个词作为下个输出。在这个任务⾥，
我们每次选词的候选集是固定的，即全量词库，所以我们并没有将全量词库信
息建模到状态⾥。
然⽽，在电商的搜索、推荐和⼴告业务中，出于效率的考量，我们会⾸先通
过召回系统将本次选品范围限定在⽐全量商品/⼴告库⼩很多的⼀个候选集上，
然后在这个候选集上应⽤排序算法或 OCPC。用户请求不同，候选集也在变，而
正是因为候选集在变，我们在建模时就必须考虑候选集信息。举⼀个简单的例
⼦，要想估计不同⼈的通勤时间，收⼊⽔平、居住地点等都只是⼀般化的特征，
如果我们能确切地知道每个⼈选择的交通⽅式，那这个信息显然可以帮助我们
做更为精准的通勤时间预估。
候选集信息主要指各种打分
在电商任务中，候选集信息主要是候选集的全部打分。为说明这⼀点，我
们不妨先把问题设定在最理想的环境下，有如下⼏点假设。
1. 强化学习中的折扣系数为 0，单个流量最优化就是全流量最优化；
2. 优化⽬标相关的全部因素都具备，⽐如优化⽬标是 RPM，我们有每个⼴
告的预估 CTR 和 BID；
3. 所有的预估值都是准确的，如 CTR 和 BID 完全准确；
4. 从因素到优化⽬标的建模是准确的，如输⼊三个⼴告的顺序和相应的预估
CTR、BID 值，建模能给你算出准确的 RPM 收益（甚⾄已经考虑了三个
⼴告的相互影响）。
在如此理想的环境下，我们不需要引⼊除候选集的全部打分之外的任何信
息，只要穷举⼴告三元组即可。
. 100 .
第九章 组合优化视⾓下基于强化学习的精准定向⼴告 OCPC 业务优化
把假设条件稍微放松，如预估值或者优化⽬标建模有瑕疵，我们可以利⽤
强化学习主动探索和对标真实奖赏的特性进⾏修正。
只有当⼀些假设严重失真的时候，我们才需要引⼊候选集的全部打分以外
的信息，⽐如
1. 折扣系数⼤于 0，这意味着单个流量最优化并⾮全流量最优化，⽽候选集
的全部打分只能帮你做到单个流量最优化，所以引⼊额外信息是必然的；
2. 优化⽬标相关因素不完备和部分预估值很不准确其实有⼀定的重叠，它们
都要求引⼊额外信息修正⽤户的点击、购买估计。
以上表述想说明的道理也可以⽤⼀个简单的例⼦类⽐。
开学初，⽼师说期末考题都在教材范围以内，吃透教材就能得到满分。
后来⽼师说，教材内容有错误，吃透教材 90 分还是有的，想得满分要同时
参考教材勘误表。
再后来⽼师又说，期末考题不限于教材范围，光看教材最多考 70 分，想得
满分要另外参考⼀本国外教材。
候选集的全部打分其实就是教材，教材（候选集的全部打分）是考试（决
策）考⾼分（获得最优奖赏）的基础，其他资料（如⽤户最近的⾏为偏好）是教
材的纠正或补充。
候选集打分是自然转移的状态
我们已经说明候选集信息必须引⼊，且候选集全部打分就是我们⽤于决策
进⽽优化奖赏的⼀类重要信息，那么它是⼀个转移的状态么？答案是肯定的。
候选集的商品构成是召回系统根据⽤户最近状态挑选出来的，候选集的打
分是排序模块根据⽤户最近的状态和⼴告⾃⾝信息计算出来的，所以候选集的
全部打分完全可以被视为对当前⽤户状态的⼀种凝练。⽤户相邻两次到访业务
场景，候选集的构成和打分都会发⽣变化，这就是⼀个⾃然的转移过程。
9.3 建模粒度
. 101 .
以上，我们主要从逻辑分析⾓度解释了我们对状态的选择，其实还可以从
建模⾓度理解。我们可以将在动作⽹络中使⽤预估值作为状态类⽐成使⽤没有
End2End 训练的 Embedding，如图 9.2 所⽰。
图 9.2: 预估值是动作⽹络的 Embedding 输⼊
9.3 建模粒度
在建模粒度这个问题上，我们关⼼的是，应该将候选集的全部打分整体输
⼊⼀个⽹络，还是将每个⼴告的打分信息分别输⼊⼀个共享参数的⽹络。我们
称前者为 session 粒度建模，称后者为 ad 粒度建模。
这个问题我们并不陌⽣，ad 粒度建模就是我们在 CTR 预估中的⼀般做法。
那么，为什么我们⼏乎没有见到过 session 粒度建模的预估⽅案呢？⼀个重要原
因是，⽤监督学习直接建模组合优化问题有很⼤的难度。
所谓组合优化问题，是指我们在电商环境下会⾯对的这样⼀类问题：如何
从一个大的候选集合中挑选一个子集，按照一定的顺序展现出来，实现流量指
标（如 RPM、GMV）最优。独⽴假设下的 ad 粒度预估排序是解决这类问题的
⼀种⾼效、近似的⼿段。
基于上述认识，我们接下来从强化学习的动作⽣成和值函数预估⾓度思考
这两种建模粒度。
ad 粒度建模中，动作是每个 ad 的调价⽐例，和 CTR 模型输出预估值类似。
. 102 .
第九章 组合优化视⾓下基于强化学习的精准定向⼴告 OCPC 业务优化
session 粒度建模中，动作的含义应该是某种区分方式，这种⽅式能够将 3
个最优⼴告和其他 397 个⼴告分开。这类动作包括但不限于以下⼏种可能：
1. 直接输出哪 3 个是最优⼴告；
2. 输出 400 维奖赏预估值向量，排序取最优 3 个；
3. 输出 400 维调价⽐例向量，⽤调价⽐例和统⼀的公式给 400 个⼴告打分，
排序取最优 3 个；
4. 输出⼀个权重，⽤这个权重和统⼀的公式给 400 个⼴告打分，排序取最优
3 个，如图 9.3。
图 9.3: session 粒度建模举例
这四种动作⽹络的建模难度是⼀样⼤的，⽽第⼀种⽹络显然是想⽤监督学
习直接建模组合优化问题。值函数预估上的分析与此类似，考虑到 session 粒度
值函数预估实际上要完成两个任务：400 选 3 和 3 ⼴告奖赏的预估。
除建模难度外，session 粒度建模还会导致动作⽣成（图 9.3 下半部橙⾊框）
和动作执⾏（图 9.3 上半部蓝⾊框）两部分在优化上的耦合。强化学习并不会训
练 f 函数，⽽ f 函数⾃⾝也有优化空间，如果我们调整了 f 或者 x，那么动作⽣
成部分的状态输⼊或⽹络设计也很可能要随之变化。
9.3 建模粒度
. 103 .
图 9.4: 组合优化问题的可持续迭代⽅法论
总的来说，我们面对的问题本质上是组合优化问题，组合优化问题自身的
难度不会随着建模形式的变化而自动消除，它必然栖身于我们系统的某个环节
中。session 粒度建模是将组合优化问题的难度放到了监督学习建模（值函数预
估与动作⽣成）⾥，⽽ ad 粒度建模则将其放到了单⼴告调价（独⽴假设，近似
组合优化）与强化学习探索（探索组合优化解空间）中。
其实在 ad 粒度和 session 粒度之间还有⼀个 ad 组合粒度，Group CTR 之类
的任务就是尝试在组合粒度上做预估，以弥补独⽴预估假设的不⾜。⽆论是 ad
粒度，还是 ad 组合粒度，本质上都是从监督学习中剥离组合优化的难度，只是
两者对组合优化问题的近似程度不同。图 9.4 展⽰了在组合优化问题上的⼀种
可持续迭代的⽅法论：一方面不断接近现有近似建模方案的效果天花板，另一
方面让建模更接近原问题的组合优化本质。
最后，考虑到我们的动作是每个 ad 的调价⽐例，特别是最终要为每个 ad 单
独计算 ECPM 进⾏排序，所以我们选择 ad 粒度建模。
. 104 .
第九章 组合优化视⾓下基于强化学习的精准定向⼴告 OCPC 业务优化
9.4 模型选择
模型选择⽅⾯，我们主要考虑三类⽅法：
1. 值函数估计⽅法，如 DQN[26]；
2. Likelihood Ratio 策略梯度⽅法，如 Actor-Critic[17]；
3. Pathwise Derivative 策略梯度⽅法，如 DDPG[22]。
DQN
DQN 是⼀种最直观的，将现有监督学习预估⽅法和强化学习结合，解决组
合优化问题的⽅式，图 9.1 的上半部分其实就是 DQN ⽅案。在这种⽅案中
1. 强化学习的探索为监督学习解决训练样本探索不充分的问题；
2. 监督学习提供的预估 CTR、CVR 等可以帮助强化学习做更⾼效率的探索；
3. 强化学习的值函数预估对标真实奖赏；
4. 监督学习提供的预估 CTR、CVR 等可以作为强化学习值函数的输⼊。
DQN ⽅案可以应⽤于⼀般的搜索、推荐场景，但它并没有显式建模动作。
我们在 9.2.2 ⼀节提到，从⼴告的商业逻辑和 OCPC ⾃⾝的产品定位看，显式定
义⼀个 Actor ⽹络是更合理的，所以我们重点考察 DDPG 和 Actor-Critic。
Actor-Critic 与 DDPG
Actor-Critic 和 DDPG 的最⼤差异已经体现在所属⽅法的命名中。Actor-Critic
属于 Likelihood Ratio ⽅法，这种⽅法让 Actor 学习好动作的经验、吸取差动作
的教训，这些动作都是真实发⽣过的，值函数预估主要起到调节动作样本权重
的作⽤。DDPG 属于 Pathwise Derivative ⽅法，这种⽅法 Actor 的监督信息来⾃
值函数的梯度，真实历史动作通过服务于值函数的学习间接影响 Actor ⽹络。
在 OCPC 任务中，我们较为倾向于使⽤ Actor-Critic ⽅法，原因有⼆：
9.5 探索学习
. 105 .
1. Actor-Critic ⽅法中的值函数预估主要起到调权的作⽤，所以我们可以做 ad
粒度建模，只对真实展现出来的⼴告组合做奖赏估计，然⽽ DDPG ⾥要⽤
到值函数对动作的梯度，也就是要求做 session 粒度建模，我们已知这是
在直接建模组合优化问题；
2. 奖赏预估，特别是其中的 GMV 预估尚不能保障预估值的精准性，⽤它做
⼴告组合的优劣判别进⽽调整权重是可以的，但⽤它的梯度去指导 Actor
的训练可能会带来误差的传播。
9.5 探索学习
我们已经对奖赏、动作、状态、建模粒度和模型选择进⾏了深⼊讨论，接下
来我们需要思考的问题只剩下三个：
1. 如何做有效的探索；
2. 如何评价每个探索；
3. 如何把好的探索记录下来。
问题 2 其实就是值函数（Q、V）预估，ad 粒度建模下相对简单，我们重点
关注问题 1、3。
探索可以天马⾏空，我们可以任意扰动每个⼴告的调价⽐例，以改变展现
出来的 3 个⼴告，关键是如何指导 Actor ⽹络把好的探索记下来。
探索⽅式主要有两种，动作空间扰动和参数空间扰动。前者指在 Actor ⽹络
输出的动作上加噪声，后者指在 Actor ⽹络的参数上加噪声，使动作输出发⽣变
化。在⽤ Actor-Critic 建模 OCPC 问题的前提下，动作空间扰动会有三⽅⾯不⾜：
1. 动作空间扰动要学习⼀个好的动作，不仅要包括每个展现⼴告的调价⽐
例，还要包括会对本次展现⼴告构成威胁的那些⼴告的当前调价⽐例，但
即便你把这些⼴告及相应的调价⽐例都送给 Actor ⽹络学习，也很可能⽆
法保证这次好的动作探索能被复现；
. 106 .
第九章 组合优化视⾓下基于强化学习的精准定向⼴告 OCPC 业务优化
2. 给 Actor ⽹络设定的动作标签并不稳定，因为未展现⼴告的调价⽐例只要
⼩于等于当前⽐例就⼀定不会影响本次展现的⼴告，那么动作标签应该如
何设定呢（⼀种可能的解法是将⼴告调价限制为⼆值，即要么调到最⾼，
要么调到最低，这么做和 DQN ⽅案区别不⼤）；
3. 动作空间扰动并不考虑扰动后的动作是否可由当前状态和 Actor ⽹络结构
学习到，更增⼤了⽆法复现本次动作的可能性。
参数空间扰动的好处与此相对：
1. 能够完整地复现本次动作，这个特性有助于记忆⼴告⼆价率；
2. 梯度来源不是动作标签的反向传播，⽽是参数的扰动量；
3. 能复现本次动作的参数是确定性存在的。
总的来说，⼆者的区别在于梯度来源不同，动作空间扰动靠标签的反向传
播获得梯度，⽽参数空间扰动的梯度就是扰动量本⾝。参数空间扰动也有其⾃
⾝局限性，可以说是以牺牲一定程度的探索与学习的灵活性为代价，换得动作
学习的稳定性。
1. 探索灵活性：OCPC 中每个⼴告可在⼀定区间内任意调价的灵活性增加了
动作学习的难度，参数空间扰动把这种难度消除了；
2. 学习灵活性：当⽆法给监督学习指定稳定可靠的标签供其做反向传播时，
我们只能使⽤遗传算法类的优化⽅法。
9.6 业务实战
9.6.1 系统设计
⽬前，我们强化学习系统的总体设计如图 9.5 所⽰，这是我们在探索实践中
不断迭代完善的结果。绿⾊虚线标出的是离线模拟流程，我们主要⽤来检验算
9.6 业务实战
. 107 .
图 9.5: 在离线强化学习系统设计
法收敛性，定性测量状态输⼊或⽹络设计的有效性。灰⾊实线标出的是在线训
练、服务流程，主要分以下⼏部分：
1. OCPC 主逻辑，负责动作⽣成与探索，并记录⽇志；
2. 状态服务，提供⽤户状态做流量端优化，提供⼴告主状态做⼴告主端优化；
3. 实时样本⽣成，Blink 分布式流处理做展现⽇志与奖赏信息的汇总，输出
到 Swift 供下游算法逻辑订阅；
4. 分布式强化学习算法，执⾏ Actor-Critic 学习逻辑；
5. 模型服务，实时模型导出，供 OCPC 访问。
2017 年双⼗⼀、双⼗⼆期间，我们系统中的 RL 核⼼算法尚不是分布式
Actor-Critic，⽽是⼀种进化策略⽅法 CEM（Cross Entropy Method）[30]。在我们
看来，CEM 是满⾜上⽂所有建模思考的⼀种极简实现⽅案，有以下⼀些优势：
1. CEM 通过参数空间扰动学习 Actor ⽹络，将蒙特卡洛采样真实奖赏替换成
值函数预估，就构成了基本的 Actor-Critic 结构；
. 108 .
第九章 组合优化视⾓下基于强化学习的精准定向⼴告 OCPC 业务优化
2. CEM 在单参数扰动上的奖赏聚合逻辑有利于降低奖赏的⽅差，挑选⾼奖
赏（Elite）部分更新参数的做法其实是通过相互⽐较去判断探索的好坏，
⽽ Actor-Critic 则是⽤基线（Baseline）或优势函数（Advantage Function）做
到这⼀点；
3. 离线模拟显⽰，CEM 在我们的数据上能稳定收敛，⽬标值近乎单调上升；
4. CEM ⼯程实现代价低，其轻量特性便于快速迁移推⼴到其他需要策略学
习的场景；
5. CEM 逻辑简单，可解释性强，有助于我们前期快速验证状态输⼊、⽹络结
构及奖赏设计的有效性，在 CEM 中无效的状态、网络或奖赏在更复杂模
型中也大可能是发挥不出作用的。
9.6.2 奖赏设计
我们要联合优化流量 RPM 与 GMV，但它们并不是完全独⽴的，甚⾄优化
其中⼀个⽬标会对另⼀个⽬标带来负⾯影响。绘制出帕累托曲⾯做多⽬标决策
⽬前还只能在离线模拟中完成，在线做法上，我们还是使⽤简单加权⽬标作为
奖赏。这当中要考虑到两⽬标量纲、量级和数据分布的不同，因应⽤场景⽽异，
就不详细展开了。下⾯我们分别介绍在 RPM、GMV 上的处理考量。
⼆阶扣费逻辑下，RPM 很难在单⼴告粒度上预估准确，考虑到⼆价率的学
习，以及场景下较⾼的点击率，我们决定使⽤真实反馈做 RPM 奖赏。然⽽，分
析 CEM 训练过程我们发现，⼀些参数扰动点的 RPM 奖赏之所以⾼，是因为它
们的⼴告候选集整体出价就很⾼，被状态中的 BID 特征捕捉到，所以在线 RPM
提升主要是 PPC（单次点击扣费）提升带来的。我们希望系统性地消除不同⼴
告候选集的出价偏差，于是提出以公式 9.2 作为奖赏，该式表达的是系统在单位
出价上的营收能力。
∑
∑
click cost
pv bid
(9.2)
9.7 总结与展望
. 109 .
在我们的业务场景下，GMV 奖赏很稀疏，⽽且因为笔单价不同，奖赏值的
波动还很⼤。不仅如此，GMV 奖赏的延迟时间还很久，也即⽤户从点击到购买
需要较长时间。针对稀疏、波动和延迟的性质，我们有必要使⽤值函数预估的
⽅法对 GMV 做估计。双⼗⼀期间，我们使⽤的是辅助 GMV 奖赏的⽅法，其实
就是⼀种简单的值函数预估⽅法。具体做法是，我们线上有⼀个 GMV 最优策略
桶，智能调价桶会先⽤这个 GMV 最优策略给⾃⼰的⼴告候选集排序，把⼴告的
序折算为 CEM 算法中的 GMV 奖赏。
9.6.3 实验效果
双⼗⼀预热期，我们进⾏了⼗天完整的实验，基准桶⽆强化学习⽅法。⼗
天累积效果，GMV 持平，RPM 提升 11%，其中 CTR 提升 4%，PPC 提升由两
部分带来，分别是（保⾃⾝ ROI 的情况下）提⾼出价和提升⼆价率。
双⼗⼀零点，我们以前⼀天参数为初值重新训练。全天效果，GMV 提升 6%，
⼏乎全部由 CTR 提升带来，RPM 提升 11%。在之后的 11 ⽉ 12 ⽇，这种提升效
果仍旧能够保持。
双⼗⼆前，我们将动作⽹络从线性模型升级为神经⽹络，并在状态中加⼊
了⼴告定向类型，考虑到⼀些定向的兴趣指向和实时性可以帮助我们更好的建
模⽤户购买意愿。双⼗⼆期间，我们进⾏了⼗⼆天完整的实验，基准桶是双⼗⼀
强化学习最优桶。⼗⼆天累积效果，RPM 提升 1%，GMV 提升 4%，其中 CTR、
CVR 的贡献占五成，笔单价贡献了另外⼀半。
9.7 总结与展望
现如今，强化学习在电商领域乃⾄整个⼯业界的应⽤⽅兴未艾，我们在
OCPC 业务上的实践只是⼤浪潮中的⼀朵浪花，能够将⾃⼰的探索历程和思考
撰写成⽂与诸君共享是我们的荣幸。
随着思考与实践的深⼊，我们的⼀种感觉越发强烈，那就是多想想强化学
习能给我们现有解决⽅案带来什么，⽐直接思考如何在业务上做强化学习建模
. 110 .
第九章 组合优化视⾓下基于强化学习的精准定向⼴告 OCPC 业务优化
更有效。强化学习是一个通用的问题解决框架，核心思想是 Trial & Error，它
不是能替代我们思考业务本质的黑魔法，而是在我们认清问题本质的前提下帮
我们优化解法的工具。
具体到 OCPC 业务上，我们认为动作受限下的组合优化是问题的本质，这
个组合优化难度不因强化学习的引⼊⽽消除，⽽探索是强化学习提供给我们的
解决组合优化问题的可持续迭代⽅案。此外，值函数预估⽀持我们直接对标长
期终局指标，策略梯度定理允许我们将系统中难于建模的部分⿊盒化。总⽽⾔
之，强化学习要素的引入让我们现有的独立预估方案在解法上更适配原始问题
的组合优化属性。
在具体的算法迭代中，我们⼀直是⼩步快跑，希望通过严谨细致的实验对
⽐，理清每⼀⼩步的收益。我们坚信，只有充分理解现有做法的成败原因，才能
明确未来迭代的提升⽅向。
最后，结合经验与思考，我们认为 OCPC 强化学习下⼀阶段的重点研究⽅
向主要有以下⼏点：
1. 状态设计：进⼀步引⼊有效状态作为候选集全部打分的修正或补充，特别
是引⼊⼴告主状态做全流量 ROI 优化；
2. 探索效率：结合启发式⽅法不断提升组合优化问题空间的探索效率；
3. 值函数预估：进⼀步提升奖赏预估精度，并提⾼值函数预估样本利⽤率。
第十章 策略优化方法在搜索广告排
序和竞价机制中的应用
10.1 业务背景
搜索⼴告业务是阿⾥巴巴电商体系下的最为重要的⼀个业务，在创造整个
集团⼤部分营收的同时，也承担着重要的⽣态调节功能，是帮助商家成长的“快
车道”和“名校”。随着⼤数据和算法越来越深刻的影响业务的发展，技术已经成
为搜索营销业务的核⼼驱动⼒。
搜索⼴告的竞价和排序遵循下⾯的业务流程：⼴告上在竞价词上定义⾃⼰
的出价，对于每个⼴告位，搜索⼴告引擎根据⼴告质量（包括⼴告的点击率、转
化率等）和⼴告主的出价对候选⼴告集合进⾏排序，排名第⼀位的⼴告获得当前
⼴告位的展⽰机会。阿⾥巴巴搜索⼴告业务采⽤⽤户点击扣费的收费模式，即
当有⽤户点击⼴告时，系统才对⼴告对应的⼴告主进⾏扣费。从整个业务流程
来看，每⼀次搜索⼴告的展⽰都牵扯到了⼴告商、⽤户和平台三房的利益。对
于⼴告商来说，借助搜索⼴告的流量获取作⽤，通过竞价来提升商品的曝光率，
从⽽提⾼商品的销量；对于⽤户来说，⽤户在平台中搜索⾃⼰感兴趣的商品，希
望平台能够提供更具个性化的推荐结果，从⽽提⾼浏览的效率和体验；⽽对搜
索⼴告平台来说，希望在保证⼴告商诉求和⽤户体验的前提下，不断提⾼⾃⼰
的收益。⽽同时最⼤化这三⽅利益的关键就是选择合适的⼴告和合适的扣费标
准。在我们的场景下，我们通过优化排序公式的设计来达到这样的⽬的。⼀⽅
. 112 .
第⼗章 策略优化⽅法在搜索⼴告排序和竞价机制中的应⽤
⾯，排序公式决定了哪个⼴告会最终被展⽰出来；另⼀⽅⾯，我们的搜索⼴告
平台采⽤⼆价扣费机制（Generalized Second Price，GSP）进⾏点击扣费计算，这
种计费⽅式按照保证展⽰⼴告能够维持⾃⼰排序位置的最低出价来对⼴告商扣
费。也就是说，排序公式也决定了⼴告最后的收费标准。
由于⽤户在淘宝搜索之后的浏览过程可以看作是⼀种于平台连续的交互过
程，我们提出⼀种策略优化算法（policy optimization）来优化不同搜索场景下的
排序公式，该算法可以针对某⼀⽬标或者多⽬标的组合进⾏实时地调优。具体
来说，我们将策略优化问题描述成⼀个强化学习问题，利⽤强化学习序列优化
的能⼒进⾏策略的优化。在模型的学习⽅⾯，提出了基于仿真系统的 Agent 初
始化⽅法和基于 Evolution Strategy 在线学习⽅法。
10.2 广告排序和竞价的数学模型和优化方法
如前⽂所述，排序公式将直接影响⼴告主、⽤户和平台的收益，对于⼴告
主来说，能够获取展⽰机会是商家进⾏商品的推⼴重要前提；对⽤户⽽⾔，展
⽰⾼质量的⼴告将有助于提升⽤户的满意度；⽽对平台来说，⽤户是否点击和
点击之后的扣费将决定平台最终的收益，⽽⽤户和⼴告商的满意度也恰恰是维
护平台长期收益的前提。因此，⼀个好的排序公式⽆疑对三⽅都有着重要的作
⽤。那么我们应该怎么进⾏排序公式的优化呢？⼀⽅⾯，搜索⼴告的优化具有
场景相关性的特点，搜索⼴告和原⽣搜索结果同时展⽰在搜索结果流⾥，两者
互为上下⽂，⼴告是否能吸引⽤户的眼球，并且在风格和内容上保证⽤户体验
是优化的⽬标之⼀。从排序公式的场景相关性来看，可以把排序公式的学习定
义如下：
a = A(s)
(10.1)
其中，a 表⽰对排序公式的参数化描述，s 为当前请求的上下⽂环境，即
s =< User; Query; Ads >。Ads 表⽰当前的候选⼴告集合。另⼀⽅⾯，搜索⼴告
的优化具有全局选优⽽⾮单点最优的特点，从上⾯提到的⽤户和平台的序列交
10.2 ⼴告排序和竞价的数学模型和优化⽅法
. 113 .
互过程来看，我们希望优化的是⼴告展⽰序列的收益最⼤花⽽⾮单点的收益最
优。也就是说，对于公式10.1，在每个单点场景 s 会获得⼀个“奖励“rs，我们希
望的是在序列交互过程中总的奖励之和（
s rs）最⼤。
∑
将排序公式的学习抽象成⼀个模型后，想要进⾏优化我们还需要回答两个
问题：1. 优化⽬标是什么？2. 哪些因素能够影响优化⽬标，也就是在⽬标优化
的过程中有哪些抓⼿可以解决问题。
⾸先是优化⽬标问题，搜索⼴告核⼼任务是给公司带来盈利，因此⼀个直
观的⽬标就是提⾼ RP M（revenue per thousand impressions，即平均千次⼴告展
⽰获得的总⼴告商扣费）。⽽且⼿淘体系是⼀个完整购物链路，保持长期的效益
离不开参与者（⽤户和⼴告主）的参与，因此搜索⼴告在不断提⾼收⼊的同时要
兼顾⽤户和⼴告主的诉求，从指标⾓度来说，表⽰成不断提⾼ RP M 兼顾 CT R
（click-through-rate，⼴告点击率），CV R（conversion rate，转化率或者⽤户购买
率）和 GM V （gross merchandise volume，电商货品总销量）等指标。
有了⽬标之后还需要找到优化⽬标的抓⼿从⽽实现⽬标优化⼯作。⾸先是
⽤户是否点击或者购买，只有点击平台才能获得收⼊，只有购买⼴告主的推⼴
诉求才能被最直接地表达出来。如上⽂所述，搜索⼴告结果在展⽰页⾯是和⾃
然搜索（主搜）的结果混合排列的（搜索⼴告在每⼀页有固定的展⽰位置），这
就使得⽤户对⼴告的响应（点击或者购买）除了受到⼴告本⾝质量的影响以外，
还受到⾃然搜索⼴告结果的影响，因此搜索⼴告的效果需要考虑⾃然搜索的结
果。当然从业务流程来看，策略端在进⾏⼴告打分排序时是拿不到⾃然搜索结
果的，但是⾃然搜索结果仍然是由⽤户的 Query 和本⾝的特点召回的，具有相
似的上下⽂环境。此外，⽤户对搜索⼴告和⾃然搜索结果的不同响应也是我们
⽐较 ad 和⾃然搜索结果的重要⼿段。也就是说模型 a = A(s) 需要感知上线⽂环
境并给根据上下⽂特点给出相应的排序参数 a。当⽤户愿意点击我们的⼴告时，
我们还能做什么呢？⼀个最直接的问题就是扣费，扣费的多少直接影响平台的
收⼊情况，在 GSP 的扣费计算公式下，相邻两个⼴告的排序分的紧凑程度会对
扣费产⽣影响，在个性化⼴告召回的算法下，不同的⽤户，不同的上下⽂环境
召回的⼴告集合都是不同，如果能够感知候选⼴告集合的分布，我们就有可能
预估不同排序参数下的扣费情况，从⽽对扣费进⾏调节。也就是说感知候选⼴
. 114 .
第⼗章 策略优化⽅法在搜索⼴告排序和竞价机制中的应⽤
告分布，预估扣费也将是我们实现⽬标优化的⼀个⼿段。此外我们还需要考虑
的是，⽤户的浏览是⼀个过程，⽤户对前⼀次 pv 展⽰的响应必然会对后⾯浏览
产⽣影响，因此⼀个完整的优化需要考虑整个浏览过程，对策略端来说是不是
考虑到⽤户的历史⾏为就可以实现⼀个完整浏览过程优化呢？不是的，因为策
略端在每⼀次进⾏⼴告打分排序时都是⼀个博弈的过程，这⾥可以做⼀些简化
和类⽐。我们假定候选⼴告集合不变，⽤户连续的浏览⼀个个的⼴告，那么从
策略端来看这件事情就变成了策略端每次从⼴告集合中⽆重复地拿出⼴告展⽰
给⽤户，并按照这个⼴告和排在第⼆位的⼴告的紧密程度进⾏扣费。这个过程
就像⽥忌赛马，并不是说每次拿出质量分最⾼的，就能获得最好的结果。从这
个⾓度看，排序公式优化就像是⼀个博弈过程，需要全局的统筹规划。
上⼀段我们分析了排序优化的三个可能的抓⼿，那么应该建⽴⼀个什么样
的模型才能让抓⼿动起来呢？模型上我们选择了强化学习，主要从以下⼏个⽅
⾯考虑：1. 强化学习是对 M DP 过程进⾏建模的，这⼀点和我们⾯向浏览过程
的优化⽬标⼀致；2. 强化学习尤其是深度强化学习的研究和发展，为我们在复
杂场景下的策略优化提供了理论上的保证；3. 强化学习是⾯向综合收益最⼤化
的优化 (V (st) = rt + (cid:17) (cid:1) V (st+1)) 这和我们希望的长期⽬标最优化是⼀致的；4.
强化学习的奖励函数的设计是灵活的，可以是连续的、离散的或者不可导的。这
⼀点和我们的优化⽬标可以进⾏很好的融合，⽐如我们⾯向 RP M 和 CT R 进
⾏优化，则可以将奖励函数设计为 r = rpm + (cid:21) (cid:1) ctr。
10.3 面向广告商、用户和平台收益的排序公式设计
未来使排序公式对于⼴告商、⽤户和平台收益具有调控能⼒，我们设计如
下所⽰的排序公式
}
ϕ(s; a; ad) = fa1(CT R) (cid:1) bid
{z
|
}
+a2 (cid:1) fa3(CT R; CV R)
{z
|
}
+a4 (cid:1) fa5(CV R; price)
{z
|
(10.2)
platform
user
advertiser
其中 a = ai (i = 1; :::; 5) 表⽰排序公式的参数，bid 表⽰⽤户对⼴告 ad 的出
10.4 系统简介
. 115 .
价，price 表⽰⼴告对应商品的价格，CT R,CV R 为系统预测点击概率和转化概
率。排序公式中的 fa1 可以认为是平台的收⼊的期望值；fa2 考虑了⽤户的点击
概率和转化概率，主要⽤于描述⽤户的满意程度；fa3 考虑了与购买相关的因素，
表⽰了⼴告主可能的收益。此外 a2, a4 ⽤于调节后两个因素的平衡关系。我们⽤
′ 表⽰排在相邻的两个位置之间的⼴告，则根据 GSP 的扣费计算⽅式，可
ad, ad
以计算当前点击扣费为
) (cid:0)(
)
click_price =
ϕ(s; a; ad
′
a2 (cid:1) fa3(CT R; CV R) + a4 (cid:1) fa5(CV R; price)
fa1(CT R)
(10.3)
10.4 系统简介
现有⽂献中的强化学习⽅法⼤多应⽤于虚拟场景，特别是游戏场景。对于像
⼴告这种场景的应⽤，需要考虑的⼀个重要的问题就是初始化和探索过程（ex-
plore）对于平台效果的影响。因此，我们设计了如下图所⽰的系统架构。系统主
要由三个模块构成：离线搜索⼴告仿真模块，离线强化学习模块和在线策略优
化模块。离线仿真模块主要⽤于仿真不同策略函数的参数的影响，如计算不同
策略下候选⼴告的排序结果，可能的⽤户⾏为和扣费情况。仿真模块的使⽤可
以使系统在离线的情况下充分探索可能的策略，同时不损害线上⽤户的真实体
验。离线强化学习模块主要根据仿真模块产⽣的结果学习最优的离线策略，完
成策略模型的初始化⼯作。当然离线仿真不可能代表线上的真实环境，需要我
们根据线上的真实反馈来调节策略模型，这部分⼯作主要由在线策略优化模块
来完成。
10.4.1 离线仿真模块
利⽤离线仿真模块⼀⽅⾯可以为强化学习进⾏环境探索产⽣⼤量的训练样
本，从⽽保证强化学习算法的有效性，另⼀⽅⾯可以避免环境探索对线上真实
. 116 .
第⼗章 策略优化⽅法在搜索⼴告排序和竞价机制中的应⽤
图 10.1: 策略优化系统框架
环境的影响，避免“不好”的策略对⽤户体验、平台收益的损失。排序结果会受到
很多因素的影响，如⼴告主的预算，竞价价格的变动以及⽤户的偏好等因素，理
想化的仿真系统相当于复制全套线上系统，⽽线上系统会受到⼴告商预算，⽤
户分布等多维因素的影响，模拟难度很⼤。为了简化仿真系统，我们的仿真系
统通过记录线上每次⼴告展⽰对应的上下⽂环境（user,query）和候选⼴告集合，
在此基础上对不同策略函数参数进⾏仿真，得到新的⽤于展⽰的⼴告并计算相
关的点击扣费 click_price，并⽤ CT R，CV R 来预估⽤户的⾏为。在奖励函数的
设计上，如果⾯向⽤户点击和平台收益进⾏优化，则 reward 函数可以设计为：
r(st; at) = CT R (cid:1) click_price + (cid:14) (cid:1) CT R
(10.4)
其中，(cid:14) 是调节因⼦，⽤于调节点击率和扣费之间的平衡。
这⾥需要说明的是，算法预估的 CT R(CV R) 和线上⼴告真实的点击率并
不相等，因此为了使⽤仿真系统的⽤户响应估计更加贴近线上真实情况，系统会
对 CT R(CV R) 的结果进⾏标定 (Calibration), 并⽤标定的结果计算 reward。对
于标定⽅法，使⽤ Isotonic regression method [1]。
10.4 系统简介
. 117 .
10.4.2 离线强化学习进行排序策略模型初始化
在上⾯介绍的离线仿真模块基础上，我们定义强化学习相关的因素，包括
状态 s, 动作 a 和奖励 reward 以及状态转移 st ! st+1。对于状态 s，我们⽤⽤户
请求时上下⽂作为状态的描述，可以包括 User,Query 和⼴告列表等相关的搜索
上下⽂信息，包括⽤户的 profile 信息，⽤户的历史⾏为等。动作 a 即为排序函
数中的参数 a = faig5
i=1。奖励函数由公式10.4定义的 reward 的⽅式计算。我们
假设⽤户在同⼀ query 下的浏览序列作为⼀次完成浏览过程，那么⼀个 Episode
就是⽤户从 page1 开始的浏览序列，状态之间的转移就是⽤户在不同 page 之前
的迁移或者离开。
在强化学习模型的选择上，我们主要考虑两个因素，第⼀个是在个性化的⼴
告系统中，前⼀页的展⽰结果和⽤户⾏为会对后续页的⼴告的打分（CT R,CV R）
产⽣影响，这个是⽬前离线仿真系统⽆法仿真的（只能对每个展⽰机会相互独
⽴的进⾏仿真），因此需要使⽤⼀种 off-policy 的强化学习模型；另⼀个是动作
空间 a 2 A 是连续的，因此需要使⽤⼀种连续策略优化⽅法。考虑上述两个因
素，我们使⽤ Deep Deterministic Policy Gradient (DDPG)[23] 模型进⾏离线强化
学习。
DDPG 网络结构
本⽂使⽤的 DDPG 模型是基于 Actor-Critic 架构的，具体的⽹络结构如上图
所⽰。因为输⼊的状态 s 特征都是 id 化的，因此所有的 id 特征会经过⼀个 em-
bedding 层进⾏特征编码。激活函数选择 ELU 函数 [7]，实验中发现使⽤ Sigmoid
和 ReLU 作为激活函数时，当输出的动作 a 偏移中⼼位置较远时，产⽣的梯度
值很⼩，收敛速度很慢或者不收敛。此外对于 critic 函数的输出，采⽤了 dueling
architecture 的⽹络结构 [37]，即将输出的 Q(s; a) 表⽰成 V (s) + A(s; a)，dueling
结构的使⽤可以使 critic 在学习过程中侧重能够获得更多奖励的动作。在实验
中，由于数据的⽅差⽐较⼤，我们观察到 a 的更新可能会⽐较剧烈，为了保证
输出的动作 a 在可控的范围，我们利⽤ clip method ⽅法对输出进⾏截断。
. 118 .
第⼗章 策略优化⽅法在搜索⼴告排序和竞价机制中的应⽤
图 10.2: DDPG ⽹络结构
异步 DDPG 学习
在 DDPG 学习过程中，我们采⽤异步学习的⽅式进⾏，学习流程如上图所
⽰，在仿真环境中，我们⽤不同策略的 agent 进⾏动作空间的探索，从⽽⽣成训
练样本 < st; at; rt; st+1 >，不同的 worker 会计算⽹络梯度并将梯度计算结果发
给参数服务器，参数服务器每 N 步进⾏⽹络参数的更新。异步更新算法如下图
所⽰。
10.5 在线排序策略模型优化
尽管在策略优化学习的过程中，我们使⽤了离线仿真模型进⾏策略空间的
探索并对预估结果进⾏了奖励标定 (calibration), 但是仿真的结果并不能代表⽤
户的真实⾏为，因为⽤户的⾏为会受到其他环境因素的影响，⽽且正如在仿真
10.5 在线排序策略模型优化
. 119 .
图 10.3: 离线 DDPG 学习流程框架
系统中提到的，仿真系统仍有很多因素没有考虑，如⼴告主的预算、出价等信
息，仿真系统⽆法得到序列化得仿真结果。因此策略优化算法需要根据线上的
真实反馈进⾏在线学习。
对于在线学习⽅法，我们利⽤ Evolution Strategy ⽅法 [31] 进⾏在线策略
更新。对于给定的排序策略模型 (cid:25)(cid:18)(st)，Evolution Strategy 通过执⾏以下两步
进⾏策略的探索和模型的更新：（1）在模型参数空间 (cid:18) 加⼊⾼斯噪声产⽣探
索动作 a;(2) 统计不同噪声下策略得到的 reward 结果，并根据结果来更新⽹
络参数。假设我们对参数空间进⾏ n 次扰动，产⽣扰动后的参数空间 (cid:2)(cid:25) =
(cid:18)(cid:25) + ϵ1; (cid:18)(cid:25) + ϵ2; :::; (cid:18)(cid:25) + ϵn，对应的线上的实际奖励为 Ri，则参数的更新的⽅法
为：
n∑
′
(cid:25) = (cid:18)(cid:25) + (cid:17)
(cid:18)
1
n(cid:27)
Riϵi
i=1
其中 (cid:17) 表⽰学习率。使⽤ Evolution Strategy 进⾏模型参数更新，具有三点优
(10.5)
. 120 .
第⼗章 策略优化⽅法在搜索⼴告排序和竞价机制中的应⽤
Algorithm 3: Asynchronous DDPG Learning
Input: Simulated transition tuple set T in the form   =< st; at; rt; st+1 >
Output: Strategy Network (cid:25)(cid:18)(cid:25) (st)
1 Initialize critic network Q(cid:18)Q(st; at) with parameter (cid:18)Q and actor network
(cid:25)(cid:18)(cid:25) (st) with parameter (cid:18)(cid:25);
′, (cid:25)
2 Initialize target network Q
3 repeat
4
′ with weights (cid:18)Q′   (cid:18)Q, (cid:18)(cid:25)′   (cid:18)(cid:25);
1
2
∑
Update network parameters (cid:18)Q, (cid:18)Q′, (cid:18)(cid:25) and (cid:18)(cid:25)′ from parameter server;
Sampling subset (cid:9) = f 1;  2; :::;  mg from T ;
∑
= rt + (cid:13) (cid:1) Q
′
′
(cid:3)
(st));
For each  i, calculate Q
(st+1; (cid:25)
(cid:1) (Q
(cid:3) (cid:0) Q(st; at))2;
Calculate critic loss L =
 i2(cid:9)
Compute gradients of Q with respect to (cid:18)Q by ▽(cid:18)QQ = @L
∑
Compute gradients of (cid:25) with respect to (cid:18)(cid:25) by
▽(cid:18)(cid:25) (cid:25) =
 i2(cid:9)
Send gradients ▽(cid:18)QQ and ▽(cid:18)(cid:25) (cid:25) to the parameter server;
Update (cid:18)Q and (cid:18)(cid:25) with ▽(cid:18)QQ and ▽(cid:18)(cid:25) (cid:25) for each global N steps by
gradients method;
Update (cid:18)Q′ and (cid:18)(cid:25)′ by (cid:18)Q′   (cid:18)Q′ + (1 (cid:0) (cid:28) )(cid:18)Q, (cid:18)(cid:25)′   (cid:18)(cid:25)′ + (1 (cid:0) (cid:28) )(cid:18)(cid:25);
(cid:1) @(cid:25)(st)
@Q(st;(cid:25)(st))
(cid:1) @(cid:25)(st)
@(cid:18)(cid:25)
;
@A(st;(cid:25)(st))
;
@(cid:18)Q
 i2(cid:9)
@(cid:25)(st)
@(cid:18)(cid:25)
@(cid:25)(st)
=
5
6
7
8
9
10
11
12
13 until Convergence;
势。⾸先 Evolution Strategy 是⼀种 derivative-free 的更新⽅式，使⽤这种更新⽅
式可以避免计算梯度带来的计算量；其次，在分布式 parameter-serving 框架下，
每⼀个 worker 只需要把 reward 数值传给 parameter-server 即可，可以⼤幅度降
低在线学习对⽹络带宽的需求；最后，这种⽅法可以以⼀个 episode 整体计算奖
励，⽽不必考虑状态转移过程中奖励稀疏性对算法的影响，从⽽实现基于浏览
序列的整体优化效果。
10.6 实验分析
. 121 .
10.6 实验分析
我们通过实验验证以下问题，⾸先，模型是否能够收敛到最优解？其次，不
同的⽹络架构和参数设计会对于模型收敛性有什么影响？最后，在线更新对于
提⾼模型的线上效果的增益⼤概是多少？
针对第⼀、⼆个问题，我们采⽤简单的搜索上下⽂特征表⽰ s，只使⽤查询
词 ID 来表⽰ s，在这种简单的表⽰情况下，通过在离线仿真平台上对排序函数
参数集合 a 进⾏滑动窗⼜搜索，可以找到排序函数参数集合的最优值，对⽐从
DDPG 搜索到的最优值和滑动窗⼜得到的最优值，即可判断⽅法的收敛性。在
图10.4和图10.5中，我们⽐较了不同模型配置情况下的训练收敛性。其中参数配
置如表10.1所⽰，从结果中，我们可以发现以下结论：(1) dueling 通过将奖励函
数 (value function) 和优势函数 (advantage function) 区分对待，明显地提升了模型
的收敛性质；（2）由于数据的⽅差⽐较⼤，选取⼤的训练数据集合尺⼨ (batch
size) 对于收敛有正向作⽤；（3）使⽤衰减的学习率对于收敛有正向作⽤。
图 10.4: 使⽤ dueling 结构对于收敛性的影响
针对问题三，我们将 DDPG 学习到策略模型放到线上进⾏ 2% 流量测试，并
⽤ ES 进⾏策略更新。实验进⾏了 4 天，主要⽐较了 CT R,P P C 和 RP M 指标
. 122 .
第⼗章 策略优化⽅法在搜索⼴告排序和竞价机制中的应⽤
图 10.5: 使⽤不同的衰减因⼦，不同的批训练数据集合⼤⼩ (batch size) 对于收
敛性的影响
的变化情况，实验结果如下图所⽰。从结果中，不难发现在线更新对于算法效
果的正向作⽤。
10.7 总结
. 123 .
ID
decay
low
high
batch size
regular
表 10.1: 图10.5中的模型参数设置。
Learning rate
Regularization Batch size
exponential decay
1.0e-5
1.0e-4
exponential decay
exponential decay
1.0e-5
1.0e-5
1.0e-5
1.0e-5
1.0e-3
50k
50k
50k
10k
50k
图 10.6: 在线 ES 效果变化趋势
10.7 总结
在本⽂中，在吸取了强化学习和遗传策略 (evolution strategy) 的优势的基础
上，我们提出了⼀套在线上系统上现实可⾏的策略学习⽅案。通过建⽴离线仿
真模型，我们完成了在不损失线上系统性能的前提下对于策略函数（动作函数）
的最⼤化探索；由于线上线下数据的不⼀致性，直接使⽤线上数据来更新离线
仿真数据中学习的策略函数会⾯临数据分布不⼀致等问题，通过使⽤遗传策略
的⽅法，我们实现了⽅法的在线更新，并且在淘宝搜索⼴告系统上实现了效果
的稳定增益。
第十一章 TaskBot －阿里小蜜的任
务型问答技术
11.1 背景和问题建模
在阿⾥⼩蜜⾥，除了 QA 问答和开发域聊天之外，还有⼀种任务型对话。这
⾥的任务型问答是指由任务驱动的多轮对话，需要在对话中协助⽤户完成某个
任务，⽐⽅说订机票，订酒店等。传统的任务型问答通常是由 slot filling 来做
的 [20]，需要较多的⼈⼯模板和规则加上⼤量的训练语料组成。我们在阿⾥⼩
蜜⾥⾯尝试了⼀个端到端可训练的 TaskBot ⽅案，基于强化学习和 Neural Belief
Tracker，旨在可以快速搭建⼀个任务型的对话服务。
下⾯我们在订机票这个业务上，展⽰⼀个完整的任务型的多轮对话过程，
如下图 11.1所⽰。这⾥系统会反问⼀些⽤户信息，⽐⽅说：请问您从哪⾥出发
(where_f rom)，请问您要到哪⾥去 (where_to)，还有最终下订单（order）这个
动作。除此之外，还需要记录下⽬前获取到的 slot 状态 (Vslots)，⽅便之后出订
单。因此，这⾥涉及两个任务：
• Action policy: 系统如何给出合适的回复（反问或者出订单）；
• Belief tracker: 如何抽取 slot 状态。因为需要产出订单，所以在每轮对话中
都需要抽取出当前⽤户给出的 slot 信息。
对于第⼀个，这是⼀个多轮对话，⽽且我们可以收集到⽤户的反馈（继续
聊天，退出，下单等操作），所以我们尝试了深度强化学习来做这个事情。对于
11.2 模型设计
. 125 .
图 11.1: 订机票场景中需要的系统反馈和 slot 状态⽰例。
第⼆个，深度学习技术在 sequential labeling ⾥有⾮常不错的效果，⾃然的我们
尝试了深度学习的⽅法。
11.2 模型设计
整体我们的系统结构分为了数据预处理层，以强化学习为中⼼的端到端的
对话管理层，和任务⽣成层。其中数据预处理层包括常见的分词，实体抽取等
模块，基于这些模块的输出接⼊以强化学习为中⼼的对话管理层。
其中，强化学习模块主要包括⼀下三个部分，Intent network ⽤来处理⽤户
的输⼊,Neural belief tracker 记录 slot 信息, 和 Policy network 决定系统的 actions
（反问哪个 slot，或者出 order）。
11.2.1 Intent Network
这⾥我们尝试了 RNN 和 CNN，在实验⾥我们发现 CNN 和 RNN 效果差不
多，但是速度会快⼀倍，所以我们最后采⽤的 CNN 的⽅案。⽹络结构参考了这
篇⽂章 [16]，具体结构如图11.2, 这⾥画的是单层的 CNN，我们也尝试了多层的
. 126 .
第⼗⼀章 TASKBOT －阿⾥⼩蜜的任务型问答技术
CNN，在订机票的任务⾥⾯差别不⼤。简单来说，我们⽤ CNN 学⼀个 sentence
embedding 来表征⽤户的意图，这个信息作为后⾯的 policy ⽹络的输⼊。
图 11.2: Intent Network.
11.2.2 Belief Tracker
Belief tracker，有时候也被称为 slot extraction，是⽤来 track 到当前回话为⽌
的 slot 状态。我们尝试了两种技术⽅案。⼀个是⽤ pointer network (PtrNet) [36]，
输出有 N*2 个（N 是 slot 数量），每个 slot 有个开始 Pointer 和结束 Pointer，都
指向输⼊数据的某个位置。这样，PtrNet 可以直接产出 slot 信息。另外⼀个是
Sequence Labeling，模型⽤的是 BiLSTM-CRF [18]，需要标记出每个词是否属于
某个 label。这⾥每个 slot Si 有两种 labels:S-Si 和 O-Si，分别代表 Si 的开始和中
间位置。除了 slot 的 label 之外，还有⼀个 label O 代表空。实验对⽐下来，⽅案
⼀模型⽐较简单，直接产出多个 slots 的结果，但是效果不如⽅案⼆。
11.2 模型设计
. 127 .
11.2.3 Policy Network
强化学习部分我们选⽤的⽅案是 Policy Network，模型的 episode，reward，
state，和 action 的定义如下。
Episode. 在订机票场景⾥，在⼀个⽤户和系统的会话⾥，如果系统第⼀次判断
当前⽤户的意图为“购买机票”，这个就作为⼀个 episode 的开始，如果⽤户购买
了机票或者退出会话，则认为 episode 结束。
Reward. 在这个场景⾥，获取⽤户反馈⾮常关键。我们有两种⽅式：第⼀，收
集线上⽤户的反馈，⽐⽅说⽤户的下单，退出等⾏为；第⼆，初始化的时候，如
果是没有训练过的模型直接上线学习的话，⽤户体验⽐较差。为了避免这个问
题，我们使⽤预训练环境，让⼩⼆⽤预训练环境训练出⼀个效果相对可以的模
型再上线。预训练环境主要需要获取两部分的反馈，⼀个是 action policy，另外
⼀个是 belief traker。下图11.3是预训练环境的⽰例。其中 Action policy 部分⽤的
是 Policy Gradient ⽅法更新模型，正反馈的 reward 为 1.0，负反馈为-1.0, Belief
tracker 部分仅使⽤正反馈作为正例，出现错误需要⼩⼆标出正确的 slots。
图 11.3: 阿⾥⼩蜜任务型对话的预训练环境。
State. 我们主要考虑了 intent network 出来的 user question embeddings，当前抽
取的 slot 状态，和历史的 slot 信息，之后接⼊⼀个全连接的神经⽹络，最后连
. 128 .
第⼗⼀章 TASKBOT －阿⾥⼩蜜的任务型问答技术
softmax 到各个 actions。
Action. 在订机票场景，action 空间是离散的，主要包括对各个 slot 的反问和
Order（下单）: 反问时间，反问出发地，反问⽬的地，和 Order。这⾥的 action
空间可以扩展，加⼊⼀些新的信息⽐⽅询问说多少个⼈同⾏，⽤户偏好等。
11.2.4 模型
我们最终的模型如图11.4，每个⽤户的问题都输⼊到 IntentNetwork ⾥得到
问题的表⽰，然后和 BelifTracker 获取 slot 信息, 还有历史的 slot 信息合起来输
⼊到 PolicyNetwork ⾥。假设当前⽤户问题为 qi，上轮系统问题为 ai(cid:0)1，历史 slot
图 11.4: TaskBot ⾥的强化学习⽅案。
11.3 业务实战
. 129 .
信息为 Si，完整的⽹络定义如下。
Oi = IntentNet(qi);
Ci = BeliefTracker(qi; ai(cid:0)1);
Xi = Oi (cid:8) Ci (cid:8) Si(cid:0)1;
Hi = FullyConnectedLayer(Xi);
P ((cid:1)) = Softmax(Hi):
(11.1)
我们⽤的是 Policy Gradient 算法，具体来说是 REINFORCE 算法，为了使得
训练更加稳定，这⾥可以加个 baseline 和 advantage function。实验中我们也尝试
了 Deep Q Network 和 Actor-Critic，但是发现效果相对 REINFORCE 提升不⼤。
11.3 业务实战
⾸先我们对⽐了两种 belief tacker 的⽅案，效果如图12.1。在⼏个指标⾥，
BiLSTM+CRF 的效果都会⽐ PtrNet 好，主要原因是前者对于序列的建模效果更
好，BiLSTM 可以挖掘出当前词的 context 的信息，最后 CRF 层能有效的对标记
序列进⾏建模。PtrNet 是端到端的 sequence to sequence 模型，但是相⽐之下对
于序列的建模效果会差⼀点，前⾯学到的信息随着序列的增长会减弱。
Acc
PtrNet
0.968
BiLSTM+CRF 0.995
F1
0.850
0.961
Precision Recall
0.856
0.844
0.965
0.965
表 11.1: Belief tracker 效果对⽐.
下⾯我们对⽐不同的 DRL 配置下的效果如图11.3。在测试的时候发现，如
果⽤户退出会话 (Quit) 给⼀个⽐较⼤的惩罚-1，模型很难学好。这个主要原因
是，⽤户退出的原因⽐较多样化，有些不是因为系统回复的不好⽽退出的，如
果这个时候给⽐较⼤的惩罚，会对正确的 actions 有影响。
. 130 .
第⼗⼀章 TASKBOT －阿⾥⼩蜜的任务型问答技术
Reward
Discount
Is Trainable Click Quit Continue Factor
0.1(cid:24)0.9
Yes
0.1(cid:24)0.9
Yes
0.1(cid:24)0.9
No
0.1(cid:24)0.9
Slow
0.1(cid:24)0.9
Slow
0.1(cid:24)0.9
Slow
0
-0.1
-1
0
-0.1
-1
1
1
1
1
1
1
0
0
0
0.1
0.1
0.1
表 11.2: DRL 的参数设置。
11.4 总结
⽬前模型已经上线，⼤家可以在阿⾥⼩蜜⾥试⽤。在这个项⽬⾥，我们探
索了基于深度强化学习（DRL）的任务型问答的⽅案，在订机票这个任务上⾛
通了这个⽅案。据我们所知，这是第⼀个在⼯业界落地的基于 DRL 的任务型问
答技术，后续我们准备把这个⽅案扩展到其他的任务上（充值，订电影票等），
理想的情况下是我们通过预训练平台就可以把 action policy 和 slot extraction 两
部分都学好。
第十二章 DRL 导购－阿里小蜜的多
轮标签推荐技术
12.1 背景
阿⾥⼩蜜作为⼀个⼈⼯智能助理，除了可以解决⽤户的业务咨询类问题，缓
解⼈⼯压⼒之外，也兼备了其他助理业务包括：查天⽓、买机票、充话费等，同
时，还可以通过多轮会话完成⼀个导购助理的⼯作。借助⼩蜜多轮会话能⼒，如
何⼀步⼀步的搜集⽤户购买意图，促成商品成交，是本⽂要解决的问题。本⽂
旨在通过主动推荐引导的⽅式推荐商品标签，搜集⽤户意图，最终为⽤户推荐
出满⾜他们需求的商品。
⼩蜜导购助理的⽬标可细分为以下⼏点：
• 借助多轮对话场景逐步理解⽤户购买意图；
• 形成通⽤的挖掘和上下位链路预测⽅案；
• 结合⽤户信息 CTR 预估，实现实时、个性化推荐，最终促成商品成交；
• 使⽤强化学习的⽅式完成在线增强式标签推荐。
最终的产品形态是标签基础上，结合图⽚和选项卡的⽅式，应⽤到多轮对
话中，如下图12.1（红框圈定范围内）。
如图所⽰，这次交互总共分四轮，⽤户和系统的⾏为如下：
. 132 .
第⼗⼆章 DRL 导购－阿⾥⼩蜜的多轮标签推荐技术
图 12.1: 阿⾥⼩蜜多轮标签推荐样例。
System
User
Ask：我要买笔记本电脑 展⽰品牌：ThinkPad，Dell，Apple, (cid:1)(cid:1)(cid:1)
展⽰屏幕属性：17 英⼨，11.6 英⼨，(cid:1)(cid:1)(cid:1)
Click：Apple
Click：13.3 英⼨
展⽰商品：Apple,13.3 英⼨电脑
Click item, buy it (or quit) End
表 12.1: 阿⾥⼩蜜多轮标签推荐中⽤户和系统的交互。
这⾥有⼏个难点，⾸先如何从海量商品数据⾥挖掘出有效的商品属性值和
属性名这样的标签对，其次是基于多轮对话信息，如果给出实时和⼈性化的标
签推荐，并且促成商品的点击和成交。前者是商品标签挖掘的任务，我们尝试
了传统的数据挖掘⽅法；后者是多轮标签推荐的任务，我们⽤深度强化学习模
型来促成 CTR 和成交量的提升。
12.2 算法框架
. 133 .
12.2 算法框架
算法框架如图12.2，包含商品标签挖掘，推荐链路预测，和多轮标签推荐。
图 12.2: 阿⾥⼩蜜多轮标签推荐算法框架。
商品标签挖掘我们把标签定义为⼀个商品的属性值和属性名，⽐如说苹果这个
标签的属性值是“品牌”，属性名是“苹果”。我们需要推荐的商品来⾃于淘宝，这
⾥商品的量⾮常⼤，需要先对商品标签进⾏抽取。对于属性名，除了标准的名
字例如“触屏”，“控油”等之外，我们挖掘了⼀些更加常⽤的⼜语化属性表⽰（⽐
如：“触摸屏”，“⽪肤⽐较油”，“千元以内”），⽤在多轮推荐中。训练语料来⾃淘宝
主搜 query，⽤互信息、出现频率、左邻接熵、右邻接熵衡量⼀个词成词的概率。
假设⼀个词是 a; b, 具体定义如下：
;
∑
∑
M Ia;b = log p(a; b)
p(a)p(b)
Hlef t = (cid:0)
Hright = (cid:0)
p(w; a; bja; b) log p(w; a; bja; b);
p(a; b; wja; b) log p(a; b; wja; b):
w
w
其中，互信息 M I 越⼤，表⽰词的内聚程度越⼤，成为候选属性的概率越⼤；左
右邻接熵 Hlef t 和 Hright 则反映了⼀个⽂本⽚段的左邻字集合和右邻字集合的随
机程度，邻熵越⼤，表明该词的左右边界越随机，成词的概率越⼤。
通过在⼤规模语料中计算限定长度的字组合的三个特征值，做归⼀化求和
计算得分，选取得分较⾼的词作为我们的候选词。最后根据挖掘结果对原有
query 进⾏分词。该⽅法对性能要求较⾼，尤其是内存，因此可以通过 batch 的
. 134 .
第⼗⼆章 DRL 导购－阿⾥⼩蜜的多轮标签推荐技术
⽅式分批进⾏。在阿⾥⼩蜜的计算环境中，我们将 batch 的⼤⼩限定在 100w，可
最⼤限度的保证不会出现 OOM。最后整理所有 batch 各个维度的指标，做加和
处理。
挖掘出标签之后，需要对同⼀属性的标签进⾏归类，⽬的是保证在每次推
荐标签时，在同⼀个属性类别下，给⽤户多重选择。标准属性的归类沿⽤主搜
已有的属性命名⽅法，⾮标准属性的归类任务转化为给每个⾮标属性打上属性
名（例如：“⽪肤⽐较油”对应的属性名是“肤质”）。我们使⽤主搜中的搜索点击数
据，即每个 query 后⽤户点击的商品详情。具体操作流程如下：
• 对 query 分词，去掉停⽤词和品类词，剩下的作为候选属性；
• 使⽤候选属性匹配商品详情页中的属性 key-value pair，将匹配到的属性值
对应的属性名与候选属性做关联 (映射过程采⽤模糊匹配：融合了编辑距
离和词共现。例如：“⽪肤⽐较油”会映射到商品详情页中会出现“肤质：油
性⽪肤”上，进⽽可以认为“⽪肤⽐较油”的属性类别是“肤质”)
推荐链路预测拿到候选的标签之后，需要在⽤户的会话链路中，即⽤户的每轮
会话，展⽰推荐的标签。这⾥我们根据主搜⽤户 query 中个属性⾃然顺序，计算
属性之间的马尔科夫链路，具体计算⽅式如下。
开始： ^p = argmaxpi
p(pijcontext);
第⼀轮：
第⼆轮：
p(a1j^p);
p(a2ja1; ^p);
p(苹果j笔记本)
p(17 英⼨j笔记本电脑，苹果)
e:g:
e:g:
这⾥涉及的概率计算都是由对全⽹类⽬⽤户的 query（采⽤模糊匹配）和标签的
点击数据统计⽽来的。每轮取最多前 50 个⾼频标签作为候选。
多轮标签推荐系统设计上，我们采⽤粗排加精排的⽅式，上⾯模块相当于是粗
排，后续连到⼀个多轮标签推荐模块去对标签做精排。传统解决上⾯问题的思
路是做 CTR 预估，根据标签的点击率来建模。然⽽这个⽅案有两个问题，第⼀，
CTR 和 GMV 通常是分开优化，两者是⾮常相关的任务，考虑多⽬标优化会对
两个任务都有帮助；第⼆，⽆法⽤到多轮交互的信息，不同的标签推荐序列会
12.3 深度强化学习模型
. 135 .
对⽤户的是否成交有影响。基于这两点考量，我们⽤ DRL 对多轮对话进⾏建模，
旨在通过多轮交互同时提升 CTR 和 GMV。
我们的问题可以看作是⼀个多轮链路决策的问题，agent 是我们的系统，agent
决定每⼀轮出什么标签（action），之后环境（⽤户）会给系统⼀个反馈，这⾥反
馈可以是正向的（⽐如商品点击或者商品成交），也可以是负向的（退出或者⽤
户表达不满等）。这个问题就成了，如何在多轮交互中获取最⼤的 reward，这就
是典型的 RL 的问题。因此使⽤ RL 来做多轮导购，可以拟合出不同⽤户的推荐
链路来最⼤化⽤户的点击和成交，即 CTR 和 GMV。
12.3 深度强化学习模型
深度强化学习是由深度学习和强化学习两者的结合，其中深度学习负责拟
合给定的输⼊和输出的关系，强化学习负责把控学习⽅向。典型的⽅法有基于
价值的深度强化学习、基于策略的深度强化学习、基于模型的深度强化学习。这
三种不同类型的深度强化学习⽤深度神经⽹络替代了强化学习的不同部件。基
于价值的深度强化学习本质上是⼀个 Q Learning 算法，⽬标是估计最优策略的
Q 值。DQN 是⼀个典型的基于价值的深度强化学习算法，它是由 DeepMind 在
2013 年在 NIPS 提出。DQN 算法的主要做法是 Experience Replay，其将系统探
索环境得到的数据储存起来，然后，打破数据之间的关联性，通过随机采样样
本更新深度神经⽹络的参数。实验下来我们发现 DQN ⽐⼀般的基于策略的深度
强化学习效果好，所以我们采⽤了 DQN 作为我们的强化学习算法。
DQN 的核⼼思想是通过⼀个 value network Q((cid:1)) 来拟合当前 state s 和 action
a 所能达到的累积的 rewards，优化的时候需要考虑所有未来 actions 所带来的
rewards。它的 loss 定义如下：
′
′
; w) (cid:0) Q(s; a; w))2]
; a
′ 和 a
L(w) = E[(r + (cid:13) max
(12.1)
′ 为下个 state 和 action，(cid:13) 为阻尼系数 (discount factor)，r 为当前的
; w) 设为 0。
其中 s
reward。这⾥如果当前状态为⼀个 episode 结束的状态的话，Q(s
; a
下⾯我们详细讲述模型的 episode，state，action，和 reward 的定义。
a′ Q(s
′
′
. 136 .
第⼗⼆章 DRL 导购－阿⾥⼩蜜的多轮标签推荐技术
12.3.1 强化学习模块
Episode. 在多轮标签推荐场景⾥，如果系统第⼀次判断当前⽤户有购买商品的
意图，这个就作为⼀个 episode 的开始，如果⽤户购买了商品或者退出会话，则
认为 episode 结束。
State. 状态主要考虑了三部分信息，⽤户的 profile，⽤户的 question，商品信息
和 session 信息。这⾥除了⽤户的 profile 是个静态特征之外，后⾯三类特征都是
会随着对话的进⾏改变的。
Action. 由于我们的 action space ⾮常⼤（所有的商品的属性名和属性值的组合），
我们对 action 进⾏了参数化，每个 action ⽤它的属性名和属性值表⽰ < pi; vi >，
两部分都是离散的 id。所以，action i 的向量表⽰为:ei (cid:8) e
′
′
i, 这⾥ ei 和 e
i 为神经
⽹络学到的两部分各⾃的向量表⽰。
Reward. 我们主要优化的是 CTR 和 GMV 两个⽬标，其中 GMV ⽐较重要。所
以我们定义 reward 的时候，购买的 reward 设为 1.0，点击的 reward 相对⼩⼀些，
为 0.1。由于我们每轮是展⽰多个标签，只有点击或者购买的标签有正的 reward，
其他部分 reward 都设为 0.0。下图12.3是多轮交互中的 reward 定义的⽰例。
图 12.3: 多轮标签推荐的 reward 定义。
12.3 深度强化学习模型
. 137 .
12.3.2 最终模型
模型上的设计我们主要参考了 Wide&Deep 模型 [6]，这个模型融合了记忆
（memorization）和归纳（generalization）的两个过程，模型中的宽线性模型（Wide
Model）⽤于记忆；深度神经⽹络模型（Deep Model）⽤于归纳。该模型不仅可
以容纳⼀些统计类特征，也可以将 id 类（或者⽂本类）特征通过 DNN 做有效
的归纳抽象，泛化性更强。
图 12.4: 多轮标签推荐的模型。
模型结构如图12.4, 是⼀个 regression 模型。模型主要是根据给定 state s，ac-
tion a，拟合出⼀个 value Q(s; a; w)，这⾥的 w 是模型参数。建模的时候，wide
部分和 deep 部分分别处理不同的特征：其中 Wide 部分放了⼀些常⽤的统计特
征和组合特征，Deep 部分放的是 state 信息，Deep 部分学到的表⽰和 Action 的
表⽰会 concat 到⼀起，经过⼀个 MLP 再和 wide 部分⼀起输出。
. 138 .
第⼗⼆章 DRL 导购－阿⾥⼩蜜的多轮标签推荐技术
12.4 业务实战
在离线数据上，我们对⽐了 DRL 和⾮ DRL 模型在 AUC，Recall 指标上的
变化。如图12.4, 这⾥主要测试的是⽤户点击的指标，在对⽐的四个衡量标准⾥，
DRL 模型都有了提升。这个说明了 DRL 能够学习到多轮对话的序列信息，从⽽
在这个任务上取得了效果的提升。
AUC Recall1 Recall2 Recall5
⾮ DRL 0.94
DRL
0.96
0.86
0.87
0.95
0.97
0.98
0.99
表 12.2: 离线评测 DRL 和⾮ DRL 模型。
在线上对⽐，DRL 模型相⽐⾮ DRL 模型在商品点击成交量提升了 6%，相
⽐纯统计⽅法提升 15%。双⼗⼀期间，DRL 模型产⽣数百万对话轮次，GMV 超
过千万，商品点击成交转化率相⽐去年提⾼ 54%。
12.5 总结和展望
我们尝试在阿⾥⼩蜜的场景中为⽤户提供⼀种新的导购助理。借助多轮对
话场景，逐步理解⽤户的购买意图，实现实时、个性化推荐，最终促成商品成
交。经过 2017 年双 11 的考验，我们的⽅案不仅承接住了流量洪峰的考验，同
时也验证了技术⽅向的可⾏性。然⽽，导购助理还只是 DRL 在阿⾥⼩蜜场景下
的初步尝试，不仅还有着很⼤的提升空间，⽽且在业务上也具有很强的扩展性。
在接下来的时间内，除了继续增强导购助理的能⼒之外，我们也将探索在业务
咨询场景下的问题补全等更多的实际问题。
参考文献
[1] Michael Best and Nilotpal Chakravarti. Active set algorithms for isotonic regres-
sion: a unifying framework. Mathematical Programming, (1-3):425–429, 1990.
[2] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamil-
ton, and Greg Hullender. Learning to rank using gradient descent. In International
Conference on Machine Learning, pages 89–96, 2005.
[3] Christopher J Burges, Robert Ragno, and Quoc V Le. Learning to rank with
nonsmooth cost functions. In NIPS, pages 193–200, 2007.
[4] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey
of multiagent reinforcement learning. IEEE Transactions on Systems, Man, And
Cybernetics-Part C: Applications and Reviews, 38 (2), 2008, 2008.
[5] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank:
from pairwise approach to listwise approach. In ICML, pages 129–136. ACM,
2007.
[6] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan
Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.
Wide & deep learning for recommender systems. CoRR, abs/1606.07792, 2016.
. 140 .
参考⽂献
[7] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate
deep network learning by exponential linear units (elus). 2016.
[8] Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube
recommendations. In ACM Conference on Recommender Systems, pages 191–
198, 2016.
[9] Fredric C Gey.
Inferring probability of relevance using the method of logistic
In Proceedings of the 17th annual international ACM SIGIR con-
regression.
ference on Research and development in information retrieval, pages 222–231.
Springer-Verlag New York, Inc., 1994.
[10] Saurabh Gupta, Sayan Pathak, and Bivas Mitra. Complementary Usage of Tips
and Reviews for Location Recommendation in Yelp. Springer International Pub-
lishing, 2015.
[11] Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval
In
Tassa. Learning continuous control policies by stochastic value gradients.
NIPS, pages 2944–2952, 2015.
[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
computation, 9(8):1735–1780, 1997.
[13] Junling Hu, Michael P Wellman, et al. Multiagent reinforcement learning: theo-
retical framework and an algorithm. In ICML, volume 98, pages 242–250, 1998.
[14] Guido W Imbens and Tony Lancaster. Efficient estimation and stratified sam-
pling. Journal of Econometrics, 74(2):289–318, 1996.
[15] Krishnaram Kenthapadi, Krishnaram Kenthapadi, and Krishnaram Kenthapadi.
Lijar: A system for job application redistribution towards efficient career mar-
ketplace. In ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 1397–1406, 2017.
参考⽂献
. 141 .
[16] Yoon Kim. Convolutional neural networks for sentence classification. In EMNLP,
2014.
[17] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in
neural information processing systems, pages 1008–1014, 2000.
[18] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya
Kawakami, and Chris Dyer. Neural architectures for named entity recognition.
CoRR, abs/1603.01360, 2016.
[19] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factor-
ization. In Advances in neural information processing systems, pages 2177–2185,
2014.
[20] Feng-Lin Li, Minghui Qiu, Haiqing Chen, Xiongwei Wang, Xing Gao, Jun
Huang, Juwei Ren, Zhongzhou Zhao, Weipeng Zhao, Lei Wang, Guwei Jin, and
Wei Chu. AliMe Assist: An Intelligent Assistant for Creating an Innovative E-
commerce Experience. 2017.
[21] Ping Li, Qiang Wu, and Christopher J Burges. Mcrank: Learning to rank using
multiple classification and gradient boosting. In Advances in neural information
processing systems, pages 897–904, 2008.
[22] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[23] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep
reinforcement learning. In Proceedings of International Conference on Learning
Representations, pages 1–14, 2015.
. 142 .
参考⽂献
[24] Michael L Littman. Markov games as a framework for multi-agent reinforcement
In Proceedings of the eleventh international conference on machine
learning.
learning, volume 157, pages 157–163, 1994.
[25] Tie-Yan Liu et al. Learning to rank for information retrieval. Foundations and
Trends® in Information Retrieval, 3(3):225–331, 2009.
[26] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Ve-
ness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
Georg Ostrovski, et al. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015.
[27] Jerzy Neyman. On the two different aspects of the representative method: the
method of stratified sampling and the method of purposive selection. Journal of
the Royal Statistical Society, 97(4):558–625, 1934.
[28] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the
art. Autonomous agents and multi-agent systems, 11(3):387–434, 2005.
[29] Tao Qin, Xu-Dong Zhang, De-Sheng Wang, Tie-Yan Liu, Wei Lai, and Hang
Li. Ranking with multiple hyperplanes. In Proceedings of the 30th annual in-
ternational ACM SIGIR conference on Research and development in information
retrieval, pages 279–286. ACM, 2007.
[30] RY Rubinstein and DP Kroese. The cross-entropy method: A unified approach to
combinatorial optimization, monte-carlo simulation and machine learning. 2004.
[31] Tim Salimans, Jonathan Ho, Xi Chen, and IIya Sutskever. Evolution strategies as
a scalable alternative to reinforcement learning. arXiv:1703.03864, 2017.
[32] Juan C Santamaría, Richard S Sutton, and Ashwin Ram. Experiments with rein-
forcement learning in problems with continuous state and action spaces. Adaptive
behavior, 6(2):163–217, 1997.
参考⽂献
. 143 .
[33] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction,
volume 1. MIT press Cambridge, 1998.
[34] Paul R Thie. Markov decision processes. Comap, Incorporated, 1983.
[35] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning
with double q-learning. In AAAI, pages 2094–2100, 2016.
[36] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.
In
C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28, pages 2692–2700. Cur-
ran Associates, Inc., 2015.
[37] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Van Hasselt, Marc Lanctot, and
Nando De Freitas. Dueling network architectures for deep reinforcement learning.
In Proceedings of International Conference on Machine Learning, 2015.
[38] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-
4):279–292, 1992.
[39] Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, and Kun Gai. Op-
timized cost per click in taobao display advertising. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery and Data Min-
ing, KDD ’17, pages 2191–2200, New York, NY, USA, 2017. ACM.
